{"messages": [{"role": "user", "content": "convert the following MLX Python code into MLX Swift: \n\n# Copyright \u00a9 2023-2024 Apple Inc.\n\nimport time\nfrom typing import Optional, Tuple\n\nimport mlx.core as mx\n\nfrom .model_io import (\n    _DEFAULT_MODEL,\n    load_autoencoder,\n    load_diffusion_config,\n    load_text_encoder,\n    load_tokenizer,\n    load_unet,\n)\nfrom .sampler import SimpleEulerAncestralSampler, SimpleEulerSampler\n\n\nclass StableDiffusion:\n    def __init__(self, model: str = _DEFAULT_MODEL, float16: bool = False):\n        self.dtype = mx.float16 if float16 else mx.float32\n        self.diffusion_config = load_diffusion_config(model)\n        self.unet = load_unet(model, float16)\n        self.text_encoder = load_text_encoder(model, float16)\n        self.autoencoder = load_autoencoder(model, False)\n        self.sampler = SimpleEulerSampler(self.diffusion_config)\n        self.tokenizer = load_tokenizer(model)\n\n    def ensure_models_are_loaded(self):\n        mx.eval(self.unet.parameters())\n        mx.eval(self.text_encoder.parameters())\n        mx.eval(self.autoencoder.parameters())\n\n    def _tokenize(self, tokenizer, text: str, negative_text: Optional[str] = None):\n        # Tokenize the text\n        tokens = [tokenizer.tokenize(text)]\n        if negative_text is not None:\n            tokens += [tokenizer.tokenize(negative_text)]\n        lengths = [len(t) for t in tokens]\n        N = max(lengths)\n        tokens = [t + [0] * (N - len(t)) for t in tokens]\n        tokens = mx.array(tokens)\n\n        return tokens\n\n    def _get_text_conditioning(\n        self,\n        text: str,\n        n_images: int = 1,\n        cfg_weight: float = 7.5,\n        negative_text: str = \"\",\n    ):\n        # Tokenize the text\n        tokens = self._tokenize(\n            self.tokenizer, text, (negative_text if cfg_weight > 1 else None)\n        )\n\n        # Compute the features\n        conditioning = self.text_encoder(tokens).last_hidden_state\n\n        # Repeat the conditioning for each of the generated images\n        if n_images > 1:\n            conditioning = mx.repeat(conditioning, n_images, axis=0)\n\n        return conditioning\n\n    def _denoising_step(\n        self, x_t, t, t_prev, conditioning, cfg_weight: float = 7.5, text_time=None\n    ):\n        x_t_unet = mx.concatenate([x_t] * 2, axis=0) if cfg_weight > 1 else x_t\n        t_unet = mx.broadcast_to(t, [len(x_t_unet)])\n        eps_pred = self.unet(\n            x_t_unet, t_unet, encoder_x=conditioning, text_time=text_time\n        )\n\n        if cfg_weight > 1:\n            eps_text, eps_neg = eps_pred.split(2)\n            eps_pred = eps_neg + cfg_weight * (eps_text - eps_neg)\n\n        x_t_prev = self.sampler.step(eps_pred, x_t, t, t_prev)\n\n        return x_t_prev\n\n    def _denoising_loop(\n        self,\n        x_T,\n        T,\n        conditioning,\n        num_steps: int = 50,\n        cfg_weight: float = 7.5,\n        text_time=None,\n    ):\n        x_t = x_T\n        for t, t_prev in self.sampler.timesteps(\n            num_steps, start_time=T, dtype=self.dtype\n        ):\n            x_t = self._denoising_step(\n                x_t, t, t_prev, conditioning, cfg_weight, text_time\n            )\n            yield x_t\n\n    def generate_latents(\n        self,\n        text: str,\n        n_images: int = 1,\n        num_steps: int = 50,\n        cfg_weight: float = 7.5,\n        negative_text: str = \"\",\n        latent_size: Tuple[int] = (64, 64),\n        seed=None,\n    ):\n        # Set the PRNG state\n        seed = int(time.time()) if seed is None else seed\n        mx.random.seed(seed)\n\n        # Get the text conditioning\n        conditioning = self._get_text_conditioning(\n            text, n_images, cfg_weight, negative_text\n        )\n\n        # Create the latent variables\n        x_T = self.sampler.sample_prior(\n            (n_images, *latent_size, self.autoencoder.latent_channels), dtype=self.dtype\n        )\n\n        # Perform the denoising loop\n        yield from self._denoising_loop(\n            x_T, self.sampler.max_time, conditioning, num_steps, cfg_weight\n        )\n\n    def generate_latents_from_image(\n        self,\n        image,\n        text: str,\n        n_images: int = 1,\n        strength: float = 0.8,\n        num_steps: int = 50,\n        cfg_weight: float = 7.5,\n        negative_text: str = \"\",\n        seed=None,\n    ):\n        # Set the PRNG state\n        seed = int(time.time()) if seed is None else seed\n        mx.random.seed(seed)\n\n        # Define the num steps and start step\n        start_step = self.sampler.max_time * strength\n        num_steps = int(num_steps * strength)\n\n        # Get the text conditioning\n        conditioning = self._get_text_conditioning(\n            text, n_images, cfg_weight, negative_text\n        )\n\n        # Get the latents from the input image and add noise according to the\n        # start time.\n        x_0, _ = self.autoencoder.encode(image[None])\n        x_0 = mx.broadcast_to(x_0, (n_images,) + x_0.shape[1:])\n        x_T = self.sampler.add_noise(x_0, mx.array(start_step))\n\n        # Perform the denoising loop\n        yield from self._denoising_loop(\n            x_T, start_step, conditioning, num_steps, cfg_weight\n        )\n\n    def decode(self, x_t):\n        x = self.autoencoder.decode(x_t)\n        x = mx.clip(x / 2 + 0.5, 0, 1)\n        return x\n\n\nclass StableDiffusionXL(StableDiffusion):\n    def __init__(self, model: str = _DEFAULT_MODEL, float16: bool = False):\n        super().__init__(model, float16)\n\n        self.sampler = SimpleEulerAncestralSampler(self.diffusion_config)\n\n        self.text_encoder_1 = self.text_encoder\n        self.tokenizer_1 = self.tokenizer\n        del self.tokenizer, self.text_encoder\n\n        self.text_encoder_2 = load_text_encoder(\n            model,\n            float16,\n            model_key=\"text_encoder_2\",\n        )\n        self.tokenizer_2 = load_tokenizer(\n            model,\n            merges_key=\"tokenizer_2_merges\",\n            vocab_key=\"tokenizer_2_vocab\",\n        )\n\n    def ensure_models_are_loaded(self):\n        mx.eval(self.unet.parameters())\n        mx.eval(self.text_encoder_1.parameters())\n        mx.eval(self.text_encoder_2.parameters())\n        mx.eval(self.autoencoder.parameters())\n\n    def _get_text_conditioning(\n        self,\n        text: str,\n        n_images: int = 1,\n        cfg_weight: float = 7.5,\n        negative_text: str = \"\",\n    ):\n        tokens_1 = self._tokenize(\n            self.tokenizer_1,\n            text,\n            (negative_text if cfg_weight > 1 else None),\n        )\n        tokens_2 = self._tokenize(\n            self.tokenizer_2,\n            text,\n            (negative_text if cfg_weight > 1 else None),\n        )\n\n        conditioning_1 = self.text_encoder_1(tokens_1)\n        conditioning_2 = self.text_encoder_2(tokens_2)\n        conditioning = mx.concatenate(\n            [conditioning_1.hidden_states[-2], conditioning_2.hidden_states[-2]],\n            axis=-1,\n        )\n        pooled_conditioning = conditioning_2.pooled_output\n\n        if n_images > 1:\n            conditioning = mx.repeat(conditioning, n_images, axis=0)\n            pooled_conditioning = mx.repeat(pooled_conditioning, n_images, axis=0)\n\n        return conditioning, pooled_conditioning\n\n    def generate_latents(\n        self,\n        text: str,\n        n_images: int = 1,\n        num_steps: int = 2,\n        cfg_weight: float = 0.0,\n        negative_text: str = \"\",\n        latent_size: Tuple[int] = (64, 64),\n        seed=None,\n    ):\n        # Set the PRNG state\n        seed = int(time.time()) if seed is None else seed\n        mx.random.seed(seed)\n\n        # Get the text conditioning\n        conditioning, pooled_conditioning = self._get_text_conditioning(\n            text, n_images, cfg_weight, negative_text\n        )\n        text_time = (\n            pooled_conditioning,\n            mx.array([[512, 512, 0, 0, 512, 512.0]] * len(pooled_conditioning)),\n        )\n\n        # Create the latent variables\n        x_T = self.sampler.sample_prior(\n            (n_images, *latent_size, self.autoencoder.latent_channels), dtype=self.dtype\n        )\n\n        # Perform the denoising loop\n        yield from self._denoising_loop(\n            x_T,\n            self.sampler.max_time,\n            conditioning,\n            num_steps,\n            cfg_weight,\n            text_time=text_time,\n        )\n\n    def generate_latents_from_image(\n        self,\n        image,\n        text: str,\n        n_images: int = 1,\n        strength: float = 0.8,\n        num_steps: int = 2,\n        cfg_weight: float = 0.0,\n        negative_text: str = \"\",\n        seed=None,\n    ):\n        # Set the PRNG state\n        seed = seed or int(time.time())\n        mx.random.seed(seed)\n\n        # Define the num steps and start step\n        start_step = self.sampler.max_time * strength\n        num_steps = int(num_steps * strength)\n\n        # Get the text conditioning\n        conditioning, pooled_conditioning = self._get_text_conditioning(\n            text, n_images, cfg_weight, negative_text\n        )\n        text_time = (\n            pooled_conditioning,\n            mx.array([[512, 512, 0, 0, 512, 512.0]] * len(pooled_conditioning)),\n        )\n\n        # Get the latents from the input image and add noise according to the\n        # start time.\n        x_0, _ = self.autoencoder.encode(image[None])\n        x_0 = mx.broadcast_to(x_0, (n_images,) + x_0.shape[1:])\n        x_T = self.sampler.add_noise(x_0, mx.array(start_step))\n\n        # Perform the denoising loop\n        yield from self._denoising_loop(\n            x_T, start_step, conditioning, num_steps, cfg_weight, text_time=text_time\n        )"}, {"role": "assistant", "content": "// Copyright \u00a9 2024 Apple Inc.\n\nimport Foundation\nimport Hub\nimport MLX\nimport MLXNN\n\n// port of https://github.com/ml-explore/mlx-examples/blob/main/stable_diffusion/stable_diffusion/__init__.py\n\n/// Iterator that produces latent images.\n///\n/// Created by:\n///\n/// - ``TextToImageGenerator/generateLatents(parameters:)``\n/// - ``ImageToImageGenerator/generateLatents(image:parameters:strength:)``\npublic struct DenoiseIterator: Sequence, IteratorProtocol {\n\n    let sd: StableDiffusion\n\n    var xt: MLXArray\n\n    let conditioning: MLXArray\n    let cfgWeight: Float\n    let textTime: (MLXArray, MLXArray)?\n\n    var i: Int\n    let steps: [(MLXArray, MLXArray)]\n\n    init(\n        sd: StableDiffusion, xt: MLXArray, t: Int, conditioning: MLXArray, steps: Int,\n        cfgWeight: Float, textTime: (MLXArray, MLXArray)? = nil\n    ) {\n        self.sd = sd\n        self.steps = sd.sampler.timeSteps(steps: steps, start: t, dType: sd.dType)\n        self.i = 0\n        self.xt = xt\n        self.conditioning = conditioning\n        self.cfgWeight = cfgWeight\n        self.textTime = textTime\n    }\n\n    public var underestimatedCount: Int {\n        steps.count\n    }\n\n    mutating public func next() -> MLXArray? {\n        guard i < steps.count else {\n            return nil\n        }\n\n        let (t, tPrev) = steps[i]\n        i += 1\n\n        xt = sd.step(\n            xt: xt, t: t, tPrev: tPrev, conditioning: conditioning, cfgWeight: cfgWeight,\n            textTime: textTime)\n        return xt\n    }\n}\n\n/// Type for the _decoder_ step.\npublic typealias ImageDecoder = (MLXArray) -> MLXArray\n\npublic protocol ImageGenerator {\n    func ensureLoaded()\n\n    /// Return a detached decoder -- this is useful if trying to conserve memory.\n    ///\n    /// The decoder can be used independently of the ImageGenerator to transform\n    /// latents into raster images.\n    func detachedDecoder() -> ImageDecoder\n\n    /// the equivalent to the ``detachedDecoder()`` but without the detatching\n    func decode(xt: MLXArray) -> MLXArray\n}\n\n/// Public interface for transforming a text prompt into an image.\n///\n/// Steps:\n///\n/// - ``generateLatents(parameters:)``\n/// - evaluate each of the latents from the iterator\n/// - ``ImageGenerator/decode(xt:)`` or ``ImageGenerator/detachedDecoder()`` to convert the final latent into an image\n/// - use ``Image`` to save the image\npublic protocol TextToImageGenerator: ImageGenerator {\n    func generateLatents(parameters: EvaluateParameters) -> DenoiseIterator\n}\n\n/// Public interface for transforming a text prompt into an image.\n///\n/// Steps:\n///\n/// - ``generateLatents(image:parameters:strength:)``\n/// - evaluate each of the latents from the iterator\n/// - ``ImageGenerator/decode(xt:)`` or ``ImageGenerator/detachedDecoder()`` to convert the final latent into an image\n/// - use ``Image`` to save the image\npublic protocol ImageToImageGenerator: ImageGenerator {\n    func generateLatents(image: MLXArray, parameters: EvaluateParameters, strength: Float)\n        -> DenoiseIterator\n}\n\nenum ModelContainerError: LocalizedError {\n    /// Unable to create the particular type of model, e.g. it doesn't support image to image\n    case unableToCreate(String, String)\n    /// When operating in conserveMemory mode, it tried to use a model that had been discarded\n    case modelDiscarded\n\n    var errorDescription: String? {\n        switch self {\n        case .unableToCreate(let modelId, let generatorType):\n            return String(\n                localized:\n                    \"Unable to create a \\(generatorType) with model ID '\\(modelId)'. The model may not support this operation type.\"\n            )\n        case .modelDiscarded:\n            return String(\n                localized:\n                    \"The model has been discarded to conserve memory and is no longer available. Please recreate the model container.\"\n            )\n        }\n    }\n}\n\n/// Container for models that guarantees single threaded access.\npublic actor ModelContainer<M> {\n\n    enum State {\n        case discarded\n        case loaded(M)\n    }\n\n    var state: State\n\n    /// if true this will discard the model in ``performTwoStage(first:second:)``\n    var conserveMemory = false\n\n    private init(model: M) {\n        self.state = .loaded(model)\n    }\n\n    /// create a ``ModelContainer`` that supports ``TextToImageGenerator``\n    static public func createTextToImageGenerator(\n        configuration: StableDiffusionConfiguration, loadConfiguration: LoadConfiguration = .init()\n    ) throws -> ModelContainer<TextToImageGenerator> {\n        if let model = try configuration.textToImageGenerator(configuration: loadConfiguration) {\n            return .init(model: model)\n        } else {\n            throw ModelContainerError.unableToCreate(configuration.id, \"TextToImageGenerator\")\n        }\n    }\n\n    /// create a ``ModelContainer`` that supports ``ImageToImageGenerator``\n    static public func createImageToImageGenerator(\n        configuration: StableDiffusionConfiguration, loadConfiguration: LoadConfiguration = .init()\n    ) throws -> ModelContainer<ImageToImageGenerator> {\n        if let model = try configuration.imageToImageGenerator(configuration: loadConfiguration) {\n            return .init(model: model)\n        } else {\n            throw ModelContainerError.unableToCreate(configuration.id, \"ImageToImageGenerator\")\n        }\n    }\n\n    public func setConserveMemory(_ conserveMemory: Bool) {\n        self.conserveMemory = conserveMemory\n    }\n\n    /// Perform an action on the model and/or tokenizer. Callers _must_ eval any `MLXArray` before returning as\n    /// `MLXArray` is not `Sendable`.\n    public func perform<R>(_ action: @Sendable (M) throws -> R) throws -> R {\n        switch state {\n        case .discarded:\n            throw ModelContainerError.modelDiscarded\n        case .loaded(let m):\n            try action(m)\n        }\n    }\n\n    /// Perform a two stage action where the first stage returns values passed to the second stage.\n    ///\n    /// If ``setConservativeMemory(_:)`` is `true` this will discard the model in between\n    /// the `first` and `second` blocks. The container will have to be recreated if a caller\n    /// wants to use it again.\n    ///\n    /// If `false` this will just run them in sequence and the container can be reused.\n    ///\n    /// Callers _must_ eval any `MLXArray` before returning as `MLXArray` is not `Sendable`.\n    public func performTwoStage<R1, R2>(\n        first: @Sendable (M) throws -> R1, second: @Sendable (R1) throws -> R2\n    ) throws -> R2 {\n        let r1 =\n            switch state {\n            case .discarded:\n                throw ModelContainerError.modelDiscarded\n            case .loaded(let m):\n                try first(m)\n            }\n        if conserveMemory {\n            self.state = .discarded\n        }\n        return try second(r1)\n    }\n\n}\n\n/// Base class for Stable Diffusion.\nopen class StableDiffusion {\n\n    let dType: DType\n    let diffusionConfiguration: DiffusionConfiguration\n    let unet: UNetModel\n    let textEncoder: CLIPTextModel\n    let autoencoder: Autoencoder\n    let sampler: SimpleEulerSampler\n    let tokenizer: CLIPTokenizer\n\n    internal init(\n        hub: HubApi, configuration: StableDiffusionConfiguration, dType: DType,\n        diffusionConfiguration: DiffusionConfiguration? = nil, unet: UNetModel? = nil,\n        textEncoder: CLIPTextModel? = nil, autoencoder: Autoencoder? = nil,\n        sampler: SimpleEulerSampler? = nil, tokenizer: CLIPTokenizer? = nil\n    ) throws {\n        self.dType = dType\n        self.diffusionConfiguration =\n            try diffusionConfiguration\n            ?? loadDiffusionConfiguration(hub: hub, configuration: configuration)\n        self.unet = try unet ?? loadUnet(hub: hub, configuration: configuration, dType: dType)\n        self.textEncoder =\n            try textEncoder ?? loadTextEncoder(hub: hub, configuration: configuration, dType: dType)\n\n        // note: autoencoder uses float32 weights\n        self.autoencoder =\n            try autoencoder\n            ?? loadAutoEncoder(hub: hub, configuration: configuration, dType: .float32)\n\n        if let sampler {\n            self.sampler = sampler\n        } else {\n            self.sampler = SimpleEulerSampler(configuration: self.diffusionConfiguration)\n        }\n        self.tokenizer = try tokenizer ?? loadTokenizer(hub: hub, configuration: configuration)\n    }\n\n    open func ensureLoaded() {\n        eval(unet, textEncoder, autoencoder)\n    }\n\n    func tokenize(tokenizer: CLIPTokenizer, text: String, negativeText: String?) -> MLXArray {\n        var tokens = [tokenizer.tokenize(text: text)]\n        if let negativeText {\n            tokens.append(tokenizer.tokenize(text: negativeText))\n        }\n\n        let c = tokens.count\n        let max = tokens.map { $0.count }.max() ?? 0\n        let mlxTokens = MLXArray(\n            tokens\n                .map {\n                    ($0 + Array(repeating: 0, count: max - $0.count))\n                }\n                .flatMap { $0 }\n        )\n        .reshaped(c, max)\n\n        return mlxTokens\n    }\n\n    open func step(\n        xt: MLXArray, t: MLXArray, tPrev: MLXArray, conditioning: MLXArray, cfgWeight: Float,\n        textTime: (MLXArray, MLXArray)?\n    ) -> MLXArray {\n        let xtUnet = cfgWeight > 1 ? concatenated([xt, xt], axis: 0) : xt\n        let tUnet = broadcast(t, to: [xtUnet.count])\n\n        var epsPred = unet(xtUnet, timestep: tUnet, encoderX: conditioning, textTime: textTime)\n\n        if cfgWeight > 1 {\n            let (epsText, epsNeg) = epsPred.split()\n            epsPred = epsNeg + cfgWeight * (epsText - epsNeg)\n        }\n\n        return sampler.step(epsPred: epsPred, xt: xt, t: t, tPrev: tPrev)\n    }\n\n    public func detachedDecoder() -> ImageDecoder {\n        let autoencoder = self.autoencoder\n        func decode(xt: MLXArray) -> MLXArray {\n            var x = autoencoder.decode(xt)\n            x = clip(x / 2 + 0.5, min: 0, max: 1)\n            return x\n        }\n        return decode(xt:)\n    }\n\n    public func decode(xt: MLXArray) -> MLXArray {\n        detachedDecoder()(xt)\n    }\n}\n\n/// Implementation of ``StableDiffusion`` for the `stabilityai/stable-diffusion-2-1-base` model.\nopen class StableDiffusionBase: StableDiffusion, TextToImageGenerator {\n\n    public init(hub: HubApi, configuration: StableDiffusionConfiguration, dType: DType) throws {\n        try super.init(hub: hub, configuration: configuration, dType: dType)\n    }\n\n    func conditionText(text: String, imageCount: Int, cfgWeight: Float, negativeText: String?)\n        -> MLXArray\n    {\n        // tokenize the text\n        let tokens = tokenize(\n            tokenizer: tokenizer, text: text, negativeText: cfgWeight > 1 ? negativeText : nil)\n\n        // compute the features\n        var conditioning = textEncoder(tokens).lastHiddenState\n\n        // repeat the conditioning for each of the generated images\n        if imageCount > 1 {\n            conditioning = repeated(conditioning, count: imageCount, axis: 0)\n        }\n\n        return conditioning\n    }\n\n    public func generateLatents(parameters: EvaluateParameters) -> DenoiseIterator {\n        MLXRandom.seed(parameters.seed)\n\n        let conditioning = conditionText(\n            text: parameters.prompt, imageCount: parameters.imageCount,\n            cfgWeight: parameters.cfgWeight, negativeText: parameters.negativePrompt)\n\n        let xt = sampler.samplePrior(\n            shape: [parameters.imageCount] + parameters.latentSize + [autoencoder.latentChannels],\n            dType: dType)\n\n        return DenoiseIterator(\n            sd: self, xt: xt, t: sampler.maxTime, conditioning: conditioning,\n            steps: parameters.steps, cfgWeight: parameters.cfgWeight)\n    }\n\n}\n\n/// Implementation of ``StableDiffusion`` for the `stabilityai/sdxl-turbo` model.\nopen class StableDiffusionXL: StableDiffusion, TextToImageGenerator, ImageToImageGenerator {\n\n    let textEncoder2: CLIPTextModel\n    let tokenizer2: CLIPTokenizer\n\n    public init(hub: HubApi, configuration: StableDiffusionConfiguration, dType: DType) throws {\n        let diffusionConfiguration = try loadConfiguration(\n            hub: hub, configuration: configuration, key: .diffusionConfig,\n            type: DiffusionConfiguration.self)\n        let sampler = SimpleEulerAncestralSampler(configuration: diffusionConfiguration)\n\n        self.textEncoder2 = try loadTextEncoder(\n            hub: hub, configuration: configuration, configKey: .textEncoderConfig2,\n            weightsKey: .textEncoderWeights2, dType: dType)\n\n        self.tokenizer2 = try loadTokenizer(\n            hub: hub, configuration: configuration, vocabulary: .tokenizerVocabulary2,\n            merges: .tokenizerMerges2)\n\n        try super.init(\n            hub: hub, configuration: configuration, dType: dType,\n            diffusionConfiguration: diffusionConfiguration, sampler: sampler)\n    }\n\n    open override func ensureLoaded() {\n        super.ensureLoaded()\n        eval(textEncoder2)\n    }\n\n    func conditionText(text: String, imageCount: Int, cfgWeight: Float, negativeText: String?) -> (\n        MLXArray, MLXArray\n    ) {\n        let tokens1 = tokenize(\n            tokenizer: tokenizer, text: text, negativeText: cfgWeight > 1 ? negativeText : nil)\n        let tokens2 = tokenize(\n            tokenizer: tokenizer2, text: text, negativeText: cfgWeight > 1 ? negativeText : nil)\n\n        let conditioning1 = textEncoder(tokens1)\n        let conditioning2 = textEncoder2(tokens2)\n        var conditioning = concatenated(\n            [\n                conditioning1.hiddenStates.dropLast().last!,\n                conditioning2.hiddenStates.dropLast().last!,\n            ],\n            axis: -1)\n        var pooledConditionng = conditioning2.pooledOutput\n\n        if imageCount > 1 {\n            conditioning = repeated(conditioning, count: imageCount, axis: 0)\n            pooledConditionng = repeated(pooledConditionng, count: imageCount, axis: 0)\n        }\n\n        return (conditioning, pooledConditionng)\n    }\n\n    public func generateLatents(parameters: EvaluateParameters) -> DenoiseIterator {\n        MLXRandom.seed(parameters.seed)\n\n        let (conditioning, pooledConditioning) = conditionText(\n            text: parameters.prompt, imageCount: parameters.imageCount,\n            cfgWeight: parameters.cfgWeight, negativeText: parameters.negativePrompt)\n\n        let textTime = (\n            pooledConditioning,\n            repeated(\n                MLXArray(converting: [512.0, 512, 0, 0, 512, 512]).reshaped(1, -1),\n                count: pooledConditioning.count, axis: 0)\n        )\n\n        let xt = sampler.samplePrior(\n            shape: [parameters.imageCount] + parameters.latentSize + [autoencoder.latentChannels],\n            dType: dType)\n\n        return DenoiseIterator(\n            sd: self, xt: xt, t: sampler.maxTime, conditioning: conditioning,\n            steps: parameters.steps, cfgWeight: parameters.cfgWeight, textTime: textTime)\n    }\n\n    public func generateLatents(image: MLXArray, parameters: EvaluateParameters, strength: Float)\n        -> DenoiseIterator\n    {\n        MLXRandom.seed(parameters.seed)\n\n        // Define the num steps and start step\n        let startStep = Float(sampler.maxTime) * strength\n        let numSteps = Int(Float(parameters.steps) * strength)\n\n        let (conditioning, pooledConditioning) = conditionText(\n            text: parameters.prompt, imageCount: parameters.imageCount,\n            cfgWeight: parameters.cfgWeight, negativeText: parameters.negativePrompt)\n\n        let textTime = (\n            pooledConditioning,\n            repeated(\n                MLXArray(converting: [512.0, 512, 0, 0, 512, 512]).reshaped(1, -1),\n                count: pooledConditioning.count, axis: 0)\n        )\n\n        // Get the latents from the input image and add noise according to the\n        // start time.\n\n        var (x0, _) = autoencoder.encode(image[.newAxis])\n        x0 = broadcast(x0, to: [parameters.imageCount] + x0.shape.dropFirst())\n        let xt = sampler.addNoise(x: x0, t: MLXArray(startStep))\n\n        return DenoiseIterator(\n            sd: self, xt: xt, t: sampler.maxTime, conditioning: conditioning, steps: numSteps,\n            cfgWeight: parameters.cfgWeight, textTime: textTime)\n    }\n}"}]}
{"messages": [{"role": "user", "content": "convert the following MLX Python code into MLX Swift: \n\n# Copyright \u00a9 2023 Apple Inc.\n\nimport regex\n\n\nclass Tokenizer:\n    \"\"\"A simple port of CLIPTokenizer from https://github.com/huggingface/transformers/ .\"\"\"\n\n    def __init__(self, bpe_ranks, vocab):\n        self.bpe_ranks = bpe_ranks\n        self.vocab = vocab\n        self.pat = regex.compile(\n            r\"\"\"<\\|startoftext\\|\\u003e|<\\|endoftext\\|\\u003e|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\",\n            regex.IGNORECASE,\n        )\n\n        self._cache = {self.bos: self.bos, self.eos: self.eos}\n\n    @property\n    def bos(self):\n        return \"\\u003c|startoftext|\\u003e\"\n\n    @property\n    def bos_token(self):\n        return self.vocab[self.bos]\n\n    @property\n    def eos(self):\n        return \"\\u003c|endoftext|\\u003e\"\n\n    @property\n    def eos_token(self):\n        return self.vocab[self.eos]\n\n    def bpe(self, text):\n        if text in self._cache:\n            return self._cache[text]\n\n        unigrams = list(text[:-1]) + [text[-1] + \"</w>\"]\n        unique_bigrams = set(zip(unigrams, unigrams[1:]))\n\n        if not unique_bigrams:\n            return unigrams\n\n        # In every iteration try to merge the two most likely bigrams. If none\n        # was merged we are done.\n        #\n        # Ported from https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/tokenization_clip.py\n        while unique_bigrams:\n            bigram = min(\n                unique_bigrams, key=lambda pair: self.bpe_ranks.get(pair, float(\"inf\"))\n            )\n            if bigram not in self.bpe_ranks:\n                break\n\n            new_unigrams = []\n            skip = False\n            for a, b in zip(unigrams, unigrams[1:]):\n                if skip:\n                    skip = False\n                    continue\n\n                if (a, b) == bigram:\n                    new_unigrams.append(a + b)\n                    skip = True\n\n                else:\n                    new_unigrams.append(a)\n\n            if not skip:\n                new_unigrams.append(b)\n\n            unigrams = new_unigrams\n            unique_bigrams = set(zip(unigrams, unigrams[1:]))\n\n        self._cache[text] = unigrams\n\n        return unigrams\n\n    def tokenize(self, text, prepend_bos=True, append_eos=True):\n        if isinstance(text, list):\n            return [self.tokenize(t, prepend_bos, append_eos) for t in text]\n\n        # Lower case cleanup and split according to self.pat. Hugging Face does\n        # a much more thorough job here but this should suffice for 95% of\n        # cases.\n        clean_text = regex.sub(r\"\\s+\", \" \", text.lower())\n        tokens = regex.findall(self.pat, clean_text)\n\n        # Split the tokens according to the byte-pair merge file\n        bpe_tokens = [ti for t in tokens for ti in self.bpe(t)]\n\n        # Map to token ids and return\n        tokens = [self.vocab[t] for t in bpe_tokens]\n        if prepend_bos:\n            tokens = [self.bos_token] + tokens\n        if append_eos:\n            tokens.append(self.eos_token)\n\n        return tokens"}, {"role": "assistant", "content": "// Copyright \u00a9 2024 Apple Inc.\n\nimport Foundation\n\nstruct Bigram: Hashable {\n    let a: String\n    let b: String\n\n    init(_ s: String) {\n        let pieces = s.split(separator: \" \")\n        precondition(pieces.count == 2, \"BPEPair expected two pieces for '\\(s)'\")\n        self.a = String(pieces[0])\n        self.b = String(pieces[1])\n    }\n\n    init(_ a: String, _ b: String) {\n        self.a = a\n        self.b = b\n    }\n\n    init(_ v: (String, String)) {\n        self.a = v.0\n        self.b = v.1\n    }\n}\n\n/// A CLIP tokenizer.\n///\n/// Ported from:\n///\n/// - https://github.com/ml-explore/mlx-examples/blob/main/stable_diffusion/stable_diffusion/tokenizer.py\n/// - https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/tokenization_clip.py\n///\n/// Ideally this would be a tokenizer from `swift-transformers` but this is too special purpose to be representable in\n/// what exists there (at time of writing).\nclass CLIPTokenizer {\n\n    let pattern =\n        #/<\\|startoftext\\|\\u003e|<\\|endoftext\\|\\u003e|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+/#\n    let bpeRanks: [Bigram: Int]\n    let vocabulary: [String: Int]\n\n    let bos = \"\\u003c|startoftext|\\u003e\"\n    let eos = \"\\u003c|endoftext|\\u003e\"\n\n    let bosToken: Int\n    let eosToken: Int\n\n    var cache = [String: [String]]()\n\n    init(merges: [String], vocabulary: [String: Int]) {\n        self.bpeRanks = Dictionary(\n            uniqueKeysWithValues:\n                merges\n                .map { Bigram($0) }\n                .enumerated()\n                .map { ($0.element, $0.offset) })\n\n        self.vocabulary = vocabulary\n        self.cache[bos] = [bos]\n        self.cache[eos] = [eos]\n        self.bosToken = vocabulary[bos]!\n        self.eosToken = vocabulary[eos]!\n    }\n\n    func bpe(text: String) -> [String] {\n        if let result = cache[text] {\n            return result\n        }\n\n        precondition(!text.isEmpty)\n\n        var unigrams = text.dropLast().map { String($0) } + [\"\\(text.last!)</w>\"]\n        var uniqueBigrams = Set(zip(unigrams, unigrams.dropFirst()).map { Bigram($0) })\n\n        // In every iteration try to merge the two most likely bigrams. If none\n        // was merged we are done\n\n        while !uniqueBigrams.isEmpty {\n            let (bigram, _) =\n                uniqueBigrams\n                .map { ($0, bpeRanks[$0] ?? Int.max) }\n                .min { $0.1 < $1.1 }!\n\n            if bpeRanks[bigram] == nil {\n                break\n            }\n\n            var newUnigrams = [String]()\n            var skip = false\n\n            for (a, b) in zip(unigrams, unigrams.dropFirst()) {\n                if skip {\n                    skip = false\n                    continue\n                }\n\n                if Bigram(a, b) == bigram {\n                    newUnigrams.append(a + b)\n                    skip = true\n                } else {\n                    newUnigrams.append(a)\n                }\n            }\n\n            if !skip, let last = unigrams.last {\n                newUnigrams.append(last)\n            }\n\n            unigrams = newUnigrams\n            uniqueBigrams = Set(zip(unigrams, unigrams.dropFirst()).map { Bigram($0) })\n        }\n\n        cache[text] = unigrams\n\n        return unigrams\n    }\n\n    public func tokenize(text: String) -> [Int32] {\n        // Lower case cleanup and split according to self.pat. Hugging Face does\n        // a much more thorough job here but this should suffice for 95% of\n        // cases.\n\n        let clean = text.lowercased().replacing(#/\\s+/#, with: \" \")\n        let tokens = clean.matches(of: pattern).map { $0.description }\n\n        // Split the tokens according to the byte-pair merge file\n        let bpeTokens = tokens.flatMap { bpe(text: String($0)) }\n\n        // Map to token ids and return\n        let result = [bosToken] + bpeTokens.compactMap { vocabulary[$0] } + [eosToken]\n\n        return result.map { Int32($0) }\n    }\n}"}]}
{"messages": [{"role": "user", "content": "convert the following MLX Python code into MLX Swift: \n\n# Copyright \u00a9 2023 Apple Inc.\n\nimport math\nfrom typing import Optional\n\nimport mlx.core as mx\nimport mlx.nn as nn\n\nfrom .config import UNetConfig\n\n\ndef upsample_nearest(x, scale: int = 2):\n    B, H, W, C = x.shape\n    x = mx.broadcast_to(x[:, :, None, :, None, :], (B, H, scale, W, scale, C))\n    x = x.reshape(B, H * scale, W * scale, C)\n\n    return x\n\n\nclass TimestepEmbedding(nn.Module):\n    def __init__(self, in_channels: int, time_embed_dim: int):\n        super().__init__()\n\n        self.linear_1 = nn.Linear(in_channels, time_embed_dim)\n        self.linear_2 = nn.Linear(time_embed_dim, time_embed_dim)\n\n    def __call__(self, x):\n        x = self.linear_1(x)\n        x = nn.silu(x)\n        x = self.linear_2(x)\n\n        return x\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(\n        self,\n        model_dims: int,\n        num_heads: int,\n        hidden_dims: Optional[int] = None,\n        memory_dims: Optional[int] = None,\n    ):\n        super().__init__()\n\n        self.norm1 = nn.LayerNorm(model_dims)\n        self.attn1 = nn.MultiHeadAttention(model_dims, num_heads)\n        self.attn1.out_proj.bias = mx.zeros(model_dims)\n\n        memory_dims = memory_dims or model_dims\n        self.norm2 = nn.LayerNorm(model_dims)\n        self.attn2 = nn.MultiHeadAttention(\n            model_dims, num_heads, key_input_dims=memory_dims\n        )\n        self.attn2.out_proj.bias = mx.zeros(model_dims)\n\n        hidden_dims = hidden_dims or 4 * model_dims\n        self.norm3 = nn.LayerNorm(model_dims)\n        self.linear1 = nn.Linear(model_dims, hidden_dims)\n        self.linear2 = nn.Linear(model_dims, hidden_dims)\n        self.linear3 = nn.Linear(hidden_dims, model_dims)\n\n    def __call__(self, x, memory, attn_mask, memory_mask):\n        # Self attention\n        y = self.norm1(x)\n        y = self.attn1(y, y, y, attn_mask)\n        x = x + y\n\n        # Cross attention\n        y = self.norm2(x)\n        y = self.attn2(y, memory, memory, memory_mask)\n        x = x + y\n\n        # FFN\n        y = self.norm3(x)\n        y_a = self.linear1(y)\n        y_b = self.linear2(y)\n        y = y_a * nn.gelu(y_b)\n        y = self.linear3(y)\n        x = x + y\n\n        return x\n\n\nclass Transformer2D(nn.Module):\n    \"\"\"A transformer model for inputs with 2 spatial dimensions.\"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        model_dims: int,\n        encoder_dims: int,\n        num_heads: int,\n        num_layers: int = 1,\n        norm_num_groups: int = 32,\n    ):\n        super().__init__()\n\n        self.norm = nn.GroupNorm(norm_num_groups, in_channels, pytorch_compatible=True)\n        self.proj_in = nn.Linear(in_channels, model_dims)\n        self.transformer_blocks = [\n            TransformerBlock(model_dims, num_heads, memory_dims=encoder_dims)\n            for i in range(num_layers)\n        ]\n        self.proj_out = nn.Linear(model_dims, in_channels)\n\n    def __call__(self, x, encoder_x, attn_mask, encoder_attn_mask):\n        # Save the input to add to the output\n        input_x = x\n        dtype = x.dtype\n\n        # Perform the input norm and projection\n        B, H, W, C = x.shape\n        x = self.norm(x).reshape(B, -1, C)\n        x = self.proj_in(x)\n\n        # Apply the transformer\n        for block in self.transformer_blocks:\n            x = block(x, encoder_x, attn_mask, encoder_attn_mask)\n\n        # Apply the output projection and reshape\n        x = self.proj_out(x)\n        x = x.reshape(B, H, W, C)\n\n        return x + input_x\n\n\nclass ResnetBlock2D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: Optional[int] = None,\n        groups: int = 32,\n        temb_channels: Optional[int] = None,\n    ):\n        super().__init__()\n\n        out_channels = out_channels or in_channels\n\n        self.norm1 = nn.GroupNorm(groups, in_channels, pytorch_compatible=True)\n        self.conv1 = nn.Conv2d(\n            in_channels, out_channels, kernel_size=3, stride=1, padding=1\n        )\n        if temb_channels is not None:\n            self.time_emb_proj = nn.Linear(temb_channels, out_channels)\n        self.norm2 = nn.GroupNorm(groups, out_channels, pytorch_compatible=True)\n        self.conv2 = nn.Conv2d(\n            out_channels, out_channels, kernel_size=3, stride=1, padding=1\n        )\n\n        if in_channels != out_channels:\n            self.conv_shortcut = nn.Linear(in_channels, out_channels)\n\n    def __call__(self, x, temb=None):\n        dtype = x.dtype\n\n        if temb is not None:\n            temb = self.time_emb_proj(nn.silu(temb))\n\n        y = self.norm1(x)\n        y = nn.silu(y)\n        y = self.conv1(y)\n        if temb is not None:\n            y = y + temb[:, None, None, :]\n        y = self.norm2(y)\n        y = nn.silu(y)\n        y = self.conv2(y)\n\n        x = y + (x if \"conv_shortcut\" not in self else self.conv_shortcut(x))\n\n        return x\n\n\nclass UNetBlock2D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        temb_channels: int,\n        prev_out_channels: Optional[int] = None,\n        num_layers: int = 1,\n        transformer_layers_per_block: int = 1,\n        num_attention_heads: int = 8,\n        cross_attention_dim=1280,\n        resnet_groups: int = 32,\n        add_downsample=True,\n        add_upsample=True,\n        add_cross_attention=True,\n    ):\n        super().__init__()\n\n        # Prepare the in channels list for the resnets\n        if prev_out_channels is None:\n            in_channels_list = [in_channels] + [out_channels] * (num_layers - 1)\n        else:\n            in_channels_list = [prev_out_channels] + [out_channels] * (num_layers - 1)\n            res_channels_list = [out_channels] * (num_layers - 1) + [in_channels]\n            in_channels_list = [\n                a + b for a, b in zip(in_channels_list, res_channels_list)\n            ]\n\n        # Add resnet blocks that also process the time embedding\n        self.resnets = [\n            ResnetBlock2D(\n                in_channels=ic,\n                out_channels=out_channels,\n                temb_channels=temb_channels,\n                groups=resnet_groups,\n            )\n            for ic in in_channels_list\n        ]\n\n        # Add optional cross attention layers\n        if add_cross_attention:\n            self.attentions = [\n                Transformer2D(\n                    in_channels=out_channels,\n                    model_dims=out_channels,\n                    num_heads=num_attention_heads,\n                    num_layers=transformer_layers_per_block,\n                    encoder_dims=cross_attention_dim,\n                )\n                for i in range(num_layers)\n            ]\n\n        # Add an optional downsampling layer\n        if add_downsample:\n            self.downsample = nn.Conv2d(\n                out_channels, out_channels, kernel_size=3, stride=2, padding=1\n            )\n\n        # or upsampling layer\n        if add_upsample:\n            self.upsample = nn.Conv2d(\n                out_channels, out_channels, kernel_size=3, stride=1, padding=1\n            )\n\n    def __call__(\n        self,\n        x,\n        encoder_x=None,\n        temb=None,\n        attn_mask=None,\n        encoder_attn_mask=None,\n        residual_hidden_states=None,\n    ):\n        output_states = []\n\n        for i in range(len(self.resnets)):\n            if residual_hidden_states is not None:\n                x = mx.concatenate([x, residual_hidden_states.pop()], axis=-1)\n\n            x = self.resnets[i](x, temb)\n\n            if \"attentions\" in self:\n                x = self.attentions[i](x, encoder_x, attn_mask, encoder_attn_mask)\n\n            output_states.append(x)\n\n        if \"downsample\" in self:\n            x = self.downsample(x)\n            output_states.append(x)\n\n        if \"upsample\" in self:\n            x = self.upsample(upsample_nearest(x))\n            output_states.append(x)\n\n        return x, output_states\n\n\nclass UNetModel(nn.Module):\n    \"\"\"The conditional 2D UNet model that actually performs the denoising.\"\"\"\n\n    def __init__(self, config: UNetConfig):\n        super().__init__()\n\n        self.conv_in = nn.Conv2d(\n            config.in_channels,\n            config.block_out_channels[0],\n            config.conv_in_kernel,\n            padding=(config.conv_in_kernel - 1) // 2,\n        )\n\n        self.timesteps = nn.SinusoidalPositionalEncoding(\n            config.block_out_channels[0],\n            max_freq=1,\n            min_freq=math.exp(\n                -math.log(10000) + 2 * math.log(10000) / config.block_out_channels[0]\n            ),\n            scale=1.0,\n            cos_first=True,\n            full_turns=False,\n        )\n        self.time_embedding = TimestepEmbedding(\n            config.block_out_channels[0],\n            config.block_out_channels[0] * 4,\n        )\n\n        if config.addition_embed_type == \"text_time\":\n            self.add_time_proj = nn.SinusoidalPositionalEncoding(\n                config.addition_time_embed_dim,\n                max_freq=1,\n                min_freq=math.exp(\n                    -math.log(10000)\n                    + 2 * math.log(10000) / config.addition_time_embed_dim\n                ),\n                scale=1.0,\n                cos_first=True,\n                full_turns=False,\n            )\n            self.add_embedding = TimestepEmbedding(\n                config.projection_class_embeddings_input_dim,\n                config.block_out_channels[0] * 4,\n            )\n\n        # Make the downsampling blocks\n        block_channels = [config.block_out_channels[0]] + list(\n            config.block_out_channels\n        )\n        self.down_blocks = [\n            UNetBlock2D(\n                in_channels=in_channels,\n                out_channels=out_channels,\n                temb_channels=config.block_out_channels[0] * 4,\n                num_layers=config.layers_per_block[i],\n                transformer_layers_per_block=config.transformer_layers_per_block[i],\n                num_attention_heads=config.num_attention_heads[i],\n                cross_attention_dim=config.cross_attention_dim[i],\n                resnet_groups=config.norm_num_groups,\n                add_downsample=(i < len(config.block_out_channels) - 1),\n                add_upsample=False,\n                add_cross_attention=\"CrossAttn\" in config.down_block_types[i],\n            )\n            for i, (in_channels, out_channels) in enumerate(\n                zip(block_channels, block_channels[1:])\n            )\n        ]\n\n        # Make the middle block\n        self.mid_blocks = [\n            ResnetBlock2D(\n                in_channels=config.block_out_channels[-1],\n                out_channels=config.block_out_channels[-1],\n                temb_channels=config.block_out_channels[0] * 4,\n                groups=config.norm_num_groups,\n            ),\n            Transformer2D(\n                in_channels=config.block_out_channels[-1],\n                model_dims=config.block_out_channels[-1],\n                num_heads=config.num_attention_heads[-1],\n                num_layers=config.transformer_layers_per_block[-1],\n                encoder_dims=config.cross_attention_dim[-1],\n            ),\n            ResnetBlock2D(\n                in_channels=config.block_out_channels[-1],\n                out_channels=config.block_out_channels[-1],\n                temb_channels=config.block_out_channels[0] * 4,\n                groups=config.norm_num_groups,\n            ),\n        ]\n\n        # Make the upsampling blocks\n        block_channels = (\n            [config.block_out_channels[0]]\n            + list(config.block_out_channels)\n            + [config.block_out_channels[-1]]\n        )\n        self.up_blocks = [\n            UNetBlock2D(\n                in_channels=in_channels,\n                out_channels=out_channels,\n                temb_channels=config.block_out_channels[0] * 4,\n                prev_out_channels=prev_out_channels,\n                num_layers=config.layers_per_block[i] + 1,\n                transformer_layers_per_block=config.transformer_layers_per_block[i],\n                num_attention_heads=config.num_attention_heads[i],\n                cross_attention_dim=config.cross_attention_dim[i],\n                resnet_groups=config.norm_num_groups,\n                add_downsample=False,\n                add_upsample=(i > 0),\n                add_cross_attention=\"CrossAttn\" in config.up_block_types[i],\n            )\n            for i, (in_channels, out_channels, prev_out_channels) in reversed(\n                list(\n                    enumerate(\n                        zip(block_channels, block_channels[1:], block_channels[2:])\n                    )\n                )\n            )\n        ]\n\n        self.conv_norm_out = nn.GroupNorm(\n            config.norm_num_groups,\n            config.block_out_channels[0],\n            pytorch_compatible=True,\n        )\n        self.conv_out = nn.Conv2d(\n            config.block_out_channels[0],\n            config.out_channels,\n            config.conv_out_kernel,\n            padding=(config.conv_out_kernel - 1) // 2,\n        )\n\n    def __call__(\n        self,\n        x,\n        timestep,\n        encoder_x,\n        attn_mask=None,\n        encoder_attn_mask=None,\n        text_time=None,\n    ):\n        # Compute the time embeddings\n        temb = self.timesteps(timestep).astype(x.dtype)\n        temb = self.time_embedding(temb)\n\n        # Add the extra text_time conditioning\n        if text_time is not None:\n            text_emb, time_ids = text_time\n            emb = self.add_time_proj(time_ids).flatten(1).astype(x.dtype)\n            emb = mx.concatenate([text_emb, emb], axis=-1)\n            emb = self.add_embedding(emb)\n            temb = temb + emb\n\n        # Preprocess the input\n        x = self.conv_in(x)\n\n        # Run the downsampling part of the unet\n        residuals = [x]\n        for block in self.down_blocks:\n            x, res = block(\n                x,\n                encoder_x=encoder_x,\n                temb=temb,\n                attn_mask=attn_mask,\n                encoder_attn_mask=encoder_attn_mask,\n            )\n            residuals.extend(res)\n\n        # Run the middle part of the unet\n        x = self.mid_blocks[0](x, temb)\n        x = self.mid_blocks[1](x, encoder_x, attn_mask, encoder_attn_mask)\n        x = self.mid_blocks[2](x, temb)\n\n        # Run the upsampling part of the unet\n        for block in self.up_blocks:\n            x, _ = block(\n                x,\n                encoder_x=encoder_x,\n                temb=temb,\n                attn_mask=attn_mask,\n                encoder_attn_mask=encoder_attn_mask,\n                residual_hidden_states=residuals,\n            )\n\n        # Postprocess the output\n        x = self.conv_norm_out(x)\n        x = nn.silu(x)\n        x = self.conv_out(x)\n\n        return x"}, {"role": "assistant", "content": "// Copyright \u00a9 2024 Apple Inc.\n\nimport Foundation\nimport MLX\nimport MLXNN\n\n// port of https://github.com/ml-explore/mlx-examples/blob/main/stable_diffusion/stable_diffusion/unet.py\n\nfunc upsampleNearest(_ x: MLXArray, scale: Int = 2) -> MLXArray {\n    precondition(x.ndim == 4)\n    let (B, H, W, C) = x.shape4\n    var x = broadcast(\n        x[0..., 0..., .newAxis, 0..., .newAxis, 0...], to: [B, H, scale, W, scale, C])\n    x = x.reshaped(B, H * scale, W * scale, C)\n    return x\n}\n\nclass TimestepEmbedding: Module, UnaryLayer {\n\n    @ModuleInfo(key: \"linear_1\") var linear1: Linear\n    @ModuleInfo(key: \"linear_2\") var linear2: Linear\n\n    init(inputChannels: Int, timeEmbedDimensions: Int) {\n        self._linear1.wrappedValue = Linear(inputChannels, timeEmbedDimensions)\n        self._linear2.wrappedValue = Linear(timeEmbedDimensions, timeEmbedDimensions)\n    }\n\n    func callAsFunction(_ x: MLXArray) -> MLXArray {\n        var x = linear1(x)\n        x = silu(x)\n        x = linear2(x)\n\n        return x\n    }\n}\n\nclass TransformerBlock: Module {\n\n    let norm1: LayerNorm\n    let attn1: MultiHeadAttention\n\n    let norm2: LayerNorm\n    let attn2: MultiHeadAttention\n\n    let norm3: LayerNorm\n    @ModuleInfo var linear1: Linear\n    @ModuleInfo var linear2: Linear\n    @ModuleInfo var linear3: Linear\n\n    init(\n        modelDimensions: Int, numHeads: Int, hiddenDimensions: Int? = nil,\n        memoryDimensions: Int? = nil\n    ) {\n        norm1 = LayerNorm(dimensions: modelDimensions)\n        attn1 = MultiHeadAttention(dimensions: modelDimensions, numHeads: numHeads)\n\n        // we want to self.attn1.out_proj.bias = mx.zeros(model_dims) turn enable the\n        // bias in one of the four Linears attached to attn1. Since bias is nil we can't\n        // update it so just replace the layer.\n        attn1.update(\n            modules: ModuleChildren(\n                values: [\"out_proj\": .value(Linear(modelDimensions, modelDimensions, bias: true))]))\n\n        let memoryDimensions = memoryDimensions ?? modelDimensions\n        self.norm2 = LayerNorm(dimensions: modelDimensions)\n        self.attn2 = MultiHeadAttention(\n            dimensions: modelDimensions, numHeads: numHeads, keyInputDimensions: memoryDimensions)\n        attn2.update(\n            modules: ModuleChildren(\n                values: [\"out_proj\": .value(Linear(modelDimensions, modelDimensions, bias: true))]))\n\n        let hiddenDimensions = hiddenDimensions ?? (4 * modelDimensions)\n        self.norm3 = LayerNorm(dimensions: modelDimensions)\n        self.linear1 = Linear(modelDimensions, hiddenDimensions)\n        self.linear2 = Linear(modelDimensions, hiddenDimensions)\n        self.linear3 = Linear(hiddenDimensions, modelDimensions)\n    }\n\n    func callAsFunction(\n        _ x: MLXArray, memory: MLXArray, attentionMask: MLXArray?, memoryMask: MLXArray?\n    ) -> MLXArray {\n        var x = x\n\n        // self attention\n        var y = norm1(x)\n        y = attn1(y, keys: y, values: y, mask: attentionMask)\n        x = x + y\n\n        // cross attention\n        y = norm2(x)\n        y = attn2(y, keys: memory, values: memory, mask: memoryMask)\n        x = x + y\n\n        // FFN\n        y = norm3(x)\n        let ya = linear1(y)\n        let yb = linear2(y)\n        y = ya * gelu(yb)\n        y = linear3(y)\n        x = x + y\n\n        return x\n    }\n}\n\n/// A transformer model for inputs with 2 spatial dimensions\nclass Transformer2D: Module {\n\n    let norm: GroupNorm\n    @ModuleInfo(key: \"proj_in\") var projectIn: Linear\n    @ModuleInfo(key: \"transformer_blocks\") var transformerBlocks: [TransformerBlock]\n    @ModuleInfo(key: \"proj_out\") var projectOut: Linear\n\n    init(\n        inputChannels: Int, modelDimensions: Int, encoderDimensions: Int, numHeads: Int,\n        numLayers: Int, groupCount: Int = 32\n    ) {\n        self.norm = GroupNorm(\n            groupCount: groupCount, dimensions: inputChannels, pytorchCompatible: true)\n        self._projectIn.wrappedValue = Linear(inputChannels, modelDimensions)\n        self._transformerBlocks.wrappedValue = (0 ..< numLayers)\n            .map { _ in\n                TransformerBlock(\n                    modelDimensions: modelDimensions, numHeads: numHeads,\n                    memoryDimensions: encoderDimensions)\n            }\n        self._projectOut.wrappedValue = Linear(modelDimensions, inputChannels)\n    }\n\n    func callAsFunction(\n        _ x: MLXArray, encoderX: MLXArray, attentionMask: MLXArray?, encoderAttentionMask: MLXArray?\n    ) -> MLXArray {\n        let inputX = x\n        let dtype = x.dtype\n        var x = x\n\n        // Perform the input norm and projection\n        let (B, H, W, C) = x.shape4\n        x = norm(x).reshaped(B, -1, C)\n        x = projectIn(x)\n\n        // apply the transformer\n        for block in transformerBlocks {\n            x = block(\n                x, memory: encoderX, attentionMask: attentionMask, memoryMask: encoderAttentionMask)\n        }\n\n        // apply the output projection and reshape\n        x = projectOut(x)\n        x = x.reshaped(B, H, W, C)\n\n        return x + inputX\n    }\n}\n\nclass ResnetBlock2D: Module {\n\n    let norm1: GroupNorm\n    let conv1: Conv2d\n\n    @ModuleInfo(key: \"time_emb_proj\") var timeEmbedProjection: Linear?\n\n    let norm2: GroupNorm\n    let conv2: Conv2d\n\n    @ModuleInfo(key: \"conv_shortcut\") var convolutionShortcut: Linear?\n\n    init(\n        inputChannels: Int, outputChannels: Int? = nil, groupCount: Int = 32,\n        timeEmbedChannels: Int? = nil\n    ) {\n        let outputChannels = outputChannels ?? inputChannels\n\n        self.norm1 = GroupNorm(\n            groupCount: groupCount, dimensions: inputChannels, pytorchCompatible: true)\n        self.conv1 = Conv2d(\n            inputChannels: inputChannels, outputChannels: outputChannels,\n            kernelSize: 3, stride: 1, padding: 1)\n\n        if let timeEmbedChannels {\n            self._timeEmbedProjection.wrappedValue = Linear(timeEmbedChannels, outputChannels)\n        }\n\n        self.norm2 = GroupNorm(\n            groupCount: groupCount, dimensions: outputChannels, pytorchCompatible: true)\n        self.conv2 = Conv2d(\n            inputChannels: outputChannels, outputChannels: outputChannels,\n            kernelSize: 3, stride: 1, padding: 1)\n\n        if inputChannels != outputChannels {\n            self._convolutionShortcut.wrappedValue = Linear(inputChannels, outputChannels)\n        }\n    }\n\n    func callAsFunction(_ x: MLXArray, timeEmbedding: MLXArray? = nil) -> MLXArray {\n        let dtype = x.dtype\n\n        var y = norm1(x)\n        y = silu(y)\n        y = conv1(y)\n\n        if var timeEmbedding, let timeEmbedProjection {\n            timeEmbedding = timeEmbedProjection(silu(timeEmbedding))\n            y = y + timeEmbedding[0..., .newAxis, .newAxis, 0...]\n        }\n\n        y = norm2(y)\n        y = silu(y)\n        y = conv2(y)\n\n        if let convolutionShortcut {\n            return y + convolutionShortcut(x)\n        } else {\n            return y + x\n        }\n    }\n}\n\nclass UNetBlock2D: Module {\n\n    let resnets: [ResnetBlock2D]\n    let attentions: [Transformer2D]?\n    let downsample: Conv2d?\n    let upsample: Conv2d?\n\n    init(\n        inputChannels: Int, outputChannels: Int, timeEmbedChannels: Int,\n        previousOutChannels: Int? = nil, numLayers: Int = 1, transformerLayersPerBlock: Int = 1,\n        numHeads: Int = 8, crossAttentionDimension: Int = 1280, resnetGroups: Int = 32,\n        addDownSample: Bool = true, addUpSample: Bool = true, addCrossAttention: Bool = true\n    ) {\n\n        // Prepare the inputChannelsArray for the resnets\n        let inputChannelsArray: [Int]\n        if let previousOutChannels {\n            let inputChannelsBuild =\n                [previousOutChannels] + Array(repeating: outputChannels, count: numLayers - 1)\n            let resChannelsArray =\n                Array(repeating: outputChannels, count: numLayers - 1) + [inputChannels]\n            inputChannelsArray = zip(inputChannelsBuild, resChannelsArray).map { $0.0 + $0.1 }\n        } else {\n            inputChannelsArray =\n                [inputChannels] + Array(repeating: outputChannels, count: numLayers - 1)\n        }\n\n        // Add resnet blocks that also process the time embedding\n        self.resnets =\n            inputChannelsArray\n            .map { ic in\n                ResnetBlock2D(\n                    inputChannels: ic, outputChannels: outputChannels, groupCount: resnetGroups,\n                    timeEmbedChannels: timeEmbedChannels)\n            }\n\n        // Add optional cross attention layers\n        if addCrossAttention {\n            self.attentions = (0 ..< numLayers)\n                .map { _ in\n                    Transformer2D(\n                        inputChannels: outputChannels, modelDimensions: outputChannels,\n                        encoderDimensions: crossAttentionDimension, numHeads: numHeads,\n                        numLayers: transformerLayersPerBlock)\n                }\n        } else {\n            self.attentions = nil\n        }\n\n        // Add an optional downsampling layer\n        if addDownSample {\n            self.downsample = Conv2d(\n                inputChannels: outputChannels, outputChannels: outputChannels, kernelSize: 3,\n                stride: 2, padding: 1)\n        } else {\n            self.downsample = nil\n        }\n\n        // or upsampling layer\n        if addUpSample {\n            self.upsample = Conv2d(\n                inputChannels: outputChannels, outputChannels: outputChannels, kernelSize: 3,\n                stride: 1, padding: 1)\n        } else {\n            self.upsample = nil\n        }\n    }\n\n    func callAsFunction(\n        _ x: MLXArray, encoderX: MLXArray, timeEmbedding: MLXArray? = nil,\n        attentionMask: MLXArray? = nil, encoderAttentionMask: MLXArray? = nil,\n        residualHiddenStates: [MLXArray]? = nil\n    ) -> (MLXArray, [MLXArray], [MLXArray]) {\n        var x = x\n        var outputStates = [MLXArray]()\n        var residualHiddenStates = residualHiddenStates\n\n        for i in 0 ..< resnets.count {\n            if residualHiddenStates != nil {\n                x = concatenated([x, residualHiddenStates!.removeLast()], axis: -1)\n            }\n\n            x = resnets[i](x, timeEmbedding: timeEmbedding)\n\n            if let attentions {\n                x = attentions[i](\n                    x, encoderX: encoderX, attentionMask: attentionMask,\n                    encoderAttentionMask: encoderAttentionMask)\n            }\n\n            outputStates.append(x)\n        }\n\n        if let downsample {\n            x = downsample(x)\n            outputStates.append(x)\n        }\n\n        if let upsample {\n            x = upsample(upsampleNearest(x))\n            outputStates.append(x)\n        }\n\n        if let residualHiddenStates {\n            return (x, outputStates, residualHiddenStates)\n        } else {\n            return (x, outputStates, [])\n        }\n    }\n}\n\nclass UNetModel: Module {\n\n    @ModuleInfo(key: \"conv_in\") var convIn: Conv2d\n    let timesteps: SinusoidalPositionalEncoding\n    @ModuleInfo(key: \"time_embedding\") var timeEmbedding: TimestepEmbedding\n\n    @ModuleInfo(key: \"addition_embed_type\") var addTimeProj: SinusoidalPositionalEncoding?\n    @ModuleInfo(key: \"add_embedding\") var addEmbedding: TimestepEmbedding?\n\n    @ModuleInfo(key: \"down_blocks\") var downBlocks: [UNetBlock2D]\n    @ModuleInfo(key: \"mid_blocks\") var midBlocks: (ResnetBlock2D, Transformer2D, ResnetBlock2D)\n    @ModuleInfo(key: \"up_blocks\") var upBlocks: [UNetBlock2D]\n\n    @ModuleInfo(key: \"conv_norm_out\") var convNormOut: GroupNorm\n    @ModuleInfo(key: \"conv_out\") var convOut: Conv2d\n\n    init(configuration: UNetConfiguration) {\n        let channels0 = configuration.blockOutChannels[0]\n\n        self._convIn.wrappedValue = Conv2d(\n            inputChannels: configuration.inputChannels, outputChannels: channels0,\n            kernelSize: .init(configuration.convolutionInKernel),\n            padding: .init((configuration.convolutionInKernel - 1) / 2))\n\n        self.timesteps = SinusoidalPositionalEncoding(\n            dimensions: channels0,\n            minFrequency: exp(-log(10_000) + 2 * log(10_000) / Float(channels0)),\n            maxFrequency: 1, scale: 1, cosineFirst: true, fullTurns: false)\n\n        self._timeEmbedding.wrappedValue = TimestepEmbedding(\n            inputChannels: channels0, timeEmbedDimensions: channels0 * 4)\n\n        if configuration.additionEmbedType == \"text_time\",\n            let additionTimeEmbedDimension = configuration.additionTimeEmbedDimension,\n            let projectionClassEmbeddingsInputDimension = configuration\n                .projectionClassEmbeddingsInputDimension\n        {\n            self._addTimeProj.wrappedValue = SinusoidalPositionalEncoding(\n                dimensions: additionTimeEmbedDimension,\n                minFrequency: exp(\n                    -log(10_000) + 2 * log(10_000) / Float(additionTimeEmbedDimension)),\n                maxFrequency: 1,\n                scale: 1, cosineFirst: true, fullTurns: false)\n\n            self._addEmbedding.wrappedValue = TimestepEmbedding(\n                inputChannels: projectionClassEmbeddingsInputDimension,\n                timeEmbedDimensions: channels0 * 4)\n        }\n\n        // make the downsampling blocks\n        let downblockChannels = [channels0] + configuration.blockOutChannels\n        self._downBlocks.wrappedValue = zip(downblockChannels, downblockChannels.dropFirst())\n            .enumerated()\n            .map { (i, pair) in\n                let (inChannels, outChannels) = pair\n                return UNetBlock2D(\n                    inputChannels: inChannels,\n                    outputChannels: outChannels,\n                    timeEmbedChannels: channels0 * 4,\n                    numLayers: configuration.layersPerBlock[i],\n                    transformerLayersPerBlock: configuration.transformerLayersPerBlock[i],\n                    numHeads: configuration.numHeads[i],\n                    crossAttentionDimension: configuration.crossAttentionDimension[i],\n                    resnetGroups: configuration.normNumGroups,\n                    addDownSample: i < configuration.blockOutChannels.count - 1,\n                    addUpSample: false,\n                    addCrossAttention: configuration.downBlockTypes[i].contains(\"CrossAttn\")\n                )\n            }\n\n        // make the middle block\n        let channelsLast = configuration.blockOutChannels.last!\n        self._midBlocks.wrappedValue = (\n            ResnetBlock2D(\n                inputChannels: channelsLast,\n                outputChannels: channelsLast,\n                groupCount: configuration.normNumGroups,\n                timeEmbedChannels: channels0 * 4\n            ),\n            Transformer2D(\n                inputChannels: channelsLast,\n                modelDimensions: channelsLast,\n                encoderDimensions: configuration.crossAttentionDimension.last!,\n                numHeads: configuration.numHeads.last!,\n                numLayers: configuration.transformerLayersPerBlock.last!\n            ),\n            ResnetBlock2D(\n                inputChannels: channelsLast,\n                outputChannels: channelsLast,\n                groupCount: configuration.normNumGroups,\n                timeEmbedChannels: channels0 * 4\n            )\n        )\n\n        // make the upsampling blocks\n        let upblockChannels =\n            [channels0] + configuration.blockOutChannels + [configuration.blockOutChannels.last!]\n        self._upBlocks.wrappedValue =\n            zip(upblockChannels, zip(upblockChannels.dropFirst(), upblockChannels.dropFirst(2)))\n            .enumerated()\n            .reversed()\n            .map { (i, triple) in\n                let (inChannels, (outChannels, prevOutChannels)) = triple\n                return UNetBlock2D(\n                    inputChannels: inChannels,\n                    outputChannels: outChannels,\n                    timeEmbedChannels: channels0 * 4,\n                    previousOutChannels: prevOutChannels,\n                    numLayers: configuration.layersPerBlock[i] + 1,\n                    transformerLayersPerBlock: configuration.transformerLayersPerBlock[i],\n                    numHeads: configuration.numHeads[i],\n                    crossAttentionDimension: configuration.crossAttentionDimension[i],\n                    resnetGroups: configuration.normNumGroups,\n                    addDownSample: false,\n                    addUpSample: i > 0,\n                    addCrossAttention: configuration.upBlockTypes[i].contains(\"CrossAttn\")\n                )\n            }\n\n        self._convNormOut.wrappedValue = GroupNorm(\n            groupCount: configuration.normNumGroups, dimensions: channels0, pytorchCompatible: true)\n        self._convOut.wrappedValue = Conv2d(\n            inputChannels: channels0, outputChannels: configuration.outputChannels,\n            kernelSize: .init(configuration.convolutionOutKernel),\n            padding: .init((configuration.convolutionOutKernel - 1) / 2))\n    }\n\n    func callAsFunction(\n        _ x: MLXArray, timestep: MLXArray, encoderX: MLXArray, attentionMask: MLXArray? = nil,\n        encoderAttentionMask: MLXArray? = nil, textTime: (MLXArray, MLXArray)? = nil\n    ) -> MLXArray {\n        // compute the time embeddings\n        var temb = timesteps(timestep).asType(x.dtype)\n        temb = timeEmbedding(temb)\n\n        // add the extra textTime conditioning\n        if let (textEmbedding, timeIds) = textTime,\n            let addTimeProj, let addEmbedding\n        {\n            var emb = addTimeProj(timeIds).flattened(start: 1).asType(x.dtype)\n            emb = concatenated([textEmbedding, emb], axis: -1)\n            emb = addEmbedding(emb)\n            temb = temb + emb\n        }\n\n        // preprocess the input\n        var x = convIn(x)\n\n        // run the downsampling part of the unet\n        var residuals = [x]\n        for block in self.downBlocks {\n            let res: [MLXArray]\n            (x, res, _) = block(\n                x, encoderX: encoderX, timeEmbedding: temb, attentionMask: attentionMask,\n                encoderAttentionMask: encoderAttentionMask)\n            residuals.append(contentsOf: res)\n        }\n\n        // run the middle part of the unet\n        x = midBlocks.0(x, timeEmbedding: temb)\n        x = midBlocks.1(\n            x, encoderX: encoderX, attentionMask: attentionMask,\n            encoderAttentionMask: encoderAttentionMask)\n        x = midBlocks.2(x, timeEmbedding: temb)\n\n        // run the upsampling part of the unet\n        for block in self.upBlocks {\n            (x, _, residuals) = block(\n                x, encoderX: encoderX, timeEmbedding: temb, attentionMask: attentionMask,\n                encoderAttentionMask: encoderAttentionMask, residualHiddenStates: residuals)\n        }\n\n        // postprocess the output\n        let dtype = x.dtype\n        x = convNormOut(x)\n        x = silu(x)\n        x = convOut(x)\n\n        return x\n    }\n}"}]}
{"messages": [{"role": "user", "content": "convert the following MLX Python code into MLX Swift: \n\n# Copyright \u00a9 2023 Apple Inc.\n\nimport math\nfrom typing import List\n\nimport mlx.core as mx\nimport mlx.nn as nn\n\nfrom .config import AutoencoderConfig\nfrom .unet import ResnetBlock2D, upsample_nearest\n\n\nclass Attention(nn.Module):\n    \"\"\"A single head unmasked attention for use with the VAE.\"\"\"\n\n    def __init__(self, dims: int, norm_groups: int = 32):\n        super().__init__()\n\n        self.group_norm = nn.GroupNorm(norm_groups, dims, pytorch_compatible=True)\n        self.query_proj = nn.Linear(dims, dims)\n        self.key_proj = nn.Linear(dims, dims)\n        self.value_proj = nn.Linear(dims, dims)\n        self.out_proj = nn.Linear(dims, dims)\n\n    def __call__(self, x):\n        B, H, W, C = x.shape\n\n        y = self.group_norm(x)\n\n        queries = self.query_proj(y).reshape(B, H * W, C)\n        keys = self.key_proj(y).reshape(B, H * W, C)\n        values = self.value_proj(y).reshape(B, H * W, C)\n\n        scale = 1 / math.sqrt(queries.shape[-1])\n        scores = (queries * scale) @ keys.transpose(0, 2, 1)\n        attn = mx.softmax(scores, axis=-1)\n        y = (attn @ values).reshape(B, H, W, C)\n\n        y = self.out_proj(y)\n        x = x + y\n\n        return x\n\n\nclass EncoderDecoderBlock2D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        num_layers: int = 1,\n        resnet_groups: int = 32,\n        add_downsample=True,\n        add_upsample=True,\n    ):\n        super().__init__()\n\n        # Add the resnet blocks\n        self.resnets = [\n            ResnetBlock2D(\n                in_channels=in_channels if i == 0 else out_channels,\n                out_channels=out_channels,\n                groups=resnet_groups,\n            )\n            for i in range(num_layers)\n        ]\n\n        # Add an optional downsampling layer\n        if add_downsample:\n            self.downsample = nn.Conv2d(\n                out_channels, out_channels, kernel_size=3, stride=2, padding=0\n            )\n\n        # or upsampling layer\n        if add_upsample:\n            self.upsample = nn.Conv2d(\n                out_channels, out_channels, kernel_size=3, stride=1, padding=1\n            )\n\n    def __call__(self, x):\n        for resnet in self.resnets:\n            x = resnet(x)\n\n        if \"downsample\" in self:\n            x = mx.pad(x, [(0, 0), (0, 1), (0, 1), (0, 0)])\n            x = self.downsample(x)\n\n        if \"upsample\" in self:\n            x = self.upsample(upsample_nearest(x))\n\n        return x\n\n\nclass Encoder(nn.Module):\n    \"\"\"Implements the encoder side of the Autoencoder.\"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        block_out_channels: List[int] = [64],\n        layers_per_block: int = 2,\n        resnet_groups: int = 32,\n    ):\n        super().__init__()\n\n        self.conv_in = nn.Conv2d(\n            in_channels, block_out_channels[0], kernel_size=3, stride=1, padding=1\n        )\n\n        channels = [block_out_channels[0]] + list(block_out_channels)\n        self.down_blocks = [\n            EncoderDecoderBlock2D(\n                in_channels,\n                out_channels,\n                num_layers=layers_per_block,\n                resnet_groups=resnet_groups,\n                add_downsample=i < len(block_out_channels) - 1,\n                add_upsample=False,\n            )\n            for i, (in_channels, out_channels) in enumerate(zip(channels, channels[1:]))\n        ]\n\n        self.mid_blocks = [\n            ResnetBlock2D(\n                in_channels=block_out_channels[-1],\n                out_channels=block_out_channels[-1],\n                groups=resnet_groups,\n            ),\n            Attention(block_out_channels[-1], resnet_groups),\n            ResnetBlock2D(\n                in_channels=block_out_channels[-1],\n                out_channels=block_out_channels[-1],\n                groups=resnet_groups,\n            ),\n        ]\n\n        self.conv_norm_out = nn.GroupNorm(\n            resnet_groups, block_out_channels[-1], pytorch_compatible=True\n        )\n        self.conv_out = nn.Conv2d(block_out_channels[-1], out_channels, 3, padding=1)\n\n    def __call__(self, x):\n        x = self.conv_in(x)\n\n        for l in self.down_blocks:\n            x = l(x)\n\n        x = self.mid_blocks[0](x)\n        x = self.mid_blocks[1](x)\n        x = self.mid_blocks[2](x)\n\n        x = self.conv_norm_out(x)\n        x = nn.silu(x)\n        x = self.conv_out(x)\n\n        return x\n\n\nclass Decoder(nn.Module):\n    \"\"\"Implements the decoder side of the Autoencoder.\"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        block_out_channels: List[int] = [64],\n        layers_per_block: int = 2,\n        resnet_groups: int = 32,\n    ):\n        super().__init__()\n\n        self.conv_in = nn.Conv2d(\n            in_channels, block_out_channels[-1], kernel_size=3, stride=1, padding=1\n        )\n\n        self.mid_blocks = [\n            ResnetBlock2D(\n                in_channels=block_out_channels[-1],\n                out_channels=block_out_channels[-1],\n                groups=resnet_groups,\n            ),\n            Attention(block_out_channels[-1], resnet_groups),\n            ResnetBlock2D(\n                in_channels=block_out_channels[-1],\n                out_channels=block_out_channels[-1],\n                groups=resnet_groups,\n            ),\n        ]\n\n        channels = list(reversed(block_out_channels))\n        channels = [channels[0]] + channels\n        self.up_blocks = [\n            EncoderDecoderBlock2D(\n                in_channels,\n                out_channels,\n                num_layers=layers_per_block,\n                resnet_groups=resnet_groups,\n                add_downsample=False,\n                add_upsample=i < len(block_out_channels) - 1,\n            )\n            for i, (in_channels, out_channels) in enumerate(zip(channels, channels[1:]))\n        ]\n\n        self.conv_norm_out = nn.GroupNorm(\n            resnet_groups, block_out_channels[0], pytorch_compatible=True\n        )\n        self.conv_out = nn.Conv2d(block_out_channels[0], out_channels, 3, padding=1)\n\n    def __call__(self, x):\n        x = self.conv_in(x)\n\n        x = self.mid_blocks[0](x)\n        x = self.mid_blocks[1](x)\n        x = self.mid_blocks[2](x)\n\n        for l in self.up_blocks:\n            x = l(x)\n\n        x = self.conv_norm_out(x)\n        x = nn.silu(x)\n        x = self.conv_out(x)\n\n        return x\n\n\nclass Autoencoder(nn.Module):\n    \"\"\"The autoencoder that allows us to perform diffusion in the latent space.\"\"\"\n\n    def __init__(self, config: AutoencoderConfig):\n        super().__init__()\n\n        self.latent_channels = config.latent_channels_in\n        self.scaling_factor = config.scaling_factor\n        self.encoder = Encoder(\n            config.in_channels,\n            config.latent_channels_out,\n            config.block_out_channels,\n            config.layers_per_block,\n            resnet_groups=config.norm_num_groups,\n        )\n        self.decoder = Decoder(\n            config.latent_channels_in,\n            config.out_channels,\n            config.block_out_channels,\n            config.layers_per_block + 1,\n            resnet_groups=config.norm_num_groups,\n        )\n\n        self.quant_proj = nn.Linear(\n            config.latent_channels_out, config.latent_channels_out\n        )\n        self.post_quant_proj = nn.Linear(\n            config.latent_channels_in, config.latent_channels_in\n        )\n\n    def decode(self, z):\n        z = z / self.scaling_factor\n        return self.decoder(self.post_quant_proj(z))\n\n    def encode(self, x):\n        x = self.encoder(x)\n        x = self.quant_proj(x)\n        mean, logvar = x.split(2, axis=-1)\n        mean = mean * self.scaling_factor\n        logvar = logvar + 2 * math.log(self.scaling_factor)\n\n        return mean, logvar\n\n    def __call__(self, x, key=None):\n        mean, logvar = self.encode(x)\n        z = mx.random.normal(mean.shape, key=key) * mx.exp(0.5 * logvar) + mean\n        x_hat = self.decode(z)\n\n        return dict(x_hat=x_hat, z=z, mean=mean, logvar=logvar)"}, {"role": "assistant", "content": "// Copyright \u00a9 2024 Apple Inc.\n\nimport Foundation\nimport MLX\nimport MLXNN\n\n// port of https://github.com/ml-explore/mlx-examples/blob/main/stable_diffusion/stable_diffusion/vae.py\n\nclass Attention: Module, UnaryLayer {\n\n    @ModuleInfo(key: \"group_norm\") public var groupNorm: GroupNorm\n\n    @ModuleInfo(key: \"query_proj\") public var queryProjection: Linear\n    @ModuleInfo(key: \"key_proj\") public var keyProjection: Linear\n    @ModuleInfo(key: \"value_proj\") public var valueProjection: Linear\n    @ModuleInfo(key: \"out_proj\") public var outProjection: Linear\n\n    init(dimensions: Int, groupCount: Int = 32) {\n        self._groupNorm.wrappedValue = GroupNorm(\n            groupCount: groupCount, dimensions: dimensions, pytorchCompatible: true)\n\n        self._queryProjection.wrappedValue = Linear(dimensions, dimensions)\n        self._keyProjection.wrappedValue = Linear(dimensions, dimensions)\n        self._valueProjection.wrappedValue = Linear(dimensions, dimensions)\n        self._outProjection.wrappedValue = Linear(dimensions, dimensions)\n    }\n\n    func callAsFunction(_ x: MLXArray) -> MLXArray {\n        let (B, H, W, C) = x.shape4\n\n        var y = groupNorm(x)\n\n        let queries = queryProjection(y).reshaped(B, H * W, C)\n        let keys = keyProjection(y).reshaped(B, H * W, C)\n        let values = valueProjection(y).reshaped(B, H * W, C)\n\n        let scale = 1 / sqrt(Float(queries.dim(-1)))\n        let scores = (queries * scale).matmul(keys.transposed(0, 2, 1))\n        let attention = softmax(scores, axis: -1)\n\n        y = matmul(attention, values).reshaped(B, H, W, C)\n        y = outProjection(y)\n\n        return x + y\n    }\n}\n\nclass EncoderDecoderBlock2D: Module, UnaryLayer {\n\n    let resnets: [ResnetBlock2D]\n    let downsample: Conv2d?\n    let upsample: Conv2d?\n\n    init(\n        inputChannels: Int, outputChannels: Int, numLayers: Int = 1, resnetGroups: Int = 32,\n        addDownSample: Bool = true, addUpSample: Bool = true\n    ) {\n        // Add the resnet blocks\n        self.resnets = (0 ..< numLayers)\n            .map { i in\n                ResnetBlock2D(\n                    inputChannels: i == 0 ? inputChannels : outputChannels,\n                    outputChannels: outputChannels,\n                    groupCount: resnetGroups)\n            }\n\n        // Add an optional downsampling layer\n        if addDownSample {\n            self.downsample = Conv2d(\n                inputChannels: outputChannels, outputChannels: outputChannels, kernelSize: 3,\n                stride: 2, padding: 0)\n        } else {\n            self.downsample = nil\n        }\n\n        // or upsampling layer\n        if addUpSample {\n            self.upsample = Conv2d(\n                inputChannels: outputChannels, outputChannels: outputChannels, kernelSize: 3,\n                stride: 1, padding: 1)\n        } else {\n            self.upsample = nil\n        }\n    }\n\n    func callAsFunction(_ x: MLXArray) -> MLXArray {\n        var x = x\n\n        for resnet in resnets {\n            x = resnet(x)\n        }\n\n        if let downsample {\n            x = padded(x, widths: [[0, 0], [0, 1], [0, 1], [0, 0]])\n            x = downsample(x)\n        }\n\n        if let upsample {\n            x = upsample(upsampleNearest(x))\n        }\n\n        return x\n    }\n}\n\n/// Implements the encoder side of the Autoencoder\nclass VAEncoder: Module, UnaryLayer {\n\n    @ModuleInfo(key: \"conv_in\") var convIn: Conv2d\n    @ModuleInfo(key: \"down_blocks\") var downBlocks: [EncoderDecoderBlock2D]\n    @ModuleInfo(key: \"mid_blocks\") var midBlocks: (ResnetBlock2D, Attention, ResnetBlock2D)\n    @ModuleInfo(key: \"conv_norm_out\") var convNormOut: GroupNorm\n    @ModuleInfo(key: \"conv_out\") var convOut: Conv2d\n\n    init(\n        inputChannels: Int, outputChannels: Int, blockOutChannels: [Int] = [64],\n        layersPerBlock: Int = 2, resnetGroups: Int = 32\n    ) {\n        let channels0 = blockOutChannels[0]\n\n        self._convIn.wrappedValue = Conv2d(\n            inputChannels: inputChannels, outputChannels: channels0, kernelSize: 3, stride: 1,\n            padding: 1)\n\n        let downblockChannels = [channels0] + blockOutChannels\n        self._downBlocks.wrappedValue = zip(downblockChannels, downblockChannels.dropFirst())\n            .enumerated()\n            .map { (i, pair) in\n                let (inChannels, outChannels) = pair\n                return EncoderDecoderBlock2D(\n                    inputChannels: inChannels, outputChannels: outChannels,\n                    numLayers: layersPerBlock, resnetGroups: resnetGroups,\n                    addDownSample: i < blockOutChannels.count - 1,\n                    addUpSample: false\n                )\n            }\n\n        let channelsLast = blockOutChannels.last!\n        self._midBlocks.wrappedValue = (\n            ResnetBlock2D(\n                inputChannels: channelsLast,\n                outputChannels: channelsLast,\n                groupCount: resnetGroups\n            ),\n            Attention(dimensions: channelsLast, groupCount: resnetGroups),\n            ResnetBlock2D(\n                inputChannels: channelsLast,\n                outputChannels: channelsLast,\n                groupCount: resnetGroups\n            )\n        )\n\n        self._convNormOut.wrappedValue = GroupNorm(\n            groupCount: resnetGroups, dimensions: channelsLast, pytorchCompatible: true)\n        self._convOut.wrappedValue = Conv2d(\n            inputChannels: channelsLast, outputChannels: outputChannels,\n            kernelSize: 3,\n            padding: 1)\n    }\n\n    func callAsFunction(_ x: MLXArray) -> MLXArray {\n        var x = convIn(x)\n\n        for l in downBlocks {\n            x = l(x)\n        }\n\n        x = midBlocks.0(x)\n        x = midBlocks.1(x)\n        x = midBlocks.2(x)\n\n        x = convNormOut(x)\n        x = silu(x)\n        x = convOut(x)\n\n        return x\n    }\n}\n\n/// Implements the decoder side of the Autoencoder\nclass VADecoder: Module, UnaryLayer {\n\n    @ModuleInfo(key: \"conv_in\") var convIn: Conv2d\n    @ModuleInfo(key: \"mid_blocks\") var midBlocks: (ResnetBlock2D, Attention, ResnetBlock2D)\n    @ModuleInfo(key: \"up_blocks\") var upBlocks: [EncoderDecoderBlock2D]\n    @ModuleInfo(key: \"conv_norm_out\") var convNormOut: GroupNorm\n    @ModuleInfo(key: \"conv_out\") var convOut: Conv2d\n\n    init(\n        inputChannels: Int, outputChannels: Int, blockOutChannels: [Int] = [64],\n        layersPerBlock: Int = 2, resnetGroups: Int = 32\n    ) {\n        let channels0 = blockOutChannels[0]\n        let channelsLast = blockOutChannels.last!\n\n        self._convIn.wrappedValue = Conv2d(\n            inputChannels: inputChannels, outputChannels: channelsLast, kernelSize: 3, stride: 1,\n            padding: 1)\n\n        self._midBlocks.wrappedValue = (\n            ResnetBlock2D(\n                inputChannels: channelsLast,\n                outputChannels: channelsLast,\n                groupCount: resnetGroups\n            ),\n            Attention(dimensions: channelsLast, groupCount: resnetGroups),\n            ResnetBlock2D(\n                inputChannels: channelsLast,\n                outputChannels: channelsLast,\n                groupCount: resnetGroups\n            )\n        )\n\n        let channels = [channelsLast] + blockOutChannels.reversed()\n        self._upBlocks.wrappedValue = zip(channels, channels.dropFirst())\n            .enumerated()\n            .map { (i, pair) in\n                let (inChannels, outChannels) = pair\n                return EncoderDecoderBlock2D(\n                    inputChannels: inChannels,\n                    outputChannels: outChannels,\n                    numLayers: layersPerBlock,\n                    resnetGroups: resnetGroups,\n                    addDownSample: false,\n                    addUpSample: i < blockOutChannels.count - 1\n                )\n            }\n\n        self._convNormOut.wrappedValue = GroupNorm(\n            groupCount: resnetGroups, dimensions: channels0, pytorchCompatible: true)\n        self._convOut.wrappedValue = Conv2d(\n            inputChannels: channels0, outputChannels: outputChannels,\n            kernelSize: 3,\n            padding: 1)\n    }\n\n    func callAsFunction(_ x: MLXArray) -> MLXArray {\n        var x = convIn(x)\n\n        x = midBlocks.0(x)\n        x = midBlocks.1(x)\n        x = midBlocks.2(x)\n\n        for l in upBlocks {\n            x = l(x)\n        }\n\n        x = convNormOut(x)\n        x = silu(x)\n        x = convOut(x)\n\n        return x\n    }\n}\n\n/// The autoencoder that allows us to perform diffusion in the latent space\nclass Autoencoder: Module {\n\n    let latentChannels: Int\n    let scalingFactor: Float\n    let encoder: VAEncoder\n    let decoder: VADecoder\n\n    @ModuleInfo(key: \"quant_proj\") public var quantProjection: Linear\n    @ModuleInfo(key: \"post_quant_proj\") public var postQuantProjection: Linear\n\n    init(configuration: AutoencoderConfiguration) {\n        self.latentChannels = configuration.latentChannelsIn\n        self.scalingFactor = configuration.scalingFactor\n        self.encoder = VAEncoder(\n            inputChannels: configuration.inputChannels,\n            outputChannels: configuration.latentChannelsOut,\n            blockOutChannels: configuration.blockOutChannels,\n            layersPerBlock: configuration.layersPerBlock,\n            resnetGroups: configuration.normNumGroups)\n        self.decoder = VADecoder(\n            inputChannels: configuration.latentChannelsIn,\n            outputChannels: configuration.outputChannels,\n            blockOutChannels: configuration.blockOutChannels,\n            layersPerBlock: configuration.layersPerBlock + 1,\n            resnetGroups: configuration.normNumGroups)\n\n        self._quantProjection.wrappedValue = Linear(\n            configuration.latentChannelsIn, configuration.latentChannelsOut)\n        self._postQuantProjection.wrappedValue = Linear(\n            configuration.latentChannelsIn, configuration.latentChannelsIn)\n    }\n\n    func decode(_ z: MLXArray) -> MLXArray {\n        let z = z / scalingFactor\n        return decoder(postQuantProjection(z))\n    }\n\n    func encode(_ x: MLXArray) -> (MLXArray, MLXArray) {\n        var x = encoder(x)\n        x = quantProjection(x)\n        var (mean, logvar) = x.split(axis: -1)\n        mean = mean * scalingFactor\n        logvar = logvar + 2 * log(scalingFactor)\n\n        return (mean, logvar)\n    }\n\n    struct Result {\n        let xHat: MLXArray\n        let z: MLXArray\n        let mean: MLXArray\n        let logvar: MLXArray\n    }\n\n    func callAsFunction(_ x: MLXArray, key: MLXArray? = nil) -> Result {\n        let (mean, logvar) = encode(x)\n        let z = MLXRandom.normal(mean.shape, key: key) * exp(0.5 * logvar) + mean\n        let xHat = decode(z)\n\n        return Result(xHat: xHat, z: z, mean: mean, logvar: logvar)\n    }\n}"}]}
{"messages": [{"role": "user", "content": "convert the following MLX Python code into MLX Swift: \n\n# Copyright \u00a9 2023-2024 Apple Inc.\n\nfrom dataclasses import dataclass\nfrom typing import List, Optional\n\nimport mlx.core as mx\nimport mlx.nn as nn\n\nfrom .config import CLIPTextModelConfig\n\n_ACTIVATIONS = {\"quick_gelu\": nn.gelu_fast_approx, \"gelu\": nn.gelu}\n\n\n@dataclass\nclass CLIPOutput:\n    # The last_hidden_state indexed at the EOS token and possibly projected if\n    # the model has a projection layer\n    pooled_output: Optional[mx.array] = None\n\n    # The full sequence output of the transformer after the final layernorm\n    last_hidden_state: Optional[mx.array] = None\n\n    # A list of hidden states corresponding to the outputs of the transformer layers\n    hidden_states: Optional[List[mx.array]] = None\n\n\nclass CLIPEncoderLayer(nn.Module):\n    \"\"\"The transformer encoder layer from CLIP.\"\"\"\n\n    def __init__(self, model_dims: int, num_heads: int, activation: str):\n        super().__init__()\n\n        self.layer_norm1 = nn.LayerNorm(model_dims)\n        self.layer_norm2 = nn.LayerNorm(model_dims)\n\n        self.attention = nn.MultiHeadAttention(model_dims, num_heads)\n        # Add biases to the attention projections to match CLIP\n        self.attention.query_proj.bias = mx.zeros(model_dims)\n        self.attention.key_proj.bias = mx.zeros(model_dims)\n        self.attention.value_proj.bias = mx.zeros(model_dims)\n        self.attention.out_proj.bias = mx.zeros(model_dims)\n\n        self.linear1 = nn.Linear(model_dims, 4 * model_dims)\n        self.linear2 = nn.Linear(4 * model_dims, model_dims)\n\n        self.act = _ACTIVATIONS[activation]\n\n    def __call__(self, x, attn_mask=None):\n        y = self.layer_norm1(x)\n        y = self.attention(y, y, y, attn_mask)\n        x = y + x\n\n        y = self.layer_norm2(x)\n        y = self.linear1(y)\n        y = self.act(y)\n        y = self.linear2(y)\n        x = y + x\n\n        return x\n\n\nclass CLIPTextModel(nn.Module):\n    \"\"\"Implements the text encoder transformer from CLIP.\"\"\"\n\n    def __init__(self, config: CLIPTextModelConfig):\n        super().__init__()\n\n        self.token_embedding = nn.Embedding(config.vocab_size, config.model_dims)\n        self.position_embedding = nn.Embedding(config.max_length, config.model_dims)\n        self.layers = [\n            CLIPEncoderLayer(config.model_dims, config.num_heads, config.hidden_act)\n            for i in range(config.num_layers)\n        ]\n        self.final_layer_norm = nn.LayerNorm(config.model_dims)\n\n        if config.projection_dim is not None:\n            self.text_projection = nn.Linear(\n                config.model_dims, config.projection_dim, bias=False\n            )\n\n    def _get_mask(self, N, dtype):\n        indices = mx.arange(N)\n        mask = indices[:, None] < indices[None]\n        mask = mask.astype(dtype) * (-6e4 if dtype == mx.float16 else -1e9)\n        return mask\n\n    def __call__(self, x):\n        # Extract some shapes\n        B, N = x.shape\n        eos_tokens = x.argmax(-1)\n\n        # Compute the embeddings\n        x = self.token_embedding(x)\n        x = x + self.position_embedding.weight[:N]\n\n        # Compute the features from the transformer\n        mask = self._get_mask(N, x.dtype)\n        hidden_states = []\n        for l in self.layers:\n            x = l(x, mask)\n            hidden_states.append(x)\n\n        # Apply the final layernorm and return\n        x = self.final_layer_norm(x)\n        last_hidden_state = x\n\n        # Select the EOS token\n        pooled_output = x[mx.arange(len(x)), eos_tokens]\n        if \"text_projection\" in self:\n            pooled_output = self.text_projection(pooled_output)\n\n        return CLIPOutput(\n            pooled_output=pooled_output,\n            last_hidden_state=last_hidden_state,\n            hidden_states=hidden_states,\n        )"}, {"role": "assistant", "content": "// Copyright \u00a9 2024 Apple Inc.\n\nimport Foundation\nimport MLX\nimport MLXNN\n\n// port of https://github.com/ml-explore/mlx-examples/blob/main/stable_diffusion/stable_diffusion/clip.py\n\nstruct CLIPOutput {\n    /// The lastHiddenState indexed at the EOS token and possibly projected if\n    /// the model has a projection layer\n    public var pooledOutput: MLXArray\n\n    /// The full sequence output of the transformer after the final layernorm\n    public var lastHiddenState: MLXArray\n\n    /// A list of hidden states corresponding to the outputs of the transformer layers\n    public var hiddenStates: [MLXArray]\n}\n\n/// The transformer encoder layer from CLIP\nclass CLIPEncoderLayer: Module {\n\n    @ModuleInfo(key: \"layer_norm1\") var layerNorm1: LayerNorm\n    @ModuleInfo(key: \"layer_norm2\") var layerNorm2: LayerNorm\n\n    let attention: MultiHeadAttention\n\n    @ModuleInfo var linear1: Linear\n    @ModuleInfo var linear2: Linear\n\n    let activation: (MLXArray) -> MLXArray\n\n    init(modelDimensions: Int, numHeads: Int, activation: @escaping (MLXArray) -> MLXArray) {\n        self._layerNorm1.wrappedValue = LayerNorm(dimensions: modelDimensions)\n        self._layerNorm2.wrappedValue = LayerNorm(dimensions: modelDimensions)\n\n        self.attention = MultiHeadAttention(\n            dimensions: modelDimensions, numHeads: numHeads, bias: true)\n\n        self.linear1 = Linear(modelDimensions, 4 * modelDimensions)\n        self.linear2 = Linear(4 * modelDimensions, modelDimensions)\n\n        self.activation = activation\n    }\n\n    func callAsFunction(_ x: MLXArray, attentionMask: MLXArray? = nil) -> MLXArray {\n        var y = layerNorm1(x)\n        y = attention(y, keys: y, values: y, mask: attentionMask)\n        var x = y + x\n\n        y = layerNorm2(x)\n        y = linear1(y)\n        y = activation(y)\n        y = linear2(y)\n        x = y + x\n\n        return x\n    }\n}\n\n/// Implements the text encoder transformer from CLIP\nclass CLIPTextModel: Module {\n\n    @ModuleInfo(key: \"token_embedding\") var tokenEmbedding: Embedding\n    @ModuleInfo(key: \"position_embedding\") var positionEmbedding: Embedding\n\n    let layers: [CLIPEncoderLayer]\n\n    @ModuleInfo(key: \"final_layer_norm\") var finalLayerNorm: LayerNorm\n\n    @ModuleInfo(key: \"text_projection\") var textProjection: Linear?\n\n    init(configuration: CLIPTextModelConfiguration) {\n        self._tokenEmbedding.wrappedValue = Embedding(\n            embeddingCount: configuration.vocabularySize, dimensions: configuration.modelDimensions)\n        self._positionEmbedding.wrappedValue = Embedding(\n            embeddingCount: configuration.maxLength, dimensions: configuration.modelDimensions)\n\n        self.layers = (0 ..< configuration.numLayers)\n            .map { _ in\n                CLIPEncoderLayer(\n                    modelDimensions: configuration.modelDimensions,\n                    numHeads: configuration.numHeads,\n                    activation: configuration.hiddenActivation.activation)\n            }\n\n        self._finalLayerNorm.wrappedValue = LayerNorm(dimensions: configuration.modelDimensions)\n\n        if let projectionDimensions = configuration.projectionDimensions {\n            self._textProjection.wrappedValue = Linear(\n                configuration.modelDimensions, projectionDimensions, bias: false)\n        } else {\n            self._textProjection.wrappedValue = nil\n        }\n    }\n\n    func mask(_ N: Int, _ dType: DType) -> MLXArray {\n        let indices = MLXArray(0 ..< Int32(N))\n        var mask = indices[0..., .newAxis] .< indices[.newAxis]\n        mask = mask.asType(dType) * (dType == .float16 ? -6e4 : -1e9)\n        return mask\n    }\n\n    func callAsFunction(_ x: MLXArray) -> CLIPOutput {\n        var x = x\n        let (_, N) = x.shape2\n        let eosTokens = x.argMax(axis: -1)\n\n        // compute the embeddings\n        x = tokenEmbedding(x)\n        x = x + positionEmbedding.weight[..<N]\n\n        // compute the features from the transformer\n        let mask = mask(N, x.dtype)\n        var hiddenStates = [MLXArray]()\n        for l in layers {\n            x = l(x, attentionMask: mask)\n            hiddenStates.append(x)\n        }\n\n        // apply the final layernorm\n        x = finalLayerNorm(x)\n        let lastHiddenState = x\n\n        // select the EOS token\n        var pooledOutput = x[MLXArray(0 ..< x.count), eosTokens]\n        if let textProjection {\n            pooledOutput = textProjection(pooledOutput)\n        }\n\n        return CLIPOutput(\n            pooledOutput: pooledOutput, lastHiddenState: lastHiddenState, hiddenStates: hiddenStates\n        )\n    }\n}"}]}
{"messages": [{"role": "user", "content": "convert the following MLX Python code into MLX Swift: \n\n# Copyright \u00a9 2023-2024 Apple Inc.\n\nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple\n\n\n@dataclass\nclass AutoencoderConfig:\n    in_channels: int = 3\n    out_channels: int = 3\n    latent_channels_out: int = 8\n    latent_channels_in: int = 4\n    block_out_channels: Tuple[int] = (128, 256, 512, 512)\n    layers_per_block: int = 2\n    norm_num_groups: int = 32\n    scaling_factor: float = 0.18215\n\n\n@dataclass\nclass CLIPTextModelConfig:\n    num_layers: int = 23\n    model_dims: int = 1024\n    num_heads: int = 16\n    max_length: int = 77\n    vocab_size: int = 49408\n    projection_dim: Optional[int] = None\n    hidden_act: str = \"quick_gelu\"\n\n\n@dataclass\nclass UNetConfig:\n    in_channels: int = 4\n    out_channels: int = 4\n    conv_in_kernel: int = 3\n    conv_out_kernel: int = 3\n    block_out_channels: Tuple[int] = (320, 640, 1280, 1280)\n    layers_per_block: Tuple[int] = (2, 2, 2, 2)\n    mid_block_layers: int = 2\n    transformer_layers_per_block: Tuple[int] = (1, 1, 1, 1)\n    num_attention_heads: Tuple[int] = (5, 10, 20, 20)\n    cross_attention_dim: Tuple[int] = (1024,) * 4\n    norm_num_groups: int = 32\n    down_block_types: Tuple[str] = (\n        \"CrossAttnDownBlock2D\",\n        \"CrossAttnDownBlock2D\",\n        \"CrossAttnDownBlock2D\",\n        \"DownBlock2D\",\n    )\n    up_block_types: Tuple[str] = (\n        \"UpBlock2D\",\n        \"CrossAttnUpBlock2D\",\n        \"CrossAttnUpBlock2D\",\n        \"CrossAttnUpBlock2D\",\n    )\n    addition_embed_type: Optional[str] = None\n    addition_time_embed_dim: Optional[int] = None\n    projection_class_embeddings_input_dim: Optional[int] = None\n\n\n@dataclass\nclass DiffusionConfig:\n    beta_schedule: str = \"scaled_linear\"\n    beta_start: float = 0.00085\n    beta_end: float = 0.012\n    num_train_steps: int = 1000"}, {"role": "assistant", "content": "// Copyright \u00a9 2024 Apple Inc.\n\nimport Foundation\nimport MLX\nimport MLXNN\n\n// port of https://github.com/ml-explore/mlx-examples/blob/main/stable_diffusion/stable_diffusion/config.py\n\n/// Configuration for ``Autoencoder``\nstruct AutoencoderConfiguration: Codable {\n\n    public var inputChannels = 3\n    public var outputChannels = 3\n    public var latentChannelsOut: Int { latentChannelsIn * 2 }\n    public var latentChannelsIn = 4\n    public var blockOutChannels = [128, 256, 512, 512]\n    public var layersPerBlock = 2\n    public var normNumGroups = 32\n    public var scalingFactor: Float = 0.18215\n\n    enum CodingKeys: String, CodingKey {\n        case inputChannels = \"in_channels\"\n        case outputChannels = \"out_channels\"\n        case latentChannelsIn = \"latent_channels\"\n        case blockOutChannels = \"block_out_channels\"\n        case layersPerBlock = \"layers_per_block\"\n        case normNumGroups = \"norm_num_groups\"\n        case scalingFactor = \"scaling_factor\"\n    }\n\n    public init(from decoder: any Decoder) throws {\n        let container: KeyedDecodingContainer<AutoencoderConfiguration.CodingKeys> =\n            try decoder.container(keyedBy: AutoencoderConfiguration.CodingKeys.self)\n\n        // load_autoencoder()\n\n        self.scalingFactor =\n            try container.decodeIfPresent(Float.self, forKey: .scalingFactor) ?? 0.18215\n\n        self.inputChannels = try container.decode(Int.self, forKey: .inputChannels)\n        self.outputChannels = try container.decode(Int.self, forKey: .outputChannels)\n        self.latentChannelsIn = try container.decode(Int.self, forKey: .latentChannelsIn)\n        self.blockOutChannels = try container.decode([Int].self, forKey: .blockOutChannels)\n        self.layersPerBlock = try container.decode(Int.self, forKey: .layersPerBlock)\n        self.normNumGroups = try container.decode(Int.self, forKey: .normNumGroups)\n    }\n\n    public func encode(to encoder: any Encoder) throws {\n        var container: KeyedEncodingContainer<AutoencoderConfiguration.CodingKeys> =\n            encoder.container(keyedBy: AutoencoderConfiguration.CodingKeys.self)\n\n        try container.encode(self.inputChannels, forKey: .inputChannels)\n        try container.encode(self.outputChannels, forKey: .outputChannels)\n        try container.encode(self.latentChannelsIn, forKey: .latentChannelsIn)\n        try container.encode(self.blockOutChannels, forKey: .blockOutChannels)\n        try container.encode(self.layersPerBlock, forKey: .layersPerBlock)\n        try container.encode(self.normNumGroups, forKey: .normNumGroups)\n        try container.encode(self.scalingFactor, forKey: .scalingFactor)\n    }\n}\n\n/// Configuration for ``CLIPTextModel``\nstruct CLIPTextModelConfiguration: Codable {\n\n    public enum ClipActivation: String, Codable {\n        case fast = \"quick_gelu\"\n        case gelu = \"gelu\"\n\n        var activation: (MLXArray) -> MLXArray {\n            switch self {\n            case .fast: MLXNN.geluFastApproximate\n            case .gelu: MLXNN.gelu\n            }\n        }\n    }\n\n    public var numLayers = 23\n    public var modelDimensions = 1024\n    public var numHeads = 16\n    public var maxLength = 77\n    public var vocabularySize = 49408\n    public var projectionDimensions: Int? = nil\n    public var hiddenActivation: ClipActivation = .fast\n\n    enum CodingKeys: String, CodingKey {\n        case numLayers = \"num_hidden_layers\"\n        case modelDimensions = \"hidden_size\"\n        case numHeads = \"num_attention_heads\"\n        case maxLength = \"max_position_embeddings\"\n        case vocabularySize = \"vocab_size\"\n        case projectionDimensions = \"projection_dim\"\n        case hiddenActivation = \"hidden_act\"\n        case architectures = \"architectures\"\n    }\n\n    public init(from decoder: any Decoder) throws {\n        let container: KeyedDecodingContainer<CLIPTextModelConfiguration.CodingKeys> =\n            try decoder.container(keyedBy: CLIPTextModelConfiguration.CodingKeys.self)\n\n        // see load_text_encoder\n\n        let architectures = try container.decode([String].self, forKey: .architectures)\n        let withProjection = architectures[0].contains(\"WithProjection\")\n\n        self.projectionDimensions =\n            withProjection\n            ? try container.decodeIfPresent(Int.self, forKey: .projectionDimensions) : nil\n        self.hiddenActivation =\n            try container.decodeIfPresent(\n                CLIPTextModelConfiguration.ClipActivation.self, forKey: .hiddenActivation) ?? .fast\n\n        self.numLayers = try container.decode(Int.self, forKey: .numLayers)\n        self.modelDimensions = try container.decode(Int.self, forKey: .modelDimensions)\n        self.numHeads = try container.decode(Int.self, forKey: .numHeads)\n        self.maxLength = try container.decode(Int.self, forKey: .maxLength)\n        self.vocabularySize = try container.decode(Int.self, forKey: .vocabularySize)\n    }\n\n    public func encode(to encoder: any Encoder) throws {\n        var container: KeyedEncodingContainer<CLIPTextModelConfiguration.CodingKeys> =\n            encoder.container(keyedBy: CLIPTextModelConfiguration.CodingKeys.self)\n\n        if projectionDimensions != nil {\n            try container.encode([\"WithProjection\"], forKey: .architectures)\n        } else {\n            try container.encode([\"Other\"], forKey: .architectures)\n        }\n\n        try container.encode(self.numLayers, forKey: .numLayers)\n        try container.encode(self.modelDimensions, forKey: .modelDimensions)\n        try container.encode(self.numHeads, forKey: .numHeads)\n        try container.encode(self.maxLength, forKey: .maxLength)\n        try container.encode(self.vocabularySize, forKey: .vocabularySize)\n        try container.encodeIfPresent(self.projectionDimensions, forKey: .projectionDimensions)\n        try container.encode(self.hiddenActivation, forKey: .hiddenActivation)\n    }\n}\n\n/// Configuration for ``UNetModel``\nstruct UNetConfiguration: Codable {\n\n    public var inputChannels = 4\n    public var outputChannels = 4\n    public var convolutionInKernel = 3\n    public var convolutionOutKernel = 3\n    public var blockOutChannels = [320, 640, 1280, 1280]\n    public var layersPerBlock = [2, 2, 2, 2]\n    public var midBlockLayers = 2\n    public var transformerLayersPerBlock = [2, 2, 2, 2]\n    public var numHeads = [5, 10, 20, 20]\n    public var crossAttentionDimension = [1024, 1024, 1024, 1024]\n    public var normNumGroups = 32\n    public var downBlockTypes: [String] = []\n    public var upBlockTypes: [String] = []\n    public var additionEmbedType: String? = nil\n    public var additionTimeEmbedDimension: Int? = nil\n    public var projectionClassEmbeddingsInputDimension: Int? = nil\n\n    enum CodingKeys: String, CodingKey {\n        case inputChannels = \"in_channels\"\n        case outputChannels = \"out_channels\"\n        case convolutionInKernel = \"conv_in_kernel\"\n        case convolutionOutKernel = \"conv_out_kernel\"\n        case blockOutChannels = \"block_out_channels\"\n        case layersPerBlock = \"layers_per_block\"\n        case midBlockLayers = \"mid_block_layers\"\n        case transformerLayersPerBlock = \"transformer_layers_per_block\"\n        case numHeads = \"attention_head_dim\"\n        case crossAttentionDimension = \"cross_attention_dim\"\n        case normNumGroups = \"norm_num_groups\"\n        case downBlockTypes = \"down_block_types\"\n        case upBlockTypes = \"up_block_types\"\n        case additionEmbedType = \"addition_embed_type\"\n        case additionTimeEmbedDimension = \"addition_time_embed_dim\"\n        case projectionClassEmbeddingsInputDimension = \"projection_class_embeddings_input_dim\"\n    }\n\n    public init() {\n    }\n\n    public init(from decoder: Decoder) throws {\n        let container: KeyedDecodingContainer<UNetConfiguration.CodingKeys> = try decoder.container(\n            keyedBy: UNetConfiguration.CodingKeys.self)\n\n        // customizations based on def load_unet(key: str = _DEFAULT_MODEL, float16: bool = False):\n        //\n        // Note: the encode() writes out the internal format (and this can load it back in)\n\n        self.blockOutChannels = try container.decode([Int].self, forKey: .blockOutChannels)\n        let nBlocks = blockOutChannels.count\n\n        self.layersPerBlock =\n            try (try? container.decode([Int].self, forKey: .layersPerBlock))\n            ?? Array(repeating: container.decode(Int.self, forKey: .layersPerBlock), count: nBlocks)\n        self.transformerLayersPerBlock =\n            (try? container.decode([Int].self, forKey: .transformerLayersPerBlock)) ?? [1, 1, 1, 1]\n        self.numHeads =\n            try (try? container.decodeIfPresent([Int].self, forKey: .numHeads))\n            ?? Array(repeating: container.decode(Int.self, forKey: .numHeads), count: nBlocks)\n        self.crossAttentionDimension =\n            try (try? container.decode([Int].self, forKey: .crossAttentionDimension))\n            ?? Array(\n                repeating: container.decode(Int.self, forKey: .crossAttentionDimension),\n                count: nBlocks)\n        self.upBlockTypes = try container.decode([String].self, forKey: .upBlockTypes).reversed()\n\n        self.convolutionInKernel =\n            try container.decodeIfPresent(Int.self, forKey: .convolutionInKernel) ?? 3\n        self.convolutionOutKernel =\n            try container.decodeIfPresent(Int.self, forKey: .convolutionOutKernel) ?? 3\n        self.midBlockLayers = try container.decodeIfPresent(Int.self, forKey: .midBlockLayers) ?? 2\n\n        self.inputChannels = try container.decode(Int.self, forKey: .inputChannels)\n        self.outputChannels = try container.decode(Int.self, forKey: .outputChannels)\n        self.normNumGroups = try container.decode(Int.self, forKey: .normNumGroups)\n        self.downBlockTypes = try container.decode([String].self, forKey: .downBlockTypes)\n        self.additionEmbedType = try container.decodeIfPresent(\n            String.self, forKey: .additionEmbedType)\n        self.additionTimeEmbedDimension = try container.decodeIfPresent(\n            Int.self, forKey: .additionTimeEmbedDimension)\n        self.projectionClassEmbeddingsInputDimension = try container.decodeIfPresent(\n            Int.self, forKey: .projectionClassEmbeddingsInputDimension)\n    }\n\n    public func encode(to encoder: Encoder) throws {\n        var container: KeyedEncodingContainer<UNetConfiguration.CodingKeys> = encoder.container(\n            keyedBy: UNetConfiguration.CodingKeys.self)\n\n        try container.encode(self.upBlockTypes.reversed(), forKey: .upBlockTypes)\n\n        try container.encode(self.inputChannels, forKey: .inputChannels)\n        try container.encode(self.outputChannels, forKey: .outputChannels)\n        try container.encode(self.convolutionInKernel, forKey: .convolutionInKernel)\n        try container.encode(self.convolutionOutKernel, forKey: .convolutionOutKernel)\n        try container.encode(self.blockOutChannels, forKey: .blockOutChannels)\n        try container.encode(self.layersPerBlock, forKey: .layersPerBlock)\n        try container.encode(self.midBlockLayers, forKey: .midBlockLayers)\n        try container.encode(self.transformerLayersPerBlock, forKey: .transformerLayersPerBlock)\n        try container.encode(self.numHeads, forKey: .numHeads)\n        try container.encode(self.crossAttentionDimension, forKey: .crossAttentionDimension)\n        try container.encode(self.normNumGroups, forKey: .normNumGroups)\n        try container.encode(self.downBlockTypes, forKey: .downBlockTypes)\n        try container.encodeIfPresent(self.additionEmbedType, forKey: .additionEmbedType)\n        try container.encodeIfPresent(\n            self.additionTimeEmbedDimension, forKey: .additionTimeEmbedDimension)\n        try container.encodeIfPresent(\n            self.projectionClassEmbeddingsInputDimension,\n            forKey: .projectionClassEmbeddingsInputDimension)\n    }\n}\n\n/// Configuration for ``StableDiffusion``\npublic struct DiffusionConfiguration: Codable {\n\n    public enum BetaSchedule: String, Codable {\n        case linear = \"linear\"\n        case scaledLinear = \"scaled_linear\"\n    }\n\n    public var betaSchedule = BetaSchedule.scaledLinear\n    public var betaStart: Float = 0.00085\n    public var betaEnd: Float = 0.012\n    public var trainSteps = 3\n\n    enum CodingKeys: String, CodingKey {\n        case betaSchedule = \"beta_schedule\"\n        case betaStart = \"beta_start\"\n        case betaEnd = \"beta_end\"\n        case trainSteps = \"num_train_timesteps\"\n    }\n}"}]}
{"messages": [{"role": "user", "content": "convert the following MLX Python code into MLX Swift: \n\n# Copyright \u00a9 2023-2024 Apple Inc.\n\nimport json\nfrom typing import Optional\n\nimport mlx.core as mx\nfrom huggingface_hub import hf_hub_download\nfrom mlx.utils import tree_unflatten\n\nfrom .clip import CLIPTextModel\nfrom .config import AutoencoderConfig, CLIPTextModelConfig, DiffusionConfig, UNetConfig\nfrom .tokenizer import Tokenizer\nfrom .unet import UNetModel\nfrom .vae import Autoencoder\n\n_DEFAULT_MODEL = \"stabilityai/stable-diffusion-2-1-base\"\n_MODELS = {\n    # See https://huggingface.co/stabilityai/sdxl-turbo for the model details and license\n    \"stabilityai/sdxl-turbo\": {\n        \"unet_config\": \"unet/config.json\",\n        \"unet\": \"unet/diffusion_pytorch_model.safetensors\",\n        \"text_encoder_config\": \"text_encoder/config.json\",\n        \"text_encoder\": \"text_encoder/model.safetensors\",\n        \"text_encoder_2_config\": \"text_encoder_2/config.json\",\n        \"text_encoder_2\": \"text_encoder_2/model.safetensors\",\n        \"vae_config\": \"vae/config.json\",\n        \"vae\": \"vae/diffusion_pytorch_model.safetensors\",\n        \"diffusion_config\": \"scheduler/scheduler_config.json\",\n        \"tokenizer_vocab\": \"tokenizer/vocab.json\",\n        \"tokenizer_merges\": \"tokenizer/merges.txt\",\n        \"tokenizer_2_vocab\": \"tokenizer_2/vocab.json\",\n        \"tokenizer_2_merges\": \"tokenizer_2/merges.txt\",\n    },\n    # See https://huggingface.co/stabilityai/stable-diffusion-2-1-base for the model details and license\n    \"stabilityai/stable-diffusion-2-1-base\": {\n        \"unet_config\": \"unet/config.json\",\n        \"unet\": \"unet/diffusion_pytorch_model.safetensors\",\n        \"text_encoder_config\": \"text_encoder/config.json\",\n        \"text_encoder\": \"text_encoder/model.safetensors\",\n        \"vae_config\": \"vae/config.json\",\n        \"vae\": \"vae/diffusion_pytorch_model.safetensors\",\n        \"diffusion_config\": \"scheduler/scheduler_config.json\",\n        \"tokenizer_vocab\": \"tokenizer/vocab.json\",\n        \"tokenizer_merges\": \"tokenizer/merges.txt\",\n    },\n}\n\n\ndef map_unet_weights(key, value):\n    # Map up/downsampling\n    if \"downsamplers\" in key:\n        key = key.replace(\"downsamplers.0.conv\", \"downsample\")\n    if \"upsamplers\" in key:\n        key = key.replace(\"upsamplers.0.conv\", \"upsample\")\n\n    # Map the mid block\n    if \"mid_block.resnets.0\" in key:\n        key = key.replace(\"mid_block.resnets.0\", \"mid_blocks.0\")\n    if \"mid_block.attentions.0\" in key:\n        key = key.replace(\"mid_block.attentions.0\", \"mid_blocks.1\")\n    if \"mid_block.resnets.1\" in key:\n        key = key.replace(\"mid_block.resnets.1\", \"mid_blocks.2\")\n\n    # Map attention layers\n    if \"to_k\" in key:\n        key = key.replace(\"to_k\", \"key_proj\")\n    if \"to_out.0\" in key:\n        key = key.replace(\"to_out.0\", \"out_proj\")\n    if \"to_q\" in key:\n        key = key.replace(\"to_q\", \"query_proj\")\n    if \"to_v\" in key:\n        key = key.replace(\"to_v\", \"value_proj\")\n\n    # Map transformer ffn\n    if \"ff.net.2\" in key:\n        key = key.replace(\"ff.net.2\", \"linear3\")\n    if \"ff.net.0\" in key:\n        k1 = key.replace(\"ff.net.0.proj\", \"linear1\")\n        k2 = key.replace(\"ff.net.0.proj\", \"linear2\")\n        v1, v2 = mx.split(value, 2)\n\n        return [(k1, v1), (k2, v2)]\n\n    if \"conv_shortcut.weight\" in key:\n        value = value.squeeze()\n\n    # Transform the weights from 1x1 convs to linear\n    if len(value.shape) == 4 and (\"proj_in\" in key or \"proj_out\" in key):\n        value = value.squeeze()\n\n    if len(value.shape) == 4:\n        value = value.transpose(0, 2, 3, 1)\n        value = value.reshape(-1).reshape(value.shape)\n\n    return [(key, value)]\n\n\ndef map_clip_text_encoder_weights(key, value):\n    # Remove prefixes\n    if key.startswith(\"text_model.\"):\n        key = key[11:]\n    if key.startswith(\"embeddings.\"):\n        key = key[11:]\n    if key.startswith(\"encoder.\"):\n        key = key[8:]\n\n    # Map attention layers\n    if \"self_attn.\" in key:\n        key = key.replace(\"self_attn.\", \"attention.\")\n    if \"q_proj.\" in key:\n        key = key.replace(\"q_proj.\", \"query_proj.\")\n    if \"k_proj.\" in key:\n        key = key.replace(\"k_proj.\", \"key_proj.\")\n    if \"v_proj.\" in key:\n        key = key.replace(\"v_proj.\", \"value_proj.\")\n\n    # Map ffn layers\n    if \"mlp.fc1\" in key:\n        key = key.replace(\"mlp.fc1\", \"linear1\")\n    if \"mlp.fc2\" in key:\n        key = key.replace(\"mlp.fc2\", \"linear2\")\n\n    return [(key, value)]\n\n\ndef map_vae_weights(key, value):\n    # Map up/downsampling\n    if \"downsamplers\" in key:\n        key = key.replace(\"downsamplers.0.conv\", \"downsample\")\n    if \"upsamplers\" in key:\n        key = key.replace(\"upsamplers.0.conv\", \"upsample\")\n\n    # Map attention layers\n    if \"to_k\" in key:\n        key = key.replace(\"to_k\", \"key_proj\")\n    if \"to_out.0\" in key:\n        key = key.replace(\"to_out.0\", \"out_proj\")\n    if \"to_q\" in key:\n        key = key.replace(\"to_q\", \"query_proj\")\n    if \"to_v\" in key:\n        key = key.replace(\"to_v\", \"value_proj\")\n\n    # Map the mid block\n    if \"mid_block.resnets.0\" in key:\n        key = key.replace(\"mid_block.resnets.0\", \"mid_blocks.0\")\n    if \"mid_block.attentions.0\" in key:\n        key = key.replace(\"mid_block.attentions.0\", \"mid_blocks.1\")\n    if \"mid_block.resnets.1\" in key:\n        key = key.replace(\"mid_block.resnets.1\", \"mid_blocks.2\")\n\n    # Map the quant/post_quant layers\n    if \"quant_conv\" in key:\n        key = key.replace(\"quant_conv\", \"quant_proj\")\n        value = value.squeeze()\n\n    # Map the conv_shortcut to linear\n    if \"conv_shortcut.weight\" in key:\n        value = value.squeeze()\n\n    if len(value.shape) == 4:\n        value = value.transpose(0, 2, 3, 1)\n        value = value.reshape(-1).reshape(value.shape)\n\n    return [(key, value)]\n\n\ndef _flatten(params):\n    return [(k, v) for p in params for (k, v) in p]\n\n\ndef _load_safetensor_weights(mapper, model, weight_file, float16: bool = False):\n    dtype = mx.float16 if float16 else mx.float32\n    weights = mx.load(weight_file)\n    weights = _flatten([mapper(k, v.astype(dtype)) for k, v in weights.items()])\n    model.update(tree_unflatten(weights))\n\n\ndef _check_key(key: str, part: str):\n    if key not in _MODELS:\n        raise ValueError(\n            f\"[{part}] '{key}' model not found, choose one of {{{','.join(_MODELS.keys())}}}\"\n        )\n\n\ndef load_unet(key: str = _DEFAULT_MODEL, float16: bool = False):\n    \"\"\"Load the stable diffusion UNet from Hugging Face Hub.\"\"\"\n    _check_key(key, \"load_unet\")\n\n    # Download the config and create the model\n    unet_config = _MODELS[key][\"unet_config\"]\n    with open(hf_hub_download(key, unet_config)) as f:\n        config = json.load(f)\n\n    n_blocks = len(config[\"block_out_channels\"])\n    model = UNetModel(\n        UNetConfig(\n            in_channels=config[\"in_channels\"],\n            out_channels=config[\"out_channels\"],\n            block_out_channels=config[\"block_out_channels\"],\n            layers_per_block=[config[\"layers_per_block\"]] * n_blocks,\n            transformer_layers_per_block=config.get(\n                \"transformer_layers_per_block\", (1,) * 4\n            ),\n            num_attention_heads=(\n                [config[\"attention_head_dim\"]] * n_blocks\n                if isinstance(config[\"attention_head_dim\"], int)\n                else config[\"attention_head_dim\"]\n            ),\n            cross_attention_dim=[config[\"cross_attention_dim\"]] * n_blocks,\n            norm_num_groups=config[\"norm_num_groups\"],\n            down_block_types=config[\"down_block_types\"],\n            up_block_types=config[\"up_block_types\"][::-1],\n            addition_embed_type=config.get(\"addition_embed_type\", None),\n            addition_time_embed_dim=config.get(\"addition_time_embed_dim\", None),\n            projection_class_embeddings_input_dim=config.get(\n                \"projection_class_embeddings_input_dim\", None\n            ),\n        )\n    )\n\n    # Download the weights and map them into the model\n    unet_weights = _MODELS[key][\"unet\"]\n    weight_file = hf_hub_download(key, unet_weights)\n    _load_safetensor_weights(map_unet_weights, model, weight_file, float16)\n\n    return model\n\n\ndef load_text_encoder(\n    key: str = _DEFAULT_MODEL,\n    float16: bool = False,\n    model_key: str = \"text_encoder\",\n    config_key: Optional[str] = None,\n):\n    \"\"\"Load the stable diffusion text encoder from Hugging Face Hub.\"\"\"\n    _check_key(key, \"load_text_encoder\")\n\n    config_key = config_key or (model_key + \"_config\")\n\n    # Download the config and create the model\n    text_encoder_config = _MODELS[key][config_key]\n    with open(hf_hub_download(key, text_encoder_config)) as f:\n        config = json.load(f)\n\n    with_projection = \"WithProjection\" in config[\"architectures\"][0]\n\n    model = CLIPTextModel(\n        CLIPTextModelConfig(\n            num_layers=config[\"num_hidden_layers\"],\n            model_dims=config[\"hidden_size\"],\n            num_heads=config[\"num_attention_heads\"],\n            max_length=config[\"max_position_embeddings\"],\n            vocab_size=config[\"vocab_size\"],\n            projection_dim=config[\"projection_dim\"] if with_projection else None,\n            hidden_act=config.get(\"hidden_act\", \"quick_gelu\"),\n        )\n    )\n\n    # Download the weights and map them into the model\n    text_encoder_weights = _MODELS[key][model_key]\n    weight_file = hf_hub_download(key, text_encoder_weights)\n    _load_safetensor_weights(map_clip_text_encoder_weights, model, weight_file, float16)\n\n    return model\n\n\ndef load_autoencoder(key: str = _DEFAULT_MODEL, float16: bool = False):\n    \"\"\"Load the stable diffusion autoencoder from Hugging Face Hub.\"\"\"\n    _check_key(key, \"load_autoencoder\")\n\n    # Download the config and create the model\n    vae_config = _MODELS[key][\"vae_config\"]\n    with open(hf_hub_download(key, vae_config)) as f:\n        config = json.load(f)\n\n    model = Autoencoder(\n        AutoencoderConfig(\n            in_channels=config[\"in_channels\"],\n            out_channels=config[\"out_channels\"],\n            latent_channels_out=2 * config[\"latent_channels\"],\n            latent_channels_in=config[\"latent_channels\"],\n            block_out_channels=config[\"block_out_channels\"],\n            layers_per_block=config[\"layers_per_block\"],\n            norm_num_groups=config[\"norm_num_groups\"],\n            scaling_factor=config.get(\"scaling_factor\", 0.18215),\n        )\n    )\n\n    # Download the weights and map them into the model\n    vae_weights = _MODELS[key][\"vae\"]\n    weight_file = hf_hub_download(key, vae_weights)\n    _load_safetensor_weights(map_vae_weights, model, weight_file, float16)\n\n    return model\n\n\ndef load_diffusion_config(key: str = _DEFAULT_MODEL):\n    \"\"\"Load the stable diffusion config from Hugging Face Hub.\"\"\"\n    _check_key(key, \"load_diffusion_config\")\n\n    diffusion_config = _MODELS[key][\"diffusion_config\"]\n    with open(hf_hub_download(key, diffusion_config)) as f:\n        config = json.load(f)\n\n    return DiffusionConfig(\n        beta_start=config[\"beta_start\"],\n        beta_end=config[\"beta_end\"],\n        beta_schedule=config[\"beta_schedule\"],\n        num_train_steps=config[\"num_train_timesteps\"],\n    )\n\n\ndef load_tokenizer(\n    key: str = _DEFAULT_MODEL,\n    vocab_key: str = \"tokenizer_vocab\",\n    merges_key: str = \"tokenizer_merges\",\n):\n    _check_key(key, \"load_tokenizer\")\n\n    vocab_file = hf_hub_download(key, _MODELS[key][vocab_key])\n    with open(vocab_file, encoding=\"utf-8\") as f:\n        vocab = json.load(f)\n\n    merges_file = hf_hub_download(key, _MODELS[key][merges_key])\n    with open(merges_file, encoding=\"utf-8\") as f:\n        bpe_merges = f.read().strip().split(\"\\n\")[1 : 49152 - 256 - 2 + 1]\n    bpe_merges = [tuple(m.split()) for m in bpe_merges]\n    bpe_ranks = dict(map(reversed, enumerate(bpe_merges)))\n\n    return Tokenizer(bpe_ranks, vocab)"}, {"role": "assistant", "content": "// Copyright \u00a9 2024 Apple Inc.\n\nimport Foundation\nimport Hub\nimport MLX\nimport MLXNN\n\n// port of https://github.com/ml-explore/mlx-examples/blob/main/stable_diffusion/stable_diffusion/model_io.py\n\n/// Configuration for loading stable diffusion weights.\n///\n/// These options can be tuned to conserve memory.\npublic struct LoadConfiguration: Sendable {\n\n    /// convert weights to float16\n    public var float16 = true\n\n    /// quantize weights\n    public var quantize = false\n\n    public var dType: DType {\n        float16 ? .float16 : .float32\n    }\n\n    public init(float16: Bool = true, quantize: Bool = false) {\n        self.float16 = float16\n        self.quantize = quantize\n    }\n}\n\n/// Parameters for evaluating a stable diffusion prompt and generating latents\npublic struct EvaluateParameters: Sendable {\n\n    /// `cfg` value from the preset\n    public var cfgWeight: Float\n\n    /// number of steps -- default is from the preset\n    public var steps: Int\n\n    /// number of images to generate at a time\n    public var imageCount = 1\n    public var decodingBatchSize = 1\n\n    /// size of the latent tensor -- the result image is a factor of 8 larger than this\n    public var latentSize = [64, 64]\n\n    public var seed: UInt64\n    public var prompt = \"\"\n    public var negativePrompt = \"\"\n\n    public init(\n        cfgWeight: Float, steps: Int, imageCount: Int = 1, decodingBatchSize: Int = 1,\n        latentSize: [Int] = [64, 64], seed: UInt64? = nil, prompt: String = \"\",\n        negativePrompt: String = \"\"\n    ) {\n        self.cfgWeight = cfgWeight\n        self.steps = steps\n        self.imageCount = imageCount\n        self.decodingBatchSize = decodingBatchSize\n        self.latentSize = latentSize\n        self.seed = seed ?? UInt64(Date.timeIntervalSinceReferenceDate * 1000)\n        self.prompt = prompt\n        self.negativePrompt = negativePrompt\n    }\n}\n\n/// File types for ``StableDiffusionConfiguration/files``. Used by the presets to provide\n/// relative file paths for different types of files.\nenum FileKey {\n    case unetConfig\n    case unetWeights\n    case textEncoderConfig\n    case textEncoderWeights\n    case textEncoderConfig2\n    case textEncoderWeights2\n    case vaeConfig\n    case vaeWeights\n    case diffusionConfig\n    case tokenizerVocabulary\n    case tokenizerMerges\n    case tokenizerVocabulary2\n    case tokenizerMerges2\n}\n\n/// Stable diffusion configuration -- this selects the model to load.\n///\n/// Use the preset values:\n/// - ``presetSDXLTurbo``\n/// - ``presetStableDiffusion21Base``\n///\n/// or use the enum (convenient for command line tools):\n///\n/// - ``Preset/sdxlTurbo``\n/// - ``Preset/sdxlTurbo``\n///\n/// Call ``download(hub:progressHandler:)`` to download the weights, then\n/// ``textToImageGenerator(hub:configuration:)`` or\n/// ``imageToImageGenerator(hub:configuration:)`` to produce the ``ImageGenerator``.\n///\n/// The ``ImageGenerator`` has a method to generate the latents:\n/// - ``TextToImageGenerator/generateLatents(parameters:)``\n/// - ``ImageToImageGenerator/generateLatents(image:parameters:strength:)``\n///\n/// Evaluate each of the latents from that iterator and use the decoder to turn the last latent\n/// into an image:\n///\n/// - ``ImageGenerator/decode(xt:)``\n///\n/// Finally use ``Image`` to save it to a file or convert to a CGImage for display.\npublic struct StableDiffusionConfiguration: Sendable {\n    public let id: String\n    let files: [FileKey: String]\n    public let defaultParameters: @Sendable () -> EvaluateParameters\n    let factory:\n        @Sendable (HubApi, StableDiffusionConfiguration, LoadConfiguration) throws ->\n            StableDiffusion\n\n    public func download(\n        hub: HubApi = HubApi(), progressHandler: @escaping (Progress) -> Void = { _ in }\n    ) async throws {\n        let repo = Hub.Repo(id: self.id)\n        try await hub.snapshot(\n            from: repo, matching: Array(files.values), progressHandler: progressHandler)\n    }\n\n    public func textToImageGenerator(hub: HubApi = HubApi(), configuration: LoadConfiguration)\n        throws -> TextToImageGenerator?\n    {\n        try factory(hub, self, configuration) as? TextToImageGenerator\n    }\n\n    public func imageToImageGenerator(hub: HubApi = HubApi(), configuration: LoadConfiguration)\n        throws -> ImageToImageGenerator?\n    {\n        try factory(hub, self, configuration) as? ImageToImageGenerator\n    }\n\n    public enum Preset: String, Codable, CaseIterable, Sendable {\n        case base\n        case sdxlTurbo = \"sdxl-turbo\"\n\n        public var configuration: StableDiffusionConfiguration {\n            switch self {\n            case .base: presetStableDiffusion21Base\n            case .sdxlTurbo: presetSDXLTurbo\n            }\n        }\n    }\n\n    /// See https://huggingface.co/stabilityai/sdxl-turbo for the model details and license\n    public static let presetSDXLTurbo = StableDiffusionConfiguration(\n        id: \"stabilityai/sdxl-turbo\",\n        files: [\n            .unetConfig: \"unet/config.json\",\n            .unetWeights: \"unet/diffusion_pytorch_model.safetensors\",\n            .textEncoderConfig: \"text_encoder/config.json\",\n            .textEncoderWeights: \"text_encoder/model.safetensors\",\n            .textEncoderConfig2: \"text_encoder_2/config.json\",\n            .textEncoderWeights2: \"text_encoder_2/model.safetensors\",\n            .vaeConfig: \"vae/config.json\",\n            .vaeWeights: \"vae/diffusion_pytorch_model.safetensors\",\n            .diffusionConfig: \"scheduler/scheduler_config.json\",\n            .tokenizerVocabulary: \"tokenizer/vocab.json\",\n            .tokenizerMerges: \"tokenizer/merges.txt\",\n            .tokenizerVocabulary2: \"tokenizer_2/vocab.json\",\n            .tokenizerMerges2: \"tokenizer_2/merges.txt\",\n        ],\n        defaultParameters: { EvaluateParameters(cfgWeight: 0, steps: 2) },\n        factory: { hub, sdConfiguration, loadConfiguration in\n            let sd = try StableDiffusionXL(\n                hub: hub, configuration: sdConfiguration, dType: loadConfiguration.dType)\n            if loadConfiguration.quantize {\n                quantize(model: sd.textEncoder, filter: { k, m in m is Linear })\n                quantize(model: sd.textEncoder2, filter: { k, m in m is Linear })\n                quantize(model: sd.unet, groupSize: 32, bits: 8)\n            }\n            return sd\n        }\n    )\n\n    /// See https://huggingface.co/stabilityai/stable-diffusion-2-1-base for the model details and license\n    public static let presetStableDiffusion21Base = StableDiffusionConfiguration(\n        id: \"stabilityai/stable-diffusion-2-1-base\",\n        files: [\n            .unetConfig: \"unet/config.json\",\n            .unetWeights: \"unet/diffusion_pytorch_model.safetensors\",\n            .textEncoderConfig: \"text_encoder/config.json\",\n            .textEncoderWeights: \"text_encoder/model.safetensors\",\n            .vaeConfig: \"vae/config.json\",\n            .vaeWeights: \"vae/diffusion_pytorch_model.safetensors\",\n            .diffusionConfig: \"scheduler/scheduler_config.json\",\n            .tokenizerVocabulary: \"tokenizer/vocab.json\",\n            .tokenizerMerges: \"tokenizer/merges.txt\",\n        ],\n        defaultParameters: { EvaluateParameters(cfgWeight: 7.5, steps: 50) },\n        factory: { hub, sdConfiguration, loadConfiguration in\n            let sd = try StableDiffusionBase(\n                hub: hub, configuration: sdConfiguration, dType: loadConfiguration.dType)\n            if loadConfiguration.quantize {\n                quantize(model: sd.textEncoder, filter: { k, m in m is Linear })\n                quantize(model: sd.unet, groupSize: 32, bits: 8)\n            }\n            return sd\n        }\n    )\n\n}\n\n// MARK: - Key Mapping\n\nfunc keyReplace(_ replace: String, _ with: String) -> @Sendable (String) -> String? {\n    return { [replace, with] key in\n        if key.contains(replace) {\n            return key.replacingOccurrences(of: replace, with: with)\n        }\n        return nil\n    }\n}\n\nfunc dropPrefix(_ prefix: String) -> @Sendable (String) -> String? {\n    return { [prefix] key in\n        if key.hasPrefix(prefix) {\n            return String(key.dropFirst(prefix.count))\n        }\n        return nil\n    }\n}\n\n// see map_unet_weights()\n\nlet unetRules: [@Sendable (String) -> String?] = [\n    // Map up/downsampling\n    keyReplace(\"downsamplers.0.conv\", \"downsample\"),\n    keyReplace(\"upsamplers.0.conv\", \"upsample\"),\n\n    // Map the mid block\n    keyReplace(\"mid_block.resnets.0\", \"mid_blocks.0\"),\n    keyReplace(\"mid_block.attentions.0\", \"mid_blocks.1\"),\n    keyReplace(\"mid_block.resnets.1\", \"mid_blocks.2\"),\n\n    // Map attention layers\n    keyReplace(\"to_k\", \"key_proj\"),\n    keyReplace(\"to_out.0\", \"out_proj\"),\n    keyReplace(\"to_q\", \"query_proj\"),\n    keyReplace(\"to_v\", \"value_proj\"),\n\n    // Map transformer ffn\n    keyReplace(\"ff.net.2\", \"linear3\"),\n]\n\nfunc unetRemap(key: String, value: MLXArray) -> [(String, MLXArray)] {\n    var key = key\n    var value = value\n\n    for rule in unetRules {\n        key = rule(key) ?? key\n    }\n\n    // Map transformer ffn\n    if key.contains(\"ff.net.0\") {\n        let k1 = key.replacingOccurrences(of: \"ff.net.0.proj\", with: \"linear1\")\n        let k2 = key.replacingOccurrences(of: \"ff.net.0.proj\", with: \"linear2\")\n        let (v1, v2) = value.split()\n        return [(k1, v1), (k2, v2)]\n    }\n\n    if key.contains(\"conv_shortcut.weight\") {\n        value = value.squeezed()\n    }\n\n    // Transform the weights from 1x1 convs to linear\n    if value.ndim == 4 && (key.contains(\"proj_in\") || key.contains(\"proj_out\")) {\n        value = value.squeezed()\n    }\n\n    if value.ndim == 4 {\n        value = value.transposed(0, 2, 3, 1)\n        value = value.reshaped(-1).reshaped(value.shape)\n    }\n\n    return [(key, value)]\n}\n\nlet clipRules: [@Sendable (String) -> String?] = [\n    dropPrefix(\"text_model.\"),\n    dropPrefix(\"embeddings.\"),\n    dropPrefix(\"encoder.\"),\n\n    // Map attention layers\n    keyReplace(\"self_attn.\", \"attention.\"),\n    keyReplace(\"q_proj.\", \"query_proj.\"),\n    keyReplace(\"k_proj.\", \"key_proj.\"),\n    keyReplace(\"v_proj.\", \"value_proj.\"),\n\n    // Map ffn layers\n    keyReplace(\"mlp.fc1\", \"linear1\"),\n    keyReplace(\"mlp.fc2\", \"linear2\"),\n]\n\nfunc clipRemap(key: String, value: MLXArray) -> [(String, MLXArray)] {\n    var key = key\n\n    for rule in clipRules {\n        key = rule(key) ?? key\n    }\n\n    // not used\n    if key == \"position_ids\" {\n        return []\n    }\n\n    return [(key, value)]\n}\n\nlet vaeRules: [@Sendable (String) -> String?] = [\n    // Map up/downsampling\n    keyReplace(\"downsamplers.0.conv\", \"downsample\"),\n    keyReplace(\"upsamplers.0.conv\", \"upsample\"),\n\n    // Map attention layers\n    keyReplace(\"to_k\", \"key_proj\"),\n    keyReplace(\"to_out.0\", \"out_proj\"),\n    keyReplace(\"to_q\", \"query_proj\"),\n    keyReplace(\"to_v\", \"value_proj\"),\n\n    // Map the mid block\n    keyReplace(\"mid_block.resnets.0\", \"mid_blocks.0\"),\n    keyReplace(\"mid_block.attentions.0\", \"mid_blocks.1\"),\n    keyReplace(\"mid_block.resnets.1\", \"mid_blocks.2\"),\n\n    keyReplace(\"mid_blocks.1.key.\", \"mid_blocks.1.key_proj.\"),\n    keyReplace(\"mid_blocks.1.query.\", \"mid_blocks.1.query_proj.\"),\n    keyReplace(\"mid_blocks.1.value.\", \"mid_blocks.1.value_proj.\"),\n    keyReplace(\"mid_blocks.1.proj_attn.\", \"mid_blocks.1.out_proj.\"),\n\n]\n\nfunc vaeRemap(key: String, value: MLXArray) -> [(String, MLXArray)] {\n    var key = key\n    var value = value\n\n    for rule in vaeRules {\n        key = rule(key) ?? key\n    }\n\n    // Map the quant/post_quant layers\n    if key.contains(\"quant_conv\") {\n        key = key.replacingOccurrences(of: \"quant_conv\", with: \"quant_proj\")\n        value = value.squeezed()\n    }\n\n    // Map the conv_shortcut to linear\n    if key.contains(\"conv_shortcut.weight\") {\n        value = value.squeezed()\n    }\n\n    if value.ndim == 4 {\n        value = value.transposed(0, 2, 3, 1)\n        value = value.reshaped(-1).reshaped(value.shape)\n    }\n\n    return [(key, value)]\n}\n\nfunc loadWeights(\n    url: URL, model: Module, mapper: (String, MLXArray) -> [(String, MLXArray)], dType: DType\n) throws {\n    let weights = try loadArrays(url: url).flatMap { mapper($0.key, $0.value.asType(dType)) }\n\n    // Note: not using verifier because some shapes change upon load\n    try model.update(parameters: ModuleParameters.unflattened(weights), verify: .none)\n}\n\n// MARK: - Loading\n\nfunc resolve(hub: HubApi, configuration: StableDiffusionConfiguration, key: FileKey) -> URL {\n    precondition(\n        configuration.files[key] != nil, \"configuration \\(configuration.id) missing key: \\(key)\")\n    let repo = Hub.Repo(id: configuration.id)\n    let directory = hub.localRepoLocation(repo)\n    return directory.appending(component: configuration.files[key]!)\n}\n\nfunc loadConfiguration<T: Decodable>(\n    hub: HubApi, configuration: StableDiffusionConfiguration, key: FileKey, type: T.Type\n) throws -> T {\n    let url = resolve(hub: hub, configuration: configuration, key: key)\n    return try JSONDecoder().decode(T.self, from: Data(contentsOf: url))\n}\n\nfunc loadUnet(hub: HubApi, configuration: StableDiffusionConfiguration, dType: DType) throws\n    -> UNetModel\n{\n    let unetConfiguration = try loadConfiguration(\n        hub: hub, configuration: configuration, key: .unetConfig, type: UNetConfiguration.self)\n    let model = UNetModel(configuration: unetConfiguration)\n\n    let weightsURL = resolve(hub: hub, configuration: configuration, key: .unetWeights)\n    try loadWeights(url: weightsURL, model: model, mapper: unetRemap, dType: dType)\n\n    return model\n}\n\nfunc loadTextEncoder(\n    hub: HubApi, configuration: StableDiffusionConfiguration,\n    configKey: FileKey = .textEncoderConfig, weightsKey: FileKey = .textEncoderWeights, dType: DType\n) throws -> CLIPTextModel {\n    let clipConfiguration = try loadConfiguration(\n        hub: hub, configuration: configuration, key: configKey,\n        type: CLIPTextModelConfiguration.self)\n    let model = CLIPTextModel(configuration: clipConfiguration)\n\n    let weightsURL = resolve(hub: hub, configuration: configuration, key: weightsKey)\n    try loadWeights(url: weightsURL, model: model, mapper: clipRemap, dType: dType)\n\n    return model\n}\n\nfunc loadAutoEncoder(hub: HubApi, configuration: StableDiffusionConfiguration, dType: DType) throws\n    -> Autoencoder\n{\n    let autoEncoderConfiguration = try loadConfiguration(\n        hub: hub, configuration: configuration, key: .vaeConfig, type: AutoencoderConfiguration.self\n    )\n    let model = Autoencoder(configuration: autoEncoderConfiguration)\n\n    let weightsURL = resolve(hub: hub, configuration: configuration, key: .vaeWeights)\n    try loadWeights(url: weightsURL, model: model, mapper: vaeRemap, dType: dType)\n\n    return model\n}\n\nfunc loadDiffusionConfiguration(hub: HubApi, configuration: StableDiffusionConfiguration) throws\n    -> DiffusionConfiguration\n{\n    try loadConfiguration(\n        hub: hub, configuration: configuration, key: .diffusionConfig,\n        type: DiffusionConfiguration.self)\n}\n\n// MARK: - Tokenizer\n\nfunc loadTokenizer(\n    hub: HubApi, configuration: StableDiffusionConfiguration,\n    vocabulary: FileKey = .tokenizerVocabulary, merges: FileKey = .tokenizerMerges\n) throws -> CLIPTokenizer {\n    let vocabularyURL = resolve(hub: hub, configuration: configuration, key: vocabulary)\n    let mergesURL = resolve(hub: hub, configuration: configuration, key: merges)\n\n    let vocabulary = try JSONDecoder().decode(\n        [String: Int].self, from: Data(contentsOf: vocabularyURL))\n    let merges = try String(contentsOf: mergesURL)\n        .components(separatedBy: .newlines)\n        // first line is a comment\n        .dropFirst()\n        .filter { !$0.isEmpty }\n\n    return CLIPTokenizer(merges: merges, vocabulary: vocabulary)\n}"}]}
{"messages": [{"role": "user", "content": "convert the following MLX Python code into MLX Swift: \n\n# Copyright \u00a9 2023 Apple Inc.\n\nimport mlx.core as mx\n\nfrom .config import DiffusionConfig\n\n\ndef _linspace(a, b, num):\n    x = mx.arange(0, num) / (num - 1)\n    return (b - a) * x + a\n\n\ndef _interp(y, x_new):\n    \"\"\"Interpolate the function defined by (arange(0, len(y)), y) at positions x_new.\"\"\"\n    x_low = x_new.astype(mx.int32)\n    x_high = mx.minimum(x_low + 1, len(y) - 1)\n\n    y_low = y[x_low]\n    y_high = y[x_high]\n    delta_x = x_new - x_low\n    y_new = y_low * (1 - delta_x) + delta_x * y_high\n\n    return y_new\n\n\nclass SimpleEulerSampler:\n    \"\"\"A simple Euler integrator that can be used to sample from our diffusion models.\n\n    The method ``step()`` performs one Euler step from x_t to x_t_prev.\n    \"\"\"\n\n    def __init__(self, config: DiffusionConfig):\n        # Compute the noise schedule\n        if config.beta_schedule == \"linear\":\n            betas = _linspace(\n                config.beta_start, config.beta_end, config.num_train_steps\n            )\n        elif config.beta_schedule == \"scaled_linear\":\n            betas = _linspace(\n                config.beta_start**0.5, config.beta_end**0.5, config.num_train_steps\n            ).square()\n        else:\n            raise NotImplementedError(f\"{config.beta_schedule} is not implemented.\")\n\n        alphas = 1 - betas\n        alphas_cumprod = mx.cumprod(alphas)\n\n        self._sigmas = mx.concatenate(\n            [mx.zeros(1), ((1 - alphas_cumprod) / alphas_cumprod).sqrt()]\n        )\n\n    @property\n    def max_time(self):\n        return len(self._sigmas) - 1\n\n    def sample_prior(self, shape, dtype=mx.float32, key=None):\n        noise = mx.random.normal(shape, key=key)\n        return (\n            noise * self._sigmas[-1] * (self._sigmas[-1].square() + 1).rsqrt()\n        ).astype(dtype)\n\n    def add_noise(self, x, t, key=None):\n        noise = mx.random.normal(x.shape, key=key)\n        s = self.sigmas(t)\n        return (x + noise * s) * (s.square() + 1).rsqrt()\n\n    def sigmas(self, t):\n        return _interp(self._sigmas, t)\n\n    def timesteps(self, num_steps: int, start_time=None, dtype=mx.float32):\n        start_time = start_time or (len(self._sigmas) - 1)\n        assert 0 < start_time <= (len(self._sigmas) - 1)\n        steps = _linspace(start_time, 0, num_steps + 1).astype(dtype)\n        return list(zip(steps, steps[1:]))\n\n    def step(self, eps_pred, x_t, t, t_prev):\n        sigma = self.sigmas(t).astype(eps_pred.dtype)\n        sigma_prev = self.sigmas(t_prev).astype(eps_pred.dtype)\n\n        dt = sigma_prev - sigma\n        x_t_prev = (sigma.square() + 1).sqrt() * x_t + eps_pred * dt\n\n        x_t_prev = x_t_prev * (sigma_prev.square() + 1).rsqrt()\n\n        return x_t_prev\n\n\nclass SimpleEulerAncestralSampler(SimpleEulerSampler):\n    def step(self, eps_pred, x_t, t, t_prev):\n        sigma = self.sigmas(t).astype(eps_pred.dtype)\n        sigma_prev = self.sigmas(t_prev).astype(eps_pred.dtype)\n\n        sigma2 = sigma.square()\n        sigma_prev2 = sigma_prev.square()\n        sigma_up = (sigma_prev2 * (sigma2 - sigma_prev2) / sigma2).sqrt()\n        sigma_down = (sigma_prev2 - sigma_up**2).sqrt()\n\n        dt = sigma_down - sigma\n        x_t_prev = (sigma2 + 1).sqrt() * x_t + eps_pred * dt\n        noise = mx.random.normal(x_t_prev.shape).astype(x_t_prev.dtype)\n        x_t_prev = x_t_prev + noise * sigma_up\n\n        x_t_prev = x_t_prev * (sigma_prev2 + 1).rsqrt()\n\n        return x_t_prev"}, {"role": "assistant", "content": "// Copyright \u00a9 2024 Apple Inc.\n\nimport Foundation\nimport MLX\n\n// port of https://github.com/ml-explore/mlx-examples/blob/main/stable_diffusion/stable_diffusion/sampler.py\n\n/// Interpolate the function defined by `(0 ..< y.count) y)` at positions `xNew`.\nfunc interpolate(y: MLXArray, xNew: MLXArray) -> MLXArray {\n    let xLow = xNew.asType(.int32)\n    let xHigh = minimum(xLow + 1, y.count - 1)\n\n    let yLow = y[xLow]\n    let yHigh = y[xHigh]\n    let deltaX = xNew - xLow\n    let yNew = yLow * (1 - deltaX) + deltaX * yHigh\n\n    return yNew\n}\n\n/// A simple Euler integrator that can be used to sample from our diffusion models.\n///\n/// The method ``step()`` performs one Euler step from `x_t` to `x_t_prev`.\nclass SimpleEulerSampler {\n\n    let sigmas: MLXArray\n\n    public init(configuration: DiffusionConfiguration) {\n        let betas: MLXArray\n\n        // compute the noise schedule\n        switch configuration.betaSchedule {\n        case .linear:\n            betas = MLXArray.linspace(\n                configuration.betaStart, configuration.betaEnd, count: configuration.trainSteps)\n        case .scaledLinear:\n            betas = MLXArray.linspace(\n                sqrt(configuration.betaStart), sqrt(configuration.betaEnd),\n                count: configuration.trainSteps\n            ).square()\n        }\n\n        let alphas = 1 - betas\n        let alphasCumprod = cumprod(alphas)\n\n        self.sigmas = concatenated([\n            MLXArray.zeros([1]), ((1 - alphasCumprod) / alphasCumprod).sqrt(),\n        ])\n    }\n\n    public var maxTime: Int {\n        sigmas.count - 1\n    }\n\n    public func samplePrior(shape: [Int], dType: DType = .float32, key: MLXArray? = nil) -> MLXArray\n    {\n        let noise = MLXRandom.normal(shape, key: key)\n        return (noise * sigmas[-1] * (sigmas[-1].square() + 1).rsqrt()).asType(dType)\n    }\n\n    public func addNoise(x: MLXArray, t: MLXArray, key: MLXArray? = nil) -> MLXArray {\n        let noise = MLXRandom.normal(x.shape, key: key)\n        let s = sigmas(t)\n        return (x + noise * s) * (s.square() + 1).rsqrt()\n    }\n\n    public func sigmas(_ t: MLXArray) -> MLXArray {\n        interpolate(y: sigmas, xNew: t)\n    }\n\n    public func timeSteps(steps: Int, start: Int? = nil, dType: DType = .float32) -> [(\n        MLXArray, MLXArray\n    )] {\n        let start = start ?? (sigmas.count - 1)\n        precondition(0 < start)\n        precondition(start <= sigmas.count - 1)\n        let steps = MLX.linspace(start, 0, count: steps + 1).asType(dType)\n\n        return Array(zip(steps, steps[1...]))\n    }\n\n    open func step(epsPred: MLXArray, xt: MLXArray, t: MLXArray, tPrev: MLXArray) -> MLXArray {\n        let dtype = epsPred.dtype\n        let sigma = sigmas(t).asType(dtype)\n        let sigmaPrev = sigmas(tPrev).asType(dtype)\n\n        let dt = sigmaPrev - sigma\n        var xtPrev = (sigma.square() + 1).sqrt() * xt + epsPred * dt\n        xtPrev = xtPrev * (sigmaPrev.square() + 1).rsqrt()\n\n        return xtPrev\n    }\n}\n\nclass SimpleEulerAncestralSampler: SimpleEulerSampler {\n\n    open override func step(epsPred: MLXArray, xt: MLXArray, t: MLXArray, tPrev: MLXArray)\n        -> MLXArray\n    {\n        let dtype = epsPred.dtype\n        let sigma = sigmas(t).asType(dtype)\n        let sigmaPrev = sigmas(tPrev).asType(dtype)\n\n        let sigma2 = sigma.square()\n        let sigmaPrev2 = sigmaPrev.square()\n        let sigmaUp = (sigmaPrev2 * (sigma2 - sigmaPrev2) / sigma2).sqrt()\n        let sigmaDown = (sigmaPrev2 - sigmaUp ** 2).sqrt()\n\n        let dt = sigmaDown - sigma\n        var xtPrev = (sigma2 + 1).sqrt() * xt + epsPred * dt\n        let noise = MLXRandom.normal(xtPrev.shape).asType(xtPrev.dtype)\n        xtPrev = xtPrev + noise * sigmaUp\n        xtPrev = xtPrev * (sigmaPrev2 + 1).rsqrt()\n\n        return xtPrev\n    }\n}"}]}
{"messages": [{"role": "user", "content": "convert the following MLX Python code into MLX Swift: \n\n# Copyright \u00a9 2023-2024 Apple Inc.\n\nfrom dataclasses import dataclass\nfrom functools import partial\nfrom typing import Any, Dict, Optional, Union\n\nimport mlx.core as mx\nimport mlx.nn as nn\n\nfrom .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention\nfrom .bitlinear_layers import BitLinear\nfrom .rope_utils import initialize_rope\n\n\n@dataclass\nclass ModelArgs(BaseModelArgs):\n    model_type: str\n    hidden_size: int\n    num_hidden_layers: int\n    intermediate_size: int\n    num_attention_heads: int\n    num_key_value_heads: int\n    rms_norm_eps: float\n    vocab_size: int\n    head_dim: Optional[int] = None\n    max_position_embeddings: Optional[int] = None\n    attention_bias: bool = False\n    mlp_bias: bool = False\n    rope_theta: float = 10000\n    rope_traditional: bool = False\n    rope_scaling: Optional[Dict[str, Union[float, str]]] = None\n    tie_word_embeddings: bool = True\n\n\nclass Attention(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n\n        dim = args.hidden_size\n        self.n_heads = n_heads = args.num_attention_heads\n        self.n_kv_heads = n_kv_heads = args.num_key_value_heads\n\n        self.head_dim = head_dim = args.head_dim or args.hidden_size // n_heads\n\n        self.scale = head_dim**-0.5\n        attention_bias = args.attention_bias\n\n        self.q_proj = BitLinear(dim, n_heads * head_dim, bias=attention_bias)\n        self.k_proj = BitLinear(dim, n_kv_heads * head_dim, bias=attention_bias)\n        self.v_proj = BitLinear(dim, n_kv_heads * head_dim, bias=attention_bias)\n        self.o_proj = BitLinear(n_heads * head_dim, dim, bias=attention_bias)\n\n        self.rope = initialize_rope(\n            self.head_dim,\n            args.rope_theta,\n            args.rope_traditional,\n            args.rope_scaling,\n            args.max_position_embeddings,\n        )\n        self.attn_sub_norm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)\n\n    def __call__(\n        self,\n        x: mx.array,\n        mask: Optional[mx.array] = None,\n        cache: Optional[Any] = None,\n    ) -> mx.array:\n        B, L, D = x.shape\n\n        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)\n\n        # Prepare the queries, keys and values for the attention computation\n        queries = queries.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)\n        keys = keys.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)\n        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)\n\n        if cache is not None:\n            queries = self.rope(queries, offset=cache.offset)\n            keys = self.rope(keys, offset=cache.offset)\n            keys, values = cache.update_and_fetch(keys, values)\n        else:\n            queries = self.rope(queries)\n            keys = self.rope(keys)\n\n        output = scaled_dot_product_attention(\n            queries, keys, values, cache=cache, scale=self.scale, mask=mask\n        )\n\n        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)\n        output = self.attn_sub_norm(output)\n        output = self.o_proj(output)\n\n        return output\n\n\n@partial(mx.compile, shapeless=True)\ndef relu2(x):\n    return mx.square(nn.relu(x))\n\n\nclass MLP(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n\n        dim = args.hidden_size\n        hidden_dim = args.intermediate_size\n        if hasattr(args, \"mlp_bias\"):\n            mlp_bias = args.mlp_bias\n        else:\n            mlp_bias = False\n\n        self.gate_proj = BitLinear(dim, hidden_dim, bias=mlp_bias)\n        self.down_proj = BitLinear(hidden_dim, dim, bias=mlp_bias)\n        self.up_proj = BitLinear(dim, hidden_dim, bias=mlp_bias)\n        self.ffn_sub_norm = nn.RMSNorm(args.intermediate_size, eps=args.rms_norm_eps)\n\n    def __call__(self, x) -> mx.array:\n        x = relu2(self.gate_proj(x)) * self.up_proj(x)\n        x = self.ffn_sub_norm(x)\n        x = self.down_proj(x)\n        return x\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.num_attention_heads = args.num_attention_heads\n        self.hidden_size = args.hidden_size\n        self.self_attn = Attention(args)\n        self.mlp = MLP(args)\n        self.input_layernorm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)\n        self.post_attention_layernorm = nn.RMSNorm(\n            args.hidden_size, eps=args.rms_norm_eps\n        )\n\n    def __call__(\n        self,\n        x: mx.array,\n        mask: Optional[mx.array] = None,\n        cache: Optional[Any] = None,\n    ) -> mx.array:\n\n        r = self.self_attn(self.input_layernorm(x), mask, cache)\n        h = x + r\n        r = self.mlp(self.post_attention_layernorm(h))\n        out = h + r\n        return out\n\n\nclass LlamaModel(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.args = args\n        self.vocab_size = args.vocab_size\n        self.num_hidden_layers = args.num_hidden_layers\n        self.embed_tokens = nn.Embedding(args.vocab_size, args.hidden_size)\n        self.layers = [\n            TransformerBlock(args=args) for _ in range(args.num_hidden_layers)\n        ]\n        self.norm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)\n\n    def __call__(\n        self,\n        inputs: mx.array,\n        mask: mx.array = None,\n        cache=None,\n    ):\n        h = self.embed_tokens(inputs)\n\n        if mask is None:\n            mask = create_attention_mask(h, cache)\n\n        if cache is None:\n            cache = [None] * len(self.layers)\n\n        for layer, c in zip(self.layers, cache):\n            h = layer(h, mask, cache=c)\n\n        return self.norm(h)\n\n\nclass Model(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.args = args\n        self.model_type = args.model_type\n        self.model = LlamaModel(args)\n        if not args.tie_word_embeddings:\n            self.lm_head = nn.Linear(args.hidden_size, args.vocab_size, bias=False)\n\n    def __call__(\n        self,\n        inputs: mx.array,\n        mask: mx.array = None,\n        cache=None,\n    ):\n        out = self.model(inputs, mask, cache)\n        if self.args.tie_word_embeddings:\n            out = self.model.embed_tokens.as_linear(out)\n        else:\n            out = self.lm_head(out)\n        return out\n\n    def sanitize(self, weights):\n        # Remove unused precomputed rotary freqs\n        weights = {\n            k: v for k, v in weights.items() if \"self_attn.rotary_emb.inv_freq\" not in k\n        }\n        if self.args.tie_word_embeddings:\n            weights.pop(\"lm_head.weight\", None)\n        return weights\n\n    @property\n    def layers(self):\n        return self.model.layers"}, {"role": "assistant", "content": "//\n//  Bitnet.swift\n//  mlx-swift-examples\n//\n//  Created by John Mai on 2025/6/12.\n//\n\nimport Foundation\nimport MLX\nimport MLXFast\nimport MLXLMCommon\nimport MLXNN\nimport Tokenizers\n\n// port of https://github.com/ml-explore/mlx-lm/blob/main/mlx_lm/models/bitnet.py\n\nprivate func makeBitLinearKernel() -> MLXFast.MLXFastKernel {\n    let source = \"\"\"\n        constexpr int M = 4;\n        constexpr int BLOCK = 32;\n\n        uint tid = thread_position_in_grid.y;\n        uint in_offset = thread_position_in_grid.x;\n\n        uint batch_idx = tid / (out_features / 4);\n        uint row_idx = tid % (out_features / 4);\n\n        float sum[4] = {0.0};\n\n        for (uint i = in_offset * M; i < in_features; i += BLOCK * M) {\n            float v[M];\n            for (int j=0; j<M; j++) {\n                v[j] = x[batch_idx * in_features + i + j];\n            }\n\n            for (int j=0; j<M; j++) {\n                uint8_t w = packed_weights[row_idx * in_features + i + j];\n                sum[0] += v[j] * ((w & 3) - 1);\n                sum[1] += v[j] * (((w >> 2) & 3) - 1);\n                sum[2] += v[j] * (((w >> 4) & 3) - 1);\n                sum[3] += v[j] * (((w >> 6) & 3) - 1);\n            }\n        }\n\n        for (int j=0; j<4; j++) {\n            sum[j] = simd_sum(sum[j]);\n        }\n\n        // Apply weight scaling by diving them or multiplying them\n        if (in_offset == 0) {\n            float scale = invert_weight_scales ? 1 / weight_scale[0] : weight_scale[0];\n            for (int i=0; i<4; i++) {\n                out[batch_idx * out_features + row_idx + i * (out_features/4)] = static_cast<T>(sum[i] * scale);\n            }\n        }\n        \"\"\"\n\n    return metalKernel(\n        name: \"bitlinear_matmul\",\n        inputNames: [\"x\", \"packed_weights\", \"weight_scale\"],\n        outputNames: [\"out\"],\n        source: source\n    )\n}\n\nprivate final class BitLinearKernelManager: @unchecked Sendable {\n    static let shared = BitLinearKernelManager()\n\n    let bitlinearKernel: MLXFast.MLXFastKernel\n\n    private init() {\n        bitlinearKernel = makeBitLinearKernel()\n    }\n}\n\nprivate class BitLinear: Module {\n    let inFeatures: Int\n    let outFeatures: Int\n    let invertWeightScales: Bool\n\n    let weight: MLXArray\n    let bias: MLXArray?\n    @ModuleInfo(key: \"weight_scale\") var weightScale: MLXArray\n\n    init(\n        _ inFeatures: Int,\n        _ outFeatures: Int,\n        bias: Bool = true,\n        invertWeightScales: Bool = false\n    ) {\n        self.inFeatures = inFeatures\n        self.outFeatures = outFeatures\n\n        let packedOutFeatures = Int(floor(Double(outFeatures + 3) / 4.0))\n        self.weight = MLXArray.zeros([packedOutFeatures, inFeatures], dtype: .uint8)\n\n        self.invertWeightScales = invertWeightScales\n        self._weightScale.wrappedValue = MLXArray([1.0])\n\n        if bias {\n            self.bias = MLXArray.zeros([outFeatures])\n        } else {\n            self.bias = nil\n        }\n\n        super.init()\n    }\n\n    private func executeMatmulKernel(_ x: MLXArray, _ packedWeights: MLXArray) -> MLXArray {\n        let originalShape = x.shape\n        var x = x\n\n        if originalShape.count > 2 {\n            x = x.reshaped(-1, originalShape[originalShape.count - 1])\n        }\n\n        let totalBatchElements = x.dim(0)\n        let inFeatures = x.dim(1)\n\n        let outFeatures = self.outFeatures\n\n        let dtype = self.weightScale.dtype\n        assert(x.dtype == dtype, \"Wrong type for input.\")\n\n        var outputs = BitLinearKernelManager.shared.bitlinearKernel(\n            [x, packedWeights, weightScale],\n            template: [\n                (\"T\", dtype),\n                (\"invert_weight_scales\", invertWeightScales),\n                (\"in_features\", inFeatures),\n                (\"out_features\", outFeatures),\n            ],\n            grid: (32, Int(floor(Double(totalBatchElements * outFeatures / 4))), 1),\n            threadGroup: (32, 1, 1),\n            outputShapes: [[totalBatchElements, outFeatures]],\n            outputDTypes: [dtype]\n        )[0]\n\n        if originalShape.count > 2 {\n            outputs = outputs.reshaped(Array(originalShape.dropLast()) + [outFeatures])\n        }\n\n        return outputs\n    }\n\n    func callAsFunction(_ x: MLXArray) -> MLXArray {\n        var y = executeMatmulKernel(x, weight)\n\n        if let bias {\n            y = y + bias\n        }\n        return y\n    }\n}\n\n// MARK: - Model Configuration\n\npublic struct BitnetConfiguration: Codable, Sendable {\n    var modelType: String\n    var hiddenSize: Int\n    var hiddenLayers: Int\n    var intermediateSize: Int\n    var attentionHeads: Int\n    var rmsNormEps: Float\n    var vocabularySize: Int\n    var headDimensions: Int?\n    var maxPositionEmbeddings: Int?\n    var kvHeads: Int?\n    var attentionBias: Bool\n    var mlpBias: Bool\n    var ropeTheta: Float\n    var ropeTraditional: Bool\n    var ropeScaling: [String: StringOrNumber]?\n    var tieWordEmbeddings: Bool\n\n    public init(\n        modelType: String = \"bitnet\",\n        hiddenSize: Int,\n        hiddenLayers: Int,\n        intermediateSize: Int,\n        attentionHeads: Int,\n        rmsNormEps: Float,\n        vocabularySize: Int,\n        headDimensions: Int? = nil,\n        maxPositionEmbeddings: Int? = nil,\n        kvHeads: Int? = nil,\n        attentionBias: Bool = false,\n        mlpBias: Bool = false,\n        ropeTheta: Float = 10000,\n        ropeTraditional: Bool = false,\n        ropeScaling: [String: StringOrNumber]? = nil,\n        tieWordEmbeddings: Bool = true\n    ) {\n        self.modelType = modelType\n        self.hiddenSize = hiddenSize\n        self.hiddenLayers = hiddenLayers\n        self.intermediateSize = intermediateSize\n        self.attentionHeads = attentionHeads\n        self.rmsNormEps = rmsNormEps\n        self.vocabularySize = vocabularySize\n        self.headDimensions = headDimensions\n        self.maxPositionEmbeddings = maxPositionEmbeddings\n        self.kvHeads = kvHeads ?? attentionHeads\n        self.attentionBias = attentionBias\n        self.mlpBias = mlpBias\n        self.ropeTheta = ropeTheta\n        self.ropeTraditional = ropeTraditional\n        self.ropeScaling = ropeScaling\n        self.tieWordEmbeddings = tieWordEmbeddings\n    }\n\n    var resolvedKvHeads: Int {\n        kvHeads ?? attentionHeads\n    }\n\n    var resolvedHeadDimensions: Int {\n        headDimensions ?? (hiddenSize / attentionHeads)\n    }\n\n    enum CodingKeys: String, CodingKey {\n        case modelType = \"model_type\"\n        case hiddenSize = \"hidden_size\"\n        case hiddenLayers = \"num_hidden_layers\"\n        case intermediateSize = \"intermediate_size\"\n        case attentionHeads = \"num_attention_heads\"\n        case rmsNormEps = \"rms_norm_eps\"\n        case vocabularySize = \"vocab_size\"\n        case headDimensions = \"head_dim\"\n        case maxPositionEmbeddings = \"max_position_embeddings\"\n        case kvHeads = \"num_key_value_heads\"\n        case attentionBias = \"attention_bias\"\n        case mlpBias = \"mlp_bias\"\n        case ropeTheta = \"rope_theta\"\n        case ropeTraditional = \"rope_traditional\"\n        case ropeScaling = \"rope_scaling\"\n        case tieWordEmbeddings = \"tie_word_embeddings\"\n    }\n\n    public init(from decoder: Swift.Decoder) throws {\n        let container = try decoder.container(keyedBy: CodingKeys.self)\n\n        modelType = try container.decodeIfPresent(String.self, forKey: .modelType) ?? \"bitnet\"\n        hiddenSize = try container.decode(Int.self, forKey: .hiddenSize)\n        hiddenLayers = try container.decode(Int.self, forKey: .hiddenLayers)\n        intermediateSize = try container.decode(Int.self, forKey: .intermediateSize)\n        attentionHeads = try container.decode(Int.self, forKey: .attentionHeads)\n        rmsNormEps = try container.decode(Float.self, forKey: .rmsNormEps)\n        vocabularySize = try container.decode(Int.self, forKey: .vocabularySize)\n        headDimensions = try container.decodeIfPresent(Int.self, forKey: .headDimensions)\n        maxPositionEmbeddings = try container.decodeIfPresent(\n            Int.self, forKey: .maxPositionEmbeddings\n        )\n        kvHeads = try container.decodeIfPresent(Int.self, forKey: .kvHeads) ?? attentionHeads\n        attentionBias = try container.decodeIfPresent(Bool.self, forKey: .attentionBias) ?? false\n        mlpBias = try container.decodeIfPresent(Bool.self, forKey: .mlpBias) ?? false\n        ropeTheta = try container.decodeIfPresent(Float.self, forKey: .ropeTheta) ?? 10000\n        ropeTraditional =\n            try container.decodeIfPresent(Bool.self, forKey: .ropeTraditional) ?? false\n        ropeScaling = try container.decodeIfPresent(\n            [String: StringOrNumber].self, forKey: .ropeScaling\n        )\n        tieWordEmbeddings =\n            try container.decodeIfPresent(Bool.self, forKey: .tieWordEmbeddings) ?? true\n    }\n}\n\n// MARK: - Attention\n\nprivate class Attention: Module {\n    let args: BitnetConfiguration\n    let scale: Float\n\n    @ModuleInfo(key: \"q_proj\") var qProj: BitLinear\n    @ModuleInfo(key: \"k_proj\") var kProj: BitLinear\n    @ModuleInfo(key: \"v_proj\") var vProj: BitLinear\n    @ModuleInfo(key: \"o_proj\") var oProj: BitLinear\n\n    @ModuleInfo(key: \"attn_sub_norm\") var attnSubNorm: RMSNorm\n\n    let rope: RoPE\n\n    init(_ args: BitnetConfiguration) {\n        self.args = args\n\n        let dim = args.hiddenSize\n        let headDim = args.resolvedHeadDimensions\n        let nHeads = args.attentionHeads\n        let nKvHeads = args.resolvedKvHeads\n\n        scale = pow(Float(headDim), -0.5)\n\n        _qProj.wrappedValue = BitLinear(dim, nHeads * headDim, bias: args.attentionBias)\n        _kProj.wrappedValue = BitLinear(dim, nKvHeads * headDim, bias: args.attentionBias)\n        _vProj.wrappedValue = BitLinear(dim, nKvHeads * headDim, bias: args.attentionBias)\n        _oProj.wrappedValue = BitLinear(nHeads * headDim, dim, bias: args.attentionBias)\n\n        _attnSubNorm.wrappedValue = RMSNorm(dimensions: args.hiddenSize, eps: args.rmsNormEps)\n\n        let ropeScale: Float\n        if let ropeScaling = args.ropeScaling, ropeScaling[\"type\"] == .string(\"linear\"),\n            let factor = ropeScaling[\"factor\"]\n        {\n            if let v = factor.asFloat() {\n                ropeScale = 1 / v\n            } else {\n                fatalError(\"ropeScaling.factor must be a float\")\n            }\n        } else {\n            ropeScale = 1\n        }\n\n        rope = RoPE(\n            dimensions: headDim, traditional: args.ropeTraditional, base: args.ropeTheta,\n            scale: ropeScale\n        )\n    }\n\n    func callAsFunction(\n        _ x: MLXArray,\n        mask: MLXFast.ScaledDotProductAttentionMaskMode,\n        cache: KVCache?\n    ) -> MLXArray {\n        let (B, L) = (x.dim(0), x.dim(1))\n\n        var queries = qProj(x)\n        var keys = kProj(x)\n        var values = vProj(x)\n\n        queries = queries.reshaped(B, L, args.attentionHeads, -1).transposed(0, 2, 1, 3)\n        keys = keys.reshaped(B, L, args.resolvedKvHeads, -1).transposed(0, 2, 1, 3)\n        values = values.reshaped(B, L, args.resolvedKvHeads, -1).transposed(0, 2, 1, 3)\n\n        if let cache {\n            queries = rope(queries, offset: cache.offset)\n            keys = rope(keys, offset: cache.offset)\n            (keys, values) = cache.update(keys: keys, values: values)\n        } else {\n            queries = rope(queries)\n            keys = rope(keys)\n        }\n\n        let output = MLXFast.scaledDotProductAttention(\n            queries: queries,\n            keys: keys,\n            values: values,\n            scale: scale,\n            mask: mask\n        )\n        .transposed(0, 2, 1, 3)\n        .reshaped(B, L, -1)\n\n        let normedOutput = attnSubNorm(output)\n        return oProj(normedOutput)\n    }\n}\n\n// MARK: - MLP\n\nprivate class MLP: Module {\n    @ModuleInfo(key: \"gate_proj\") var gateProj: BitLinear\n    @ModuleInfo(key: \"down_proj\") var downProj: BitLinear\n    @ModuleInfo(key: \"up_proj\") var upProj: BitLinear\n\n    @ModuleInfo(key: \"ffn_sub_norm\") var ffnSubNorm: RMSNorm\n\n    init(_ args: BitnetConfiguration) {\n        let dim = args.hiddenSize\n        let hiddenDim = args.intermediateSize\n\n        _gateProj.wrappedValue = BitLinear(dim, hiddenDim, bias: args.mlpBias)\n        _downProj.wrappedValue = BitLinear(hiddenDim, dim, bias: args.mlpBias)\n        _upProj.wrappedValue = BitLinear(dim, hiddenDim, bias: args.mlpBias)\n        _ffnSubNorm.wrappedValue = RMSNorm(dimensions: args.intermediateSize, eps: args.rmsNormEps)\n    }\n\n    func callAsFunction(_ x: MLXArray) -> MLXArray {\n        let gated = reluSquared(gateProj(x)) * upProj(x)\n        let normed = ffnSubNorm(gated)\n        return downProj(normed)\n    }\n}\n\n// MARK: - Transformer Block\n\nprivate class TransformerBlock: Module {\n    @ModuleInfo(key: \"self_attn\") var attention: Attention\n    var mlp: MLP\n\n    @ModuleInfo(key: \"input_layernorm\") var inputLayerNorm: RMSNorm\n    @ModuleInfo(key: \"post_attention_layernorm\") var postAttentionLayerNorm: RMSNorm\n\n    init(_ args: BitnetConfiguration) {\n        _attention.wrappedValue = Attention(args)\n        mlp = MLP(args)\n        _inputLayerNorm.wrappedValue = RMSNorm(\n            dimensions: args.hiddenSize, eps: args.rmsNormEps\n        )\n        _postAttentionLayerNorm.wrappedValue = RMSNorm(\n            dimensions: args.hiddenSize, eps: args.rmsNormEps\n        )\n    }\n\n    func callAsFunction(\n        _ x: MLXArray,\n        mask: MLXFast.ScaledDotProductAttentionMaskMode,\n        cache: KVCache?\n    ) -> MLXArray {\n        var r = attention(inputLayerNorm(x), mask: mask, cache: cache)\n        let h = x + r\n        r = mlp(postAttentionLayerNorm(h))\n        let out = h + r\n        return out\n    }\n}\n\n// MARK: - Bitnet Model Inner\n\nprivate class BitnetModelInner: Module {\n    @ModuleInfo(key: \"embed_tokens\") var embedTokens: Embedding\n\n    fileprivate let layers: [TransformerBlock]\n    var norm: RMSNorm\n\n    init(_ args: BitnetConfiguration) {\n        precondition(args.vocabularySize > 0)\n\n        _embedTokens.wrappedValue = Embedding(\n            embeddingCount: args.vocabularySize, dimensions: args.hiddenSize\n        )\n\n        layers = (0 ..< args.hiddenLayers).map { _ in\n            TransformerBlock(args)\n        }\n        norm = RMSNorm(dimensions: args.hiddenSize, eps: args.rmsNormEps)\n    }\n\n    func callAsFunction(_ inputs: MLXArray, cache: [KVCache]? = nil) -> MLXArray {\n        var h = embedTokens(inputs)\n\n        let mask = createAttentionMask(h: h, cache: cache)\n\n        for (i, layer) in layers.enumerated() {\n            h = layer(h, mask: mask, cache: cache?[i])\n        }\n\n        return norm(h)\n    }\n}\n\n// MARK: - Bitnet Model\n\npublic class BitnetModel: Module, LLMModel, KVCacheDimensionProvider {\n    public let vocabularySize: Int\n    public let kvHeads: [Int]\n\n    fileprivate let model: BitnetModelInner\n    let configuration: BitnetConfiguration\n\n    @ModuleInfo(key: \"lm_head\") var lmHead: Linear?\n\n    public init(_ args: BitnetConfiguration) {\n        configuration = args\n        vocabularySize = args.vocabularySize\n        kvHeads = (0 ..< args.hiddenLayers).map { _ in args.resolvedKvHeads }\n        model = BitnetModelInner(args)\n\n        if !args.tieWordEmbeddings {\n            _lmHead.wrappedValue = Linear(args.hiddenSize, args.vocabularySize, bias: false)\n        }\n    }\n\n    public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]?) -> MLXArray {\n        let out = model(inputs, cache: cache)\n        if let lmHead {\n            return lmHead(out)\n        } else {\n            return model.embedTokens.asLinear(out)\n        }\n    }\n\n    public func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {\n        var weights = weights\n\n        weights = weights.filter {\n            !$0.key.contains(\"self_attn.rotary_emb.inv_freq\")\n        }\n\n        if configuration.tieWordEmbeddings {\n            weights[\"lm_head.weight\"] = nil\n        }\n\n        return weights\n    }\n}\n\nextension BitnetModel: LoRAModel {\n    public func loraLinearLayers() -> LoRALinearLayers {\n        model.layers.map { ($0.attention, [\"q_proj\", \"v_proj\"]) }\n    }\n}"}]}
{"messages": [{"role": "user", "content": "convert the following MLX Python code into MLX Swift: \n\n# Copyright \u00a9 2023-2024 Apple Inc.\n\nfrom dataclasses import dataclass\nfrom typing import Any, Optional\n\nimport mlx.core as mx\nimport mlx.nn as nn\n\nfrom .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention\n\n\n@dataclass\nclass ModelArgs(BaseModelArgs):\n    model_type: str\n    hidden_size: int = 8192\n    num_hidden_layers: int = 40\n    intermediate_size: int = 22528\n    num_attention_heads: int = 64\n    num_key_value_heads: int = 64\n    rope_theta: float = 8000000.0\n    vocab_size: int = 256000\n    layer_norm_eps: float = 1e-05\n    logit_scale: float = 0.0625\n    attention_bias: bool = False\n    layer_norm_bias: bool = False\n    use_qk_norm: bool = False\n\n\nclass LayerNorm2D(nn.Module):\n\n    def __init__(self, d1, d2, eps):\n        super().__init__()\n        self.weight = mx.zeros((d1, d2))\n        self.eps = eps\n\n    def __call__(self, x):\n        return self.weight * mx.fast.layer_norm(x, None, None, self.eps)\n\n\nclass Attention(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.args = args\n\n        dim = args.hidden_size\n        self.n_heads = n_heads = args.num_attention_heads\n        self.n_kv_heads = n_kv_heads = args.num_key_value_heads\n\n        head_dim = args.hidden_size // args.num_attention_heads\n        self.scale = head_dim**-0.5\n\n        attetion_bias = args.attention_bias\n\n        self.q_proj = nn.Linear(dim, n_heads * head_dim, bias=attetion_bias)\n        self.k_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=attetion_bias)\n        self.v_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=attetion_bias)\n        self.o_proj = nn.Linear(n_heads * head_dim, dim, bias=attetion_bias)\n\n        self.use_qk_norm = args.use_qk_norm\n        if self.use_qk_norm:\n            self.q_norm = LayerNorm2D(self.n_heads, head_dim, eps=args.layer_norm_eps)\n            self.k_norm = LayerNorm2D(\n                self.n_kv_heads, head_dim, eps=args.layer_norm_eps\n            )\n\n        self.rope = nn.RoPE(head_dim, traditional=True, base=args.rope_theta)\n\n    def __call__(\n        self,\n        x: mx.array,\n        mask: Optional[mx.array] = None,\n        cache: Optional[Any] = None,\n    ) -> mx.array:\n        B, L, D = x.shape\n\n        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)\n\n        queries = queries.reshape(B, L, self.n_heads, -1)\n        keys = keys.reshape(B, L, self.n_kv_heads, -1)\n        if self.use_qk_norm:\n            queries = self.q_norm(queries)\n            keys = self.k_norm(keys)\n\n        queries = queries.transpose(0, 2, 1, 3)\n        keys = keys.transpose(0, 2, 1, 3)\n        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)\n\n        if cache is not None:\n            queries = self.rope(queries, offset=cache.offset)\n            keys = self.rope(keys, offset=cache.offset)\n            keys, values = cache.update_and_fetch(keys, values)\n        else:\n            queries = self.rope(queries)\n            keys = self.rope(keys)\n\n        output = scaled_dot_product_attention(\n            queries, keys, values, cache=cache, scale=self.scale, mask=mask\n        )\n\n        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)\n        return self.o_proj(output)\n\n\nclass MLP(nn.Module):\n    def __init__(self, dim, hidden_dim):\n        super().__init__()\n        self.gate_proj = nn.Linear(dim, hidden_dim, bias=False)\n        self.up_proj = nn.Linear(dim, hidden_dim, bias=False)\n        self.down_proj = nn.Linear(hidden_dim, dim, bias=False)\n\n    def __call__(self, x):\n        return self.down_proj(nn.silu(self.gate_proj(x)) * self.up_proj(x))\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.hidden_size = args.hidden_size\n        self.n_heads = args.num_attention_heads\n\n        self.self_attn = Attention(args)\n        self.mlp = MLP(args.hidden_size, args.intermediate_size)\n        self.input_layernorm = nn.LayerNorm(\n            args.hidden_size, eps=args.layer_norm_eps, bias=args.layer_norm_bias\n        )\n        self.args = args\n\n    def __call__(\n        self,\n        x: mx.array,\n        mask: Optional[mx.array] = None,\n        cache: Optional[Any] = None,\n    ) -> mx.array:\n        h = self.input_layernorm(x)\n        attn_h = self.self_attn(h, mask, cache)\n        ff_h = self.mlp(h)\n        return attn_h + ff_h + x\n\n\nclass CohereModel(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.args = args\n        self.vocab_size = args.vocab_size\n        self.num_hidden_layers = args.num_hidden_layers\n        assert self.vocab_size > 0\n        self.embed_tokens = nn.Embedding(args.vocab_size, args.hidden_size)\n        self.layers = [\n            TransformerBlock(args=args) for _ in range(args.num_hidden_layers)\n        ]\n        self.norm = nn.LayerNorm(\n            args.hidden_size, eps=args.layer_norm_eps, bias=args.layer_norm_bias\n        )\n\n    def __call__(\n        self,\n        inputs: mx.array,\n        mask: mx.array = None,\n        cache=None,\n    ):\n        h = self.embed_tokens(inputs)\n\n        if mask is None:\n            mask = create_attention_mask(h, cache)\n\n        if cache is None:\n            cache = [None] * len(self.layers)\n\n        for layer, c in zip(self.layers, cache):\n            h = layer(h, mask, c)\n\n        return self.norm(h)\n\n\nclass Model(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.model_type = args.model_type\n        self.model = CohereModel(args)\n        self.args = args\n\n    def __call__(\n        self,\n        inputs: mx.array,\n        mask: mx.array = None,\n        cache=None,\n    ):\n        out = self.model(inputs, mask, cache)\n        out = self.model.embed_tokens.as_linear(out)\n        out = out * self.model.args.logit_scale\n        return out\n\n    @property\n    def layers(self):\n        return self.model.layers"}, {"role": "assistant", "content": "import Foundation\nimport MLX\nimport MLXLMCommon\nimport MLXNN\n\n// port of https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/models/cohere.py\n\nprivate class Attention: Module {\n\n    let args: CohereConfiguration\n    let scale: Float\n\n    @ModuleInfo(key: \"q_proj\") var wq: Linear\n    @ModuleInfo(key: \"k_proj\") var wk: Linear\n    @ModuleInfo(key: \"v_proj\") var wv: Linear\n    @ModuleInfo(key: \"o_proj\") var wo: Linear\n\n    let rope: RoPE\n\n    public init(_ args: CohereConfiguration) {\n        self.args = args\n\n        let dim = args.hiddenSize\n        let heads = args.attentionHeads\n        let kvHeads = args.kvHeads\n\n        let headDim = args.hiddenSize / heads\n        self.scale = pow(Float(headDim), -0.5)\n\n        self._wq.wrappedValue = Linear(dim, heads * headDim, bias: false)\n        self._wk.wrappedValue = Linear(dim, kvHeads * headDim, bias: false)\n        self._wv.wrappedValue = Linear(dim, kvHeads * headDim, bias: false)\n        self._wo.wrappedValue = Linear(heads * headDim, dim, bias: false)\n\n        self.rope = RoPE(\n            dimensions: headDim, traditional: args.ropeTraditional, base: args.ropeTheta)\n    }\n\n    public func callAsFunction(\n        _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?\n    ) -> MLXArray {\n        let (B, L) = (x.dim(0), x.dim(1))\n\n        var queries = wq(x)\n        var keys = wk(x)\n        var values = wv(x)\n\n        // prepare the queries, keys and values for the attention computation\n        queries = queries.reshaped(B, L, args.attentionHeads, -1).transposed(0, 2, 1, 3)\n        keys = keys.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)\n        values = values.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)\n\n        if let cache {\n            queries = rope(queries, offset: cache.offset)\n            keys = rope(keys, offset: cache.offset)\n        } else {\n            queries = rope(queries)\n            keys = rope(keys)\n        }\n\n        let output = attentionWithCacheUpdate(\n            queries: queries,\n            keys: keys,\n            values: values,\n            cache: cache,\n            scale: scale,\n            mask: mask\n        )\n        .transposed(0, 2, 1, 3)\n        .reshaped(B, L, -1)\n\n        return wo(output)\n    }\n}\n\nprivate class MLP: Module, UnaryLayer {\n\n    @ModuleInfo(key: \"gate_proj\") var gate: Linear\n    @ModuleInfo(key: \"down_proj\") var down: Linear\n    @ModuleInfo(key: \"up_proj\") var up: Linear\n\n    public init(dimensions: Int, hiddenDimensions: Int) {\n        self._gate.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)\n        self._up.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)\n        self._down.wrappedValue = Linear(hiddenDimensions, dimensions, bias: false)\n\n    }\n\n    public func callAsFunction(_ x: MLXArray) -> MLXArray {\n        down(silu(gate(x)) * up(x))\n    }\n}\n\nprivate class TransformerBlock: Module {\n\n    @ModuleInfo(key: \"self_attn\") var attention: Attention\n    let mlp: MLP\n\n    @ModuleInfo(key: \"input_layernorm\") var inputLayerNorm: LayerNorm\n\n    public init(_ args: CohereConfiguration) {\n        self._attention.wrappedValue = Attention(args)\n        self.mlp = MLP(dimensions: args.hiddenSize, hiddenDimensions: args.intermediateSize)\n        self._inputLayerNorm.wrappedValue = LayerNorm(\n            dimensions: args.hiddenSize, eps: args.layerNormEps)\n\n    }\n\n    public func callAsFunction(\n        _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?\n    ) -> MLXArray {\n        let h = inputLayerNorm(x)\n        let attnH = attention(h, mask: mask, cache: cache)\n        let ffH = mlp(h)\n        return attnH + ffH + x\n    }\n}\n\npublic class CohereModelInner: Module {\n\n    @ModuleInfo(key: \"embed_tokens\") var embedTokens: Embedding\n\n    fileprivate let layers: [TransformerBlock]\n    let norm: LayerNorm\n\n    public init(_ args: CohereConfiguration) {\n        precondition(args.vocabularySize > 0)\n\n        self._embedTokens.wrappedValue = Embedding(\n            embeddingCount: args.vocabularySize, dimensions: args.hiddenSize)\n\n        self.layers = (0 ..< args.hiddenLayers)\n            .map { _ in\n                TransformerBlock(args)\n            }\n        self.norm = LayerNorm(dimensions: args.hiddenSize, eps: args.layerNormEps)\n    }\n\n    public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]? = nil) -> MLXArray {\n        var h = embedTokens(inputs)\n\n        let mask = createAttentionMask(h: h, cache: cache)\n\n        for (i, layer) in layers.enumerated() {\n            h = layer(h, mask: mask, cache: cache?[i])\n        }\n\n        return norm(h)\n    }\n}\n\npublic class CohereModel: Module, LLMModel, KVCacheDimensionProvider {\n\n    public let vocabularySize: Int\n    public let kvHeads: [Int]\n\n    let model: CohereModelInner\n    let logitScale: Float\n\n    public init(_ args: CohereConfiguration) {\n        self.vocabularySize = args.vocabularySize\n        self.kvHeads = (0 ..< args.hiddenLayers).map { _ in args.kvHeads }\n        self.model = CohereModelInner(args)\n        self.logitScale = args.logitScale\n    }\n\n    public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]?) -> MLXArray {\n        var out = model(inputs, cache: cache)\n        out = model.embedTokens.asLinear(out)\n        out = out * self.logitScale\n        return out\n    }\n}\n\npublic struct CohereConfiguration: Codable, Sendable {\n\n    var hiddenSize: Int\n    var hiddenLayers: Int\n    var intermediateSize: Int\n    var attentionHeads: Int\n    var layerNormEps: Float\n    var vocabularySize: Int\n    var kvHeads: Int\n    var ropeTheta: Float = 8000000.0\n    var ropeTraditional: Bool = true\n    var ropeScaling: [String: StringOrNumber]? = nil\n    var logitScale: Float\n\n    enum CodingKeys: String, CodingKey {\n        case hiddenSize = \"hidden_size\"\n        case hiddenLayers = \"num_hidden_layers\"\n        case intermediateSize = \"intermediate_size\"\n        case attentionHeads = \"num_attention_heads\"\n        case kvHeads = \"num_key_value_heads\"\n        case ropeTheta = \"rope_theta\"\n        case vocabularySize = \"vocab_size\"\n        case layerNormEps = \"layer_norm_eps\"\n        case logitScale = \"logit_scale\"\n        case ropeTraditional = \"rope_traditional\"\n        case ropeScaling = \"rope_scaling\"\n    }\n\n    public init(from decoder: Decoder) throws {\n        // custom implementation to handle optional keys with required values\n        let container: KeyedDecodingContainer<CohereConfiguration.CodingKeys> =\n            try decoder.container(\n                keyedBy: CohereConfiguration.CodingKeys.self)\n\n        self.hiddenSize = try container.decode(\n            Int.self, forKey: CohereConfiguration.CodingKeys.hiddenSize)\n        self.hiddenLayers = try container.decode(\n            Int.self, forKey: CohereConfiguration.CodingKeys.hiddenLayers)\n        self.intermediateSize = try container.decode(\n            Int.self, forKey: CohereConfiguration.CodingKeys.intermediateSize)\n        self.attentionHeads = try container.decode(\n            Int.self, forKey: CohereConfiguration.CodingKeys.attentionHeads)\n        self.layerNormEps = try container.decode(\n            Float.self, forKey: CohereConfiguration.CodingKeys.layerNormEps)\n        self.vocabularySize = try container.decode(\n            Int.self, forKey: CohereConfiguration.CodingKeys.vocabularySize)\n        self.kvHeads = try container.decode(\n            Int.self, forKey: CohereConfiguration.CodingKeys.kvHeads)\n        self.ropeTheta =\n            try container.decodeIfPresent(\n                Float.self, forKey: CohereConfiguration.CodingKeys.ropeTheta)\n            ?? 8000000.0\n        self.ropeScaling = try container.decodeIfPresent(\n            [String: StringOrNumber].self, forKey: CohereConfiguration.CodingKeys.ropeScaling)\n        self.logitScale = try container.decode(\n            Float.self, forKey: CohereConfiguration.CodingKeys.logitScale)\n    }\n}\n\n// MARK: - LoRA\n\nextension CohereModel: LoRAModel {\n    public func loraLinearLayers() -> LoRALinearLayers {\n        model.layers.map { ($0.attention, [\"q_proj\", \"v_proj\"]) }\n    }\n}"}]}
{"messages": [{"role": "user", "content": "convert the following MLX Python code into MLX Swift: \n\n# Copyright \u00a9 2025 Apple Inc.\n\nfrom dataclasses import dataclass\nfrom typing import Any, Optional\n\nimport mlx.core as mx\nimport mlx.nn as nn\n\nfrom .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention\n\n\n@dataclass\nclass ModelArgs(BaseModelArgs):\n    model_type: str\n    hidden_size: int\n    num_hidden_layers: int\n    intermediate_size: int\n    num_attention_heads: int\n    attention_bias: bool\n    head_dim: int\n    rms_norm_eps: float\n    vocab_size: int\n    num_key_value_heads: int\n    partial_rotary_factor: float\n    rope_theta: float\n    rope_traditional: bool = True\n    max_position_embeddings: int = 32768\n\n\nclass Glm4MLP(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.gate_up_proj = nn.Linear(\n            args.hidden_size, 2 * args.intermediate_size, bias=False\n        )\n        self.down_proj = nn.Linear(args.intermediate_size, args.hidden_size, bias=False)\n\n    def __call__(self, x) -> mx.array:\n        x = self.gate_up_proj(x)\n        gate, up_states = mx.split(x, 2, axis=-1)\n        return self.down_proj(nn.silu(gate) * up_states)\n\n\nclass Glm4Attention(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.head_dim = getattr(\n            args, \"head_dim\", args.hidden_size // args.num_attention_heads\n        )\n        self.n_heads = args.num_attention_heads\n        self.n_kv_heads = args.num_key_value_heads\n        self.scale = self.head_dim**-0.5\n\n        self.q_proj = nn.Linear(\n            args.hidden_size,\n            args.num_attention_heads * self.head_dim,\n            bias=args.attention_bias,\n        )\n        self.k_proj = nn.Linear(\n            args.hidden_size,\n            args.num_key_value_heads * self.head_dim,\n            bias=args.attention_bias,\n        )\n        self.v_proj = nn.Linear(\n            args.hidden_size,\n            args.num_key_value_heads * self.head_dim,\n            bias=args.attention_bias,\n        )\n        self.o_proj = nn.Linear(\n            args.num_attention_heads * self.head_dim, args.hidden_size, bias=False\n        )\n\n        self.rope = nn.RoPE(\n            dims=int(self.head_dim * args.partial_rotary_factor),\n            base=args.rope_theta,\n            traditional=args.rope_traditional,\n        )\n\n    def __call__(\n        self, x: mx.array, mask: Optional[mx.array] = None, cache: Optional[Any] = None\n    ) -> mx.array:\n        B, L, D = x.shape\n\n        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)\n\n        queries = queries.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)\n        keys = keys.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)\n        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)\n\n        if cache is not None:\n            queries = self.rope(queries, offset=cache.offset)\n            keys = self.rope(keys, offset=cache.offset)\n            keys, values = cache.update_and_fetch(keys, values)\n        else:\n            queries = self.rope(queries)\n            keys = self.rope(keys)\n\n        output = scaled_dot_product_attention(\n            queries, keys, values, cache=cache, scale=self.scale, mask=mask\n        )\n\n        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)\n        return self.o_proj(output)\n\n\nclass Glm4DecoderLayer(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.self_attn = Glm4Attention(args=args)\n\n        self.mlp = Glm4MLP(args)\n        self.input_layernorm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)\n        self.post_attention_layernorm = nn.RMSNorm(\n            args.hidden_size, eps=args.rms_norm_eps\n        )\n        self.post_self_attn_layernorm = nn.RMSNorm(\n            args.hidden_size, eps=args.rms_norm_eps\n        )\n        self.post_mlp_layernorm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)\n\n    def __call__(\n        self, x: mx.array, mask: Optional[mx.array] = None, cache: Optional[Any] = None\n    ) -> mx.array:\n        x = x + self.post_self_attn_layernorm(\n            self.self_attn(self.input_layernorm(x), mask, cache)\n        )\n        residual = x\n        x = (\n            self.post_mlp_layernorm(self.mlp(self.post_attention_layernorm(x)))\n            + residual\n        )\n        return x\n\n\nclass Glm4Model(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.embed_tokens = nn.Embedding(args.vocab_size, args.hidden_size)\n        self.layers = [\n            Glm4DecoderLayer(args=args) for _ in range(args.num_hidden_layers)\n        ]\n        self.norm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)\n\n    def __call__(\n        self,\n        inputs: mx.array,\n        mask: Optional[mx.array] = None,\n        cache: Optional[Any] = None,\n    ):\n        h = self.embed_tokens(inputs)\n\n        if mask is None:\n            mask = create_attention_mask(h, cache)\n\n        if cache is None:\n            cache = [None] * len(self.layers)\n\n        for layer, c in zip(self.layers, cache):\n            h = layer(h, mask, cache=c)\n\n        return self.norm(h)\n\n\nclass Model(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.args = args\n        self.model_type = args.model_type\n        self.model = Glm4Model(args)\n        self.lm_head = nn.Linear(args.hidden_size, args.vocab_size, bias=False)\n\n    def __call__(\n        self,\n        inputs: mx.array,\n        mask: Optional[mx.array] = None,\n        cache: Optional[Any] = None,\n    ):\n        out = self.model(inputs, mask, cache)\n        return self.lm_head(out)\n\n    @property\n    def layers(self):\n        return self.model.layers"}, {"role": "assistant", "content": "//\n//  GLM4.swift\n//  LLM\n//\n//  Created by John Mai on 2025/5/1.\n//\n\nimport Foundation\nimport MLX\nimport MLXLMCommon\nimport MLXNN\n\n// port of https://github.com/ml-explore/mlx-lm/blob/main/mlx_lm/models/glm4.py\n\nprivate class Attention: Module {\n    let args: GLM4Configuration\n    let scale: Float\n\n    @ModuleInfo(key: \"q_proj\") var wq: Linear\n    @ModuleInfo(key: \"k_proj\") var wk: Linear\n    @ModuleInfo(key: \"v_proj\") var wv: Linear\n    @ModuleInfo(key: \"o_proj\") var wo: Linear\n\n    let rope: RoPE\n\n    public init(_ args: GLM4Configuration) {\n        self.args = args\n\n        let headDim = args.headDim > 0 ? args.headDim : args.hiddenSize / args.attentionHeads\n        self.scale = pow(Float(headDim), -0.5)\n\n        _wq.wrappedValue = Linear(\n            args.hiddenSize, args.attentionHeads * headDim, bias: args.attentionBias)\n        _wk.wrappedValue = Linear(args.hiddenSize, args.kvHeads * headDim, bias: args.attentionBias)\n        _wv.wrappedValue = Linear(args.hiddenSize, args.kvHeads * headDim, bias: args.attentionBias)\n        _wo.wrappedValue = Linear(args.attentionHeads * headDim, args.hiddenSize, bias: false)\n\n        self.rope = RoPE(\n            dimensions: Int(Float(headDim) * args.partialRotaryFactor),\n            traditional: args.ropeTraditional, base: args.ropeTheta)\n    }\n\n    public func callAsFunction(\n        _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?\n    ) -> MLXArray {\n        let (B, L) = (x.dim(0), x.dim(1))\n\n        var queries = wq(x)\n        var keys = wk(x)\n        var values = wv(x)\n\n        queries = queries.reshaped(B, L, args.attentionHeads, -1).transposed(0, 2, 1, 3)\n        keys = keys.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)\n        values = values.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)\n\n        if let cache {\n            queries = rope(queries, offset: cache.offset)\n            keys = rope(keys, offset: cache.offset)\n        } else {\n            queries = rope(queries)\n            keys = rope(keys)\n        }\n\n        let output = attentionWithCacheUpdate(\n            queries: queries,\n            keys: keys,\n            values: values,\n            cache: cache,\n            scale: scale,\n            mask: mask\n        )\n        .transposed(0, 2, 1, 3)\n        .reshaped(B, L, -1)\n\n        return wo(output)\n    }\n}\n\nprivate class MLP: Module, UnaryLayer {\n    @ModuleInfo(key: \"gate_up_proj\") var gateUp: Linear\n    @ModuleInfo(key: \"down_proj\") var down: Linear\n\n    public init(_ args: GLM4Configuration) {\n        _gateUp.wrappedValue = Linear(args.hiddenSize, 2 * args.intermediateSize, bias: false)\n        _down.wrappedValue = Linear(args.intermediateSize, args.hiddenSize, bias: false)\n    }\n\n    public func callAsFunction(_ x: MLXArray) -> MLXArray {\n        let x = gateUp(x)\n        let chunks = split(x, parts: 2, axis: -1)\n        return down(silu(chunks[0]) * chunks[1])\n    }\n}\n\nprivate class GLM4DecoderLayer: Module {\n    @ModuleInfo(key: \"self_attn\") var attention: Attention\n    let mlp: MLP\n\n    @ModuleInfo(key: \"input_layernorm\") var inputLayerNorm: RMSNorm\n    @ModuleInfo(key: \"post_attention_layernorm\") var postAttentionLayerNorm: RMSNorm\n    @ModuleInfo(key: \"post_self_attn_layernorm\") var postSelfAttnLayerNorm: RMSNorm\n    @ModuleInfo(key: \"post_mlp_layernorm\") var postMlpLayerNorm: RMSNorm\n\n    public init(_ args: GLM4Configuration) {\n        _attention.wrappedValue = Attention(args)\n        self.mlp = MLP(args)\n        _inputLayerNorm.wrappedValue = RMSNorm(\n            dimensions: args.hiddenSize, eps: args.rmsNormEps)\n        _postAttentionLayerNorm.wrappedValue = RMSNorm(\n            dimensions: args.hiddenSize, eps: args.rmsNormEps)\n        _postSelfAttnLayerNorm.wrappedValue = RMSNorm(\n            dimensions: args.hiddenSize, eps: args.rmsNormEps)\n        _postMlpLayerNorm.wrappedValue = RMSNorm(\n            dimensions: args.hiddenSize, eps: args.rmsNormEps)\n    }\n\n    public func callAsFunction(\n        _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?\n    ) -> MLXArray {\n        var x =\n            x\n            + postSelfAttnLayerNorm(\n                attention(inputLayerNorm(x), mask: mask, cache: cache)\n            )\n        let residual = x\n        x = postMlpLayerNorm(mlp(postAttentionLayerNorm(x))) + residual\n        return x\n    }\n}\n\nprivate class GLM4ModelInner: Module {\n    @ModuleInfo(key: \"embed_tokens\") var embedTokens: Embedding\n\n    fileprivate let layers: [GLM4DecoderLayer]\n    let norm: RMSNorm\n\n    public init(_ args: GLM4Configuration) {\n        precondition(args.vocabularySize > 0)\n\n        _embedTokens.wrappedValue = Embedding(\n            embeddingCount: args.vocabularySize, dimensions: args.hiddenSize)\n\n        self.layers = (0 ..< args.hiddenLayers)\n            .map { _ in\n                GLM4DecoderLayer(args)\n            }\n        self.norm = RMSNorm(dimensions: args.hiddenSize, eps: args.rmsNormEps)\n    }\n\n    public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]? = nil) -> MLXArray {\n        var h = embedTokens(inputs)\n\n        let mask = createAttentionMask(h: h, cache: cache)\n\n        for (i, layer) in layers.enumerated() {\n            h = layer(h, mask: mask, cache: cache?[i])\n        }\n\n        return norm(h)\n    }\n}\n\npublic class GLM4Model: Module, LLMModel, KVCacheDimensionProvider {\n    public let vocabularySize: Int\n    public let kvHeads: [Int]\n\n    private let model: GLM4ModelInner\n    let configuration: GLM4Configuration\n    let modelType: String\n\n    @ModuleInfo(key: \"lm_head\") var lmHead: Linear\n\n    public init(_ args: GLM4Configuration) {\n        self.configuration = args\n        self.vocabularySize = args.vocabularySize\n        self.kvHeads = (0 ..< args.hiddenLayers).map { _ in args.kvHeads }\n        self.modelType = args.modelType\n        self.model = GLM4ModelInner(args)\n\n        _lmHead.wrappedValue = Linear(args.hiddenSize, args.vocabularySize, bias: false)\n    }\n\n    public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]?) -> MLXArray {\n        let out = model(inputs, cache: cache)\n        return lmHead(out)\n    }\n\n    public func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {\n        var weights = weights\n\n        if configuration.tieWordEmbeddings {\n            weights[\"lm_head.weight\"] = nil\n        }\n\n        return weights\n    }\n}\n\npublic struct GLM4Configuration: Codable, Sendable {\n    var hiddenSize: Int\n    var hiddenLayers: Int\n    var intermediateSize: Int\n    var attentionHeads: Int\n    var attentionBias: Bool\n    var headDim: Int\n    var rmsNormEps: Float\n    var vocabularySize: Int\n    var kvHeads: Int\n    var partialRotaryFactor: Float\n    var ropeTheta: Float = 10000.0\n    var ropeTraditional: Bool = true\n    var tieWordEmbeddings = false\n    var maxPositionEmbeddings: Int = 32768\n    var modelType: String\n\n    enum CodingKeys: String, CodingKey {\n        case hiddenSize = \"hidden_size\"\n        case hiddenLayers = \"num_hidden_layers\"\n        case intermediateSize = \"intermediate_size\"\n        case attentionHeads = \"num_attention_heads\"\n        case attentionBias = \"attention_bias\"\n        case headDim = \"head_dim\"\n        case rmsNormEps = \"rms_norm_eps\"\n        case vocabularySize = \"vocab_size\"\n        case kvHeads = \"num_key_value_heads\"\n        case partialRotaryFactor = \"partial_rotary_factor\"\n        case ropeTheta = \"rope_theta\"\n        case ropeTraditional = \"rope_traditional\"\n        case tieWordEmbeddings = \"tie_word_embeddings\"\n        case maxPositionEmbeddings = \"max_position_embeddings\"\n        case modelType = \"model_type\"\n    }\n\n    public init(from decoder: Decoder) throws {\n        let container: KeyedDecodingContainer<GLM4Configuration.CodingKeys> =\n            try decoder.container(\n                keyedBy: GLM4Configuration.CodingKeys.self)\n\n        self.modelType = try container.decode(\n            String.self, forKey: GLM4Configuration.CodingKeys.modelType)\n        self.hiddenSize = try container.decode(\n            Int.self, forKey: GLM4Configuration.CodingKeys.hiddenSize)\n        self.hiddenLayers = try container.decode(\n            Int.self, forKey: GLM4Configuration.CodingKeys.hiddenLayers)\n        self.intermediateSize = try container.decode(\n            Int.self, forKey: GLM4Configuration.CodingKeys.intermediateSize)\n        self.attentionHeads = try container.decode(\n            Int.self, forKey: GLM4Configuration.CodingKeys.attentionHeads)\n        self.attentionBias = try container.decode(\n            Bool.self, forKey: GLM4Configuration.CodingKeys.attentionBias)\n        self.headDim = try container.decode(\n            Int.self, forKey: GLM4Configuration.CodingKeys.headDim)\n        self.rmsNormEps = try container.decode(\n            Float.self, forKey: GLM4Configuration.CodingKeys.rmsNormEps)\n        self.vocabularySize = try container.decode(\n            Int.self, forKey: GLM4Configuration.CodingKeys.vocabularySize)\n        self.kvHeads = try container.decode(Int.self, forKey: GLM4Configuration.CodingKeys.kvHeads)\n        self.partialRotaryFactor = try container.decode(\n            Float.self, forKey: GLM4Configuration.CodingKeys.partialRotaryFactor)\n        self.ropeTheta =\n            try container.decodeIfPresent(\n                Float.self, forKey: GLM4Configuration.CodingKeys.ropeTheta)\n            ?? 10000.0\n        self.ropeTraditional =\n            try container.decodeIfPresent(\n                Bool.self, forKey: GLM4Configuration.CodingKeys.ropeTraditional)\n            ?? true\n        self.tieWordEmbeddings =\n            try container.decodeIfPresent(Bool.self, forKey: .tieWordEmbeddings) ?? false\n        self.maxPositionEmbeddings =\n            try container.decodeIfPresent(Int.self, forKey: .maxPositionEmbeddings) ?? 32768\n    }\n}\n\n// MARK: - LoRA\n\nextension GLM4Model: LoRAModel {\n    public func loraLinearLayers() -> LoRALinearLayers {\n        model.layers.map { ($0.attention, [\"q_proj\", \"v_proj\"]) }\n    }\n}"}]}
{"messages": [{"role": "user", "content": "convert the following MLX Python code into MLX Swift: \n\n# Copyright \u00a9 2023-2024 Apple Inc.\n\nfrom dataclasses import dataclass\nfrom typing import Any, Optional\n\nimport mlx.core as mx\nimport mlx.nn as nn\n\nfrom .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention\n\n\n@dataclass\nclass ModelArgs(BaseModelArgs):\n    model_type: str\n    hidden_size: int\n    num_hidden_layers: int\n    intermediate_size: int\n    num_attention_heads: int\n    head_dim: int\n    rms_norm_eps: float\n    vocab_size: int\n    num_key_value_heads: int\n    rope_theta: float = 10000\n    rope_traditional: bool = False\n\n\nclass RMSNorm(nn.Module):\n    def __init__(self, dims: int, eps: float = 1e-5):\n        super().__init__()\n        self.weight = mx.ones((dims,))\n        self.eps = eps\n\n    def __call__(self, x):\n        return mx.fast.rms_norm(x, 1.0 + self.weight, self.eps)\n\n\nclass Attention(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n\n        dim = args.hidden_size\n        self.n_heads = n_heads = args.num_attention_heads\n        self.n_kv_heads = n_kv_heads = args.num_key_value_heads\n        self.head_dim = head_dim = args.head_dim\n\n        self.scale = head_dim**-0.5\n\n        self.q_proj = nn.Linear(dim, n_heads * head_dim, bias=False)\n        self.k_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=False)\n        self.v_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=False)\n        self.o_proj = nn.Linear(n_heads * head_dim, dim, bias=False)\n\n        self.rope = nn.RoPE(\n            head_dim,\n            traditional=args.rope_traditional,\n            base=args.rope_theta,\n        )\n\n    def __call__(\n        self,\n        x: mx.array,\n        mask: Optional[mx.array] = None,\n        cache: Optional[Any] = None,\n    ) -> mx.array:\n        B, L, D = x.shape\n\n        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)\n\n        # Prepare the queries, keys and values for the attention computation\n        queries = queries.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)\n        keys = keys.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)\n        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)\n\n        if cache is not None:\n            queries = self.rope(queries, offset=cache.offset)\n            keys = self.rope(keys, offset=cache.offset)\n            keys, values = cache.update_and_fetch(keys, values)\n        else:\n            queries = self.rope(queries)\n            keys = self.rope(keys)\n\n        output = scaled_dot_product_attention(\n            queries, keys, values, cache=cache, scale=self.scale, mask=mask\n        )\n\n        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)\n        return self.o_proj(output)\n\n\nclass MLP(nn.Module):\n    def __init__(self, dim, hidden_dim):\n        super().__init__()\n        self.gate_proj = nn.Linear(dim, hidden_dim, bias=False)\n        self.down_proj = nn.Linear(hidden_dim, dim, bias=False)\n        self.up_proj = nn.Linear(dim, hidden_dim, bias=False)\n\n    def __call__(self, x) -> mx.array:\n        return self.down_proj(nn.gelu(self.gate_proj(x)) * self.up_proj(x))\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.num_attention_heads = args.num_attention_heads\n        self.hidden_size = args.hidden_size\n        self.self_attn = Attention(args)\n        self.mlp = MLP(args.hidden_size, args.intermediate_size)\n        self.input_layernorm = RMSNorm(args.hidden_size, eps=args.rms_norm_eps)\n        self.post_attention_layernorm = RMSNorm(args.hidden_size, eps=args.rms_norm_eps)\n        self.args = args\n\n    def __call__(\n        self,\n        x: mx.array,\n        mask: Optional[mx.array] = None,\n        cache: Optional[Any] = None,\n    ) -> mx.array:\n        r = self.self_attn(self.input_layernorm(x), mask, cache)\n        h = x + r\n        r = self.mlp(self.post_attention_layernorm(h))\n        out = h + r\n        return out\n\n\nclass GemmaModel(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.args = args\n        self.vocab_size = args.vocab_size\n        self.num_hidden_layers = args.num_hidden_layers\n        assert self.vocab_size > 0\n        self.embed_tokens = nn.Embedding(args.vocab_size, args.hidden_size)\n        self.layers = [\n            TransformerBlock(args=args) for _ in range(args.num_hidden_layers)\n        ]\n        self.norm = RMSNorm(args.hidden_size, eps=args.rms_norm_eps)\n\n    def __call__(\n        self,\n        inputs: mx.array,\n        mask: mx.array = None,\n        cache=None,\n    ):\n        h = self.embed_tokens(inputs)\n        h = h * (self.args.hidden_size**0.5)\n\n        if mask is None:\n            mask = create_attention_mask(h, cache)\n\n        if cache is None:\n            cache = [None] * len(self.layers)\n\n        for layer, c in zip(self.layers, cache):\n            h = layer(h, mask, c)\n\n        return self.norm(h)\n\n\nclass Model(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.model_type = args.model_type\n        self.model = GemmaModel(args)\n        self.args = args\n\n    def __call__(\n        self,\n        inputs: mx.array,\n        mask: mx.array = None,\n        cache=None,\n    ):\n        out = self.model(inputs, mask, cache)\n        out = self.model.embed_tokens.as_linear(out)\n        return out\n\n    @property\n    def layers(self):\n        return self.model.layers"}, {"role": "assistant", "content": "// Copyright \u00a9 2024 Apple Inc.\n\nimport Foundation\nimport MLX\nimport MLXLMCommon\nimport MLXNN\nimport Tokenizers\n\n// Port of https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/models/gemma.py\n\n// Specialized norm for Gemma\nprivate class RMSNorm: Module, UnaryLayer {\n    let weight: MLXArray\n    let eps: Float\n\n    public init(dimensions: Int, eps: Float = 1e-5) {\n        self.weight = MLXArray.ones([dimensions])\n        self.eps = eps\n    }\n\n    public func callAsFunction(_ x: MLXArray) -> MLXArray {\n        return MLXFast.rmsNorm(x, weight: 1.0 + self.weight, eps: self.eps)\n    }\n}\n\nprivate class Attention: Module {\n    let args: GemmaConfiguration\n    let nHeads: Int\n    let nKVHeads: Int\n    let headDim: Int\n    let scale: Float\n\n    @ModuleInfo(key: \"q_proj\") var wq: Linear\n    @ModuleInfo(key: \"k_proj\") var wk: Linear\n    @ModuleInfo(key: \"v_proj\") var wv: Linear\n    @ModuleInfo(key: \"o_proj\") var wo: Linear\n\n    let rope: RoPE\n\n    public init(_ args: GemmaConfiguration) {\n        self.args = args\n\n        let dim = args.hiddenSize\n        self.nHeads = args.attentionHeads\n        self.nKVHeads = args.kvHeads\n        self.headDim = args.headDimensions\n        self.scale = pow(Float(headDim), -0.5)\n\n        self._wq.wrappedValue = Linear(dim, nHeads * headDim, bias: false)\n        self._wk.wrappedValue = Linear(dim, nKVHeads * headDim, bias: false)\n        self._wv.wrappedValue = Linear(dim, nKVHeads * headDim, bias: false)\n        self._wo.wrappedValue = Linear(nHeads * headDim, dim, bias: false)\n\n        self.rope = RoPE(\n            dimensions: headDim, traditional: args.ropeTraditional, base: args.ropeTheta)\n    }\n\n    public func callAsFunction(\n        _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?\n    ) -> MLXArray {\n        let (B, L) = (x.dim(0), x.dim(1))\n\n        var queries = wq(x)\n        var keys = wk(x)\n        var values = wv(x)\n\n        // Prepare the queries, keys and values for the attention computation\n        queries = queries.reshaped(B, L, nHeads, -1).transposed(0, 2, 1, 3)\n        keys = keys.reshaped(B, L, nKVHeads, -1).transposed(0, 2, 1, 3)\n        values = values.reshaped(B, L, nKVHeads, -1).transposed(0, 2, 1, 3)\n\n        if let cache {\n            queries = rope(queries, offset: cache.offset)\n            keys = rope(keys, offset: cache.offset)\n        } else {\n            queries = rope(queries)\n            keys = rope(keys)\n        }\n\n        let output = attentionWithCacheUpdate(\n            queries: queries,\n            keys: keys,\n            values: values,\n            cache: cache,\n            scale: scale,\n            mask: mask\n        )\n        .transposed(0, 2, 1, 3)\n        .reshaped(B, L, -1)\n\n        return wo(output)\n    }\n}\n\nprivate class MLP: Module, UnaryLayer {\n    @ModuleInfo(key: \"gate_proj\") var gate: Linear\n    @ModuleInfo(key: \"down_proj\") var down: Linear\n    @ModuleInfo(key: \"up_proj\") var up: Linear\n\n    public init(dimensions: Int, hiddenDimensions: Int) {\n        self._gate.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)\n        self._down.wrappedValue = Linear(hiddenDimensions, dimensions, bias: false)\n        self._up.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)\n    }\n\n    public func callAsFunction(_ x: MLXArray) -> MLXArray {\n        down(gelu(gate(x)) * up(x))\n    }\n}\n\nprivate class TransformerBlock: Module {\n    @ModuleInfo(key: \"self_attn\") var attention: Attention\n    let mlp: MLP\n\n    @ModuleInfo(key: \"input_layernorm\") var inputLayerNorm: Gemma.RMSNorm\n    @ModuleInfo(key: \"post_attention_layernorm\") var postAttentionLayerNorm: Gemma.RMSNorm\n\n    public init(_ args: GemmaConfiguration) {\n        self._attention.wrappedValue = Attention(args)\n        self.mlp = MLP(dimensions: args.hiddenSize, hiddenDimensions: args.intermediateSize)\n        self._inputLayerNorm.wrappedValue = Gemma.RMSNorm(\n            dimensions: args.hiddenSize, eps: args.rmsNormEps)\n        self._postAttentionLayerNorm.wrappedValue = Gemma.RMSNorm(\n            dimensions: args.hiddenSize, eps: args.rmsNormEps)\n    }\n\n    public func callAsFunction(\n        _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?\n    ) -> MLXArray {\n        var r = attention(inputLayerNorm(x), mask: mask, cache: cache)\n        let h = x + r\n        r = mlp(postAttentionLayerNorm(h))\n        return h + r\n    }\n}\n\nprivate class GemmaModelInner: Module {\n    let args: GemmaConfiguration\n    let vocabularySize: Int\n    let numHiddenLayers: Int\n\n    @ModuleInfo(key: \"embed_tokens\") var embedTokens: Embedding\n    fileprivate let layers: [TransformerBlock]\n    fileprivate let norm: Gemma.RMSNorm\n\n    public init(_ args: GemmaConfiguration) {\n        precondition(args.vocabularySize > 0)\n\n        self.args = args\n        self.vocabularySize = args.vocabularySize\n        self.numHiddenLayers = args.hiddenLayers\n\n        self._embedTokens.wrappedValue = Embedding(\n            embeddingCount: args.vocabularySize, dimensions: args.hiddenSize)\n\n        self.layers = (0 ..< args.hiddenLayers)\n            .map { _ in\n                TransformerBlock(args)\n            }\n        self.norm = Gemma.RMSNorm(dimensions: args.hiddenSize, eps: args.rmsNormEps)\n    }\n\n    public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]? = nil) -> MLXArray {\n        var h = embedTokens(inputs)\n        h = h * pow(Float(args.hiddenSize), 0.5)\n\n        let mask = createAttentionMask(h: h, cache: cache)\n\n        for (i, layer) in layers.enumerated() {\n            h = layer(h, mask: mask, cache: cache?[i])\n        }\n\n        return norm(h)\n    }\n}\n\npublic class GemmaModel: Module, LLMModel, KVCacheDimensionProvider {\n    public let vocabularySize: Int\n    public let kvHeads: [Int]\n\n    let modelType: String\n    private let model: GemmaModelInner\n\n    public init(_ args: GemmaConfiguration) {\n        self.modelType = args.modelType\n        self.vocabularySize = args.vocabularySize\n        self.kvHeads = Array(repeating: args.kvHeads, count: args.hiddenLayers)\n        self.model = GemmaModelInner(args)\n    }\n\n    public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]?) -> MLXArray {\n        let out = model(inputs, cache: cache)\n        return model.embedTokens.asLinear(out)\n    }\n\n    public func messageGenerator(tokenizer: any Tokenizer) -> any MessageGenerator {\n        NoSystemMessageGenerator()\n    }\n}\n\npublic struct GemmaConfiguration: Codable, Sendable {\n    var modelType: String\n    var hiddenSize: Int\n    var hiddenLayers: Int\n    var intermediateSize: Int\n    var attentionHeads: Int\n    var headDimensions: Int\n    var rmsNormEps: Float\n    var vocabularySize: Int\n    var kvHeads: Int\n    private let _ropeTheta: Float?\n    public var ropeTheta: Float { _ropeTheta ?? 10_000 }\n    private let _ropeTraditional: Bool?\n    public var ropeTraditional: Bool { _ropeTraditional ?? false }\n\n    enum CodingKeys: String, CodingKey {\n        case modelType = \"model_type\"\n        case hiddenSize = \"hidden_size\"\n        case hiddenLayers = \"num_hidden_layers\"\n        case intermediateSize = \"intermediate_size\"\n        case attentionHeads = \"num_attention_heads\"\n        case headDimensions = \"head_dim\"\n        case rmsNormEps = \"rms_norm_eps\"\n        case vocabularySize = \"vocab_size\"\n        case kvHeads = \"num_key_value_heads\"\n        case _ropeTheta = \"rope_theta\"\n        case _ropeTraditional = \"rope_traditional\"\n    }\n}\n\n// MARK: - LoRA\n\nextension GemmaModel: LoRAModel {\n    public func loraLinearLayers() -> LoRALinearLayers {\n        model.layers.map { ($0.attention, [\"q_proj\", \"v_proj\"]) }\n    }\n}"}]}
{"messages": [{"role": "user", "content": "convert the following MLX Python code into MLX Swift: \n\n# Copyright \u00a9 2023-2024 Apple Inc.\n\nfrom dataclasses import dataclass\nfrom typing import Any, Optional\n\nimport mlx.core as mx\nimport mlx.nn as nn\n\nfrom .base import BaseModelArgs, create_attention_mask\n\n\n@dataclass\nclass ModelArgs(BaseModelArgs):\n    model_type: str\n    hidden_size: int\n    num_hidden_layers: int\n    intermediate_size: int\n    num_attention_heads: int\n    head_dim: int\n    rms_norm_eps: float\n    vocab_size: int\n    num_key_value_heads: int\n    rope_theta: float = 10000\n    rope_traditional: bool = False\n    attn_logit_softcapping: float = 50.0\n    final_logit_softcapping: float = 30.0\n    query_pre_attn_scalar: float = 144.0\n\n\nclass RMSNorm(nn.Module):\n    def __init__(self, dims: int, eps: float = 1e-5):\n        super().__init__()\n        self.weight = mx.ones((dims,))\n        self.eps = eps\n\n    def __call__(self, x):\n        return mx.fast.rms_norm(x, 1.0 + self.weight, self.eps)\n\n\nclass Attention(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n\n        dim = args.hidden_size\n        self.n_heads = n_heads = args.num_attention_heads\n        self.n_kv_heads = n_kv_heads = args.num_key_value_heads\n        self.repeats = n_heads // n_kv_heads\n        self.head_dim = head_dim = args.head_dim\n\n        self.scale = 1.0 / (args.query_pre_attn_scalar**0.5)\n\n        self.q_proj = nn.Linear(dim, n_heads * head_dim, bias=False)\n        self.k_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=False)\n        self.v_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=False)\n        self.o_proj = nn.Linear(n_heads * head_dim, dim, bias=False)\n        self.attn_logit_softcapping = args.attn_logit_softcapping\n        self.rope = nn.RoPE(\n            head_dim,\n            traditional=args.rope_traditional,\n            base=args.rope_theta,\n        )\n\n    def __call__(\n        self,\n        x: mx.array,\n        mask: Optional[mx.array] = None,\n        cache: Optional[Any] = None,\n    ) -> mx.array:\n        B, L, D = x.shape\n        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)\n        queries = queries.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)\n        keys = keys.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)\n        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)\n\n        if cache is not None:\n            queries = self.rope(queries, offset=cache.offset)\n            keys = self.rope(keys, offset=cache.offset)\n            keys, values = cache.update_and_fetch(keys, values)\n        else:\n            queries = self.rope(queries)\n            keys = self.rope(keys)\n\n        queries = queries * self.scale\n\n        if self.repeats > 1:\n            queries = queries.reshape(\n                B, self.n_kv_heads, self.repeats, L, self.head_dim\n            )\n            keys = mx.expand_dims(keys, 2)\n            values = mx.expand_dims(values, 2)\n\n        scores = queries @ keys.swapaxes(-1, -2)\n        scores = mx.tanh(scores / self.attn_logit_softcapping)\n        scores *= self.attn_logit_softcapping\n\n        if mask is not None:\n            if mask.dtype == mx.bool_:\n                scores = mx.where(\n                    mask, scores, mx.array(mx.finfo(scores.dtype).min, scores.dtype)\n                )\n            else:\n                scores = scores + mask\n        scores = mx.softmax(scores, precise=True, axis=-1)\n        output = scores @ values\n        if self.repeats > 1:\n            output = output.reshape(B, self.n_heads, L, self.head_dim)\n        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)\n        return self.o_proj(output)\n\n\nclass MLP(nn.Module):\n    def __init__(self, dim, hidden_dim):\n        super().__init__()\n        self.gate_proj = nn.Linear(dim, hidden_dim, bias=False)\n        self.down_proj = nn.Linear(hidden_dim, dim, bias=False)\n        self.up_proj = nn.Linear(dim, hidden_dim, bias=False)\n\n    def __call__(self, x) -> mx.array:\n        return self.down_proj(nn.gelu_approx(self.gate_proj(x)) * self.up_proj(x))\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.num_attention_heads = args.num_attention_heads\n        self.hidden_size = args.hidden_size\n        self.self_attn = Attention(args)\n        self.mlp = MLP(args.hidden_size, args.intermediate_size)\n        self.input_layernorm = RMSNorm(args.hidden_size, eps=args.rms_norm_eps)\n        self.pre_feedforward_layernorm = RMSNorm(\n            args.hidden_size, eps=args.rms_norm_eps\n        )\n        self.post_feedforward_layernorm = RMSNorm(\n            args.hidden_size, eps=args.rms_norm_eps\n        )\n        self.post_attention_layernorm = RMSNorm(args.hidden_size, eps=args.rms_norm_eps)\n        self.args = args\n\n    def __call__(\n        self,\n        x: mx.array,\n        mask: Optional[mx.array] = None,\n        cache: Optional[Any] = None,\n    ) -> mx.array:\n        r = self.self_attn(self.input_layernorm(x), mask, cache)\n        h = x + self.post_attention_layernorm(r)\n        r = self.mlp(self.pre_feedforward_layernorm(h))\n        out = h + self.post_feedforward_layernorm(r)\n        return out\n\n\nclass GemmaModel(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.args = args\n        self.vocab_size = args.vocab_size\n        self.num_hidden_layers = args.num_hidden_layers\n        assert self.vocab_size > 0\n        self.embed_tokens = nn.Embedding(args.vocab_size, args.hidden_size)\n        self.layers = [\n            TransformerBlock(args=args) for _ in range(args.num_hidden_layers)\n        ]\n        self.norm = RMSNorm(args.hidden_size, eps=args.rms_norm_eps)\n\n    def __call__(\n        self,\n        inputs: mx.array,\n        mask: mx.array = None,\n        cache=None,\n    ):\n        h = self.embed_tokens(inputs)\n        h = h * (self.args.hidden_size**0.5)\n\n        if mask is None:\n            mask = create_attention_mask(h, cache, return_array=True)\n\n        if cache is None:\n            cache = [None] * len(self.layers)\n\n        for layer, c in zip(self.layers, cache):\n            h = layer(h, mask, c)\n\n        return self.norm(h)\n\n\nclass Model(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.model_type = args.model_type\n        self.final_logit_softcapping = args.final_logit_softcapping\n        self.model = GemmaModel(args)\n        self.args = args\n\n    def __call__(\n        self,\n        inputs: mx.array,\n        mask: mx.array = None,\n        cache=None,\n    ):\n        out = self.model(inputs, mask, cache)\n        out = self.model.embed_tokens.as_linear(out)\n        out = mx.tanh(out / self.final_logit_softcapping)\n        out = out * self.final_logit_softcapping\n        return out\n\n    @property\n    def layers(self):\n        return self.model.layers"}, {"role": "assistant", "content": "// Copyright \u00a9 2024 Apple Inc.\n\nimport Foundation\nimport MLX\nimport MLXLMCommon\nimport MLXNN\nimport Tokenizers\n\n// Port of https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/models/gemma2.py\n\nprivate class Attention: Module {\n    let args: Gemma2Configuration\n    let scale: Float\n    let logitSoftCap: Float\n    let headDim: Int\n    let nHeads: Int\n    let nKVHeads: Int\n    let repeats: Int\n\n    @ModuleInfo(key: \"q_proj\") var wq: Linear\n    @ModuleInfo(key: \"k_proj\") var wk: Linear\n    @ModuleInfo(key: \"v_proj\") var wv: Linear\n    @ModuleInfo(key: \"o_proj\") var wo: Linear\n\n    let rope: RoPE\n\n    public init(_ args: Gemma2Configuration) {\n        self.args = args\n\n        let dim = args.hiddenSize\n        self.nHeads = args.attentionHeads\n        self.nKVHeads = args.kvHeads\n        self.repeats = args.attentionHeads / args.kvHeads\n        self.headDim = args.headDimensions\n\n        self.scale = 1.0 / pow(Float(args.queryPreAttnScalar), 0.5)\n\n        self._wq.wrappedValue = Linear(dim, nHeads * headDim, bias: false)\n        self._wk.wrappedValue = Linear(dim, nKVHeads * headDim, bias: false)\n        self._wv.wrappedValue = Linear(dim, nKVHeads * headDim, bias: false)\n        self._wo.wrappedValue = Linear(nHeads * headDim, dim, bias: false)\n        self.logitSoftCap = args.attnLogitSoftcapping\n        self.rope = RoPE(\n            dimensions: headDim, traditional: args.ropeTraditional, base: args.ropeTheta)\n    }\n\n    public func callAsFunction(\n        _ x: MLXArray, mask: MLXArray?, cache: KVCache?\n    ) -> MLXArray {\n        let (B, L) = (x.dim(0), x.dim(1))\n        var queries = wq(x)\n        var keys = wk(x)\n        var values = wv(x)\n        queries = queries.reshaped(B, L, nHeads, -1).transposed(0, 2, 1, 3)\n        keys = keys.reshaped(B, L, nKVHeads, -1).transposed(0, 2, 1, 3)\n        values = values.reshaped(B, L, nKVHeads, -1).transposed(0, 2, 1, 3)\n\n        if let cache {\n            queries = rope(queries, offset: cache.offset)\n            keys = rope(keys, offset: cache.offset)\n            (keys, values) = cache.update(keys: keys, values: values)\n        } else {\n            queries = rope(queries)\n            keys = rope(keys)\n        }\n\n        queries = queries * self.scale\n\n        if repeats > 1 {\n            queries = queries.reshaped([B, nKVHeads, repeats, L, headDim])\n            keys = expandedDimensions(keys, axes: [2])\n            values = expandedDimensions(values, axes: [2])\n        }\n\n        var scores = matmul(queries, keys.swappedAxes(-1, -2))\n        scores = tanh(scores / logitSoftCap) * logitSoftCap\n\n        if let mask {\n            scores = scores + mask\n        }\n        scores = softmax(scores, axis: -1, precise: true)\n        var output = matmul(scores, values)\n        if repeats > 1 {\n            output = output.reshaped([B, nHeads, L, headDim])\n        }\n        output = output.transposed(0, 2, 1, 3).reshaped(B, L, -1)\n        return wo(output)\n    }\n}\n\nprivate class MLP: Module, UnaryLayer {\n    @ModuleInfo(key: \"gate_proj\") var gate: Linear\n    @ModuleInfo(key: \"down_proj\") var down: Linear\n    @ModuleInfo(key: \"up_proj\") var up: Linear\n\n    public init(dimensions: Int, hiddenDimensions: Int) {\n        self._gate.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)\n        self._down.wrappedValue = Linear(hiddenDimensions, dimensions, bias: false)\n        self._up.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)\n    }\n\n    public func callAsFunction(_ x: MLXArray) -> MLXArray {\n        down(gelu(gate(x)) * up(x))\n    }\n}\n\n// Minimal changes from Gemma TransformerBlock\nprivate class TransformerBlock: Module {\n    @ModuleInfo(key: \"self_attn\") var attention: Attention\n    let mlp: MLP\n\n    @ModuleInfo(key: \"input_layernorm\") var inputLayerNorm: Gemma.RMSNorm\n    @ModuleInfo(key: \"pre_feedforward_layernorm\") var preFeedforwardLayerNorm: Gemma.RMSNorm\n    @ModuleInfo(key: \"post_feedforward_layernorm\") var postFeedforwardLayerNorm: Gemma.RMSNorm\n    @ModuleInfo(key: \"post_attention_layernorm\") var postAttentionLayerNorm: Gemma.RMSNorm\n\n    public init(_ args: Gemma2Configuration) {\n        self._attention.wrappedValue = Attention(args)\n        self.mlp = MLP(dimensions: args.hiddenSize, hiddenDimensions: args.intermediateSize)\n        self._inputLayerNorm.wrappedValue = Gemma.RMSNorm(\n            dimensions: args.hiddenSize, eps: args.rmsNormEps)\n        self._preFeedforwardLayerNorm.wrappedValue = Gemma.RMSNorm(\n            dimensions: args.hiddenSize, eps: args.rmsNormEps)\n        self._postFeedforwardLayerNorm.wrappedValue = Gemma.RMSNorm(\n            dimensions: args.hiddenSize, eps: args.rmsNormEps)\n        self._postAttentionLayerNorm.wrappedValue = Gemma.RMSNorm(\n            dimensions: args.hiddenSize, eps: args.rmsNormEps)\n    }\n\n    public func callAsFunction(\n        _ x: MLXArray, mask: MLXArray?, cache: KVCache?\n    ) -> MLXArray {\n        var r = attention(inputLayerNorm(x), mask: mask, cache: cache)\n        let h = x + postAttentionLayerNorm(r)\n        r = mlp(preFeedforwardLayerNorm(h))\n        let out = h + postFeedforwardLayerNorm(r)\n        return out\n    }\n}\n\n// Uses Gemma2TransformerBlock, otherwise same as GemmaModelInner\nprivate class ModelInner: Module {\n    @ModuleInfo(key: \"embed_tokens\") var embedTokens: Embedding\n\n    fileprivate let layers: [TransformerBlock]\n    fileprivate let norm: Gemma.RMSNorm\n\n    let hiddenScale: Float\n\n    public init(_ args: Gemma2Configuration) {\n        precondition(args.vocabularySize > 0)\n\n        self._embedTokens.wrappedValue = Embedding(\n            embeddingCount: args.vocabularySize, dimensions: args.hiddenSize)\n\n        self.hiddenScale = pow(Float(args.hiddenSize), 0.5)\n\n        self.layers = (0 ..< args.hiddenLayers)\n            .map { _ in\n                TransformerBlock(args)\n            }\n        self.norm = Gemma.RMSNorm(dimensions: args.hiddenSize, eps: args.rmsNormEps)\n    }\n\n    public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]? = nil) -> MLXArray {\n        var h = embedTokens(inputs)\n        h = h * hiddenScale\n\n        let mask: MLXArray? = createAttentionMask(h: h, cache: cache)\n\n        for (i, layer) in layers.enumerated() {\n            h = layer(h, mask: mask, cache: cache?[i])\n        }\n\n        return norm(h)\n    }\n}\n\n// Uses Gemma2ModelInner, otherwise same as GemmaModel\npublic class Gemma2Model: Module, LLMModel, KVCacheDimensionProvider {\n    public let vocabularySize: Int\n    public let kvHeads: [Int]\n\n    private let model: ModelInner\n    let logitSoftCap: Float\n\n    public init(_ args: Gemma2Configuration) {\n        self.vocabularySize = args.vocabularySize\n        self.kvHeads = Array(repeating: args.kvHeads, count: args.hiddenLayers)\n        self.model = ModelInner(args)\n        self.logitSoftCap = args.finalLogitSoftcapping\n    }\n\n    public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]?) -> MLXArray {\n        var out = model(inputs, cache: cache)\n        out = model.embedTokens.asLinear(out)\n        out = tanh(out / logitSoftCap) * logitSoftCap\n        return out\n    }\n\n    public func messageGenerator(tokenizer: any Tokenizer) -> any MessageGenerator {\n        NoSystemMessageGenerator()\n    }\n}\n\npublic struct Gemma2Configuration: Codable {\n    var hiddenSize: Int\n    var hiddenLayers: Int\n    var intermediateSize: Int\n    var attentionHeads: Int\n    var headDimensions: Int\n    var rmsNormEps: Float\n    var vocabularySize: Int\n    var kvHeads: Int\n    var ropeTheta: Float = 10_000\n    var ropeTraditional: Bool = false\n    var attnLogitSoftcapping: Float = 50.0\n    var finalLogitSoftcapping: Float = 30.0\n    var queryPreAttnScalar: Float = 144.0\n\n    enum CodingKeys: String, CodingKey {\n        case hiddenSize = \"hidden_size\"\n        case hiddenLayers = \"num_hidden_layers\"\n        case intermediateSize = \"intermediate_size\"\n        case attentionHeads = \"num_attention_heads\"\n        case headDimensions = \"head_dim\"\n        case rmsNormEps = \"rms_norm_eps\"\n        case vocabularySize = \"vocab_size\"\n        case kvHeads = \"num_key_value_heads\"\n        case ropeTheta = \"rope_theta\"\n        case ropeTraditional = \"rope_traditional\"\n        case attnLogitSoftcapping = \"attn_logit_softcapping\"\n        case finalLogitSoftcapping = \"final_logit_softcapping\"\n        case queryPreAttnScalar = \"query_pre_attn_scalar\"\n    }\n\n    public init(from decoder: Swift.Decoder) throws {\n        // Custom implementation to handle optional keys with required values\n        let container: KeyedDecodingContainer<CodingKeys> = try decoder.container(\n            keyedBy: CodingKeys.self)\n\n        self.hiddenSize = try container.decode(\n            Int.self, forKey: CodingKeys.hiddenSize)\n        self.hiddenLayers = try container.decode(\n            Int.self, forKey: CodingKeys.hiddenLayers)\n        self.intermediateSize = try container.decode(\n            Int.self, forKey: CodingKeys.intermediateSize)\n        self.attentionHeads = try container.decode(\n            Int.self, forKey: CodingKeys.attentionHeads)\n        self.headDimensions = try container.decode(\n            Int.self, forKey: CodingKeys.headDimensions)\n        self.rmsNormEps = try container.decode(\n            Float.self, forKey: CodingKeys.rmsNormEps)\n        self.vocabularySize = try container.decode(\n            Int.self, forKey: CodingKeys.vocabularySize)\n        self.kvHeads = try container.decode(Int.self, forKey: CodingKeys.kvHeads)\n        self.ropeTheta =\n            try container.decodeIfPresent(Float.self, forKey: CodingKeys.ropeTheta)\n            ?? 10_000\n        self.ropeTraditional =\n            try container.decodeIfPresent(\n                Bool.self, forKey: CodingKeys.ropeTraditional) ?? false\n        self.attnLogitSoftcapping = try container.decode(\n            Float.self, forKey: CodingKeys.attnLogitSoftcapping)\n        self.finalLogitSoftcapping = try container.decode(\n            Float.self, forKey: CodingKeys.finalLogitSoftcapping)\n        self.queryPreAttnScalar = try container.decode(\n            Float.self, forKey: CodingKeys.queryPreAttnScalar)\n    }\n}\n\n// MARK: - LoRA\n\nextension Gemma2Model: LoRAModel {\n    public func loraLinearLayers() -> LoRALinearLayers {\n        model.layers.map { ($0.attention, [\"q_proj\", \"v_proj\"]) }\n    }\n}"}]}
{"messages": [{"role": "user", "content": "convert the following MLX Python code into MLX Swift: \n\n# Copyright \u00a9 2025 Apple Inc.\n\nfrom dataclasses import dataclass\nfrom functools import partial\nfrom typing import Any, Optional\n\nimport mlx.core as mx\nimport mlx.nn as nn\n\nfrom .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention\nfrom .cache import KVCache, RotatingKVCache\n\n\n@dataclass\nclass ModelArgs(BaseModelArgs):\n    model_type: str\n    hidden_size: int = 1152\n    num_hidden_layers: int = 26\n    intermediate_size: int = 6912\n    num_attention_heads: int = 4\n    head_dim: int = 256\n    rms_norm_eps: float = 1.0e-6\n    vocab_size: int = 262144\n    num_key_value_heads: int = 1\n    rope_global_base_freq: float = 1_000_000.0\n    rope_local_base_freq: float = 10_000.0\n    rope_traditional: bool = False\n    query_pre_attn_scalar: float = 256\n    sliding_window: int = 512\n    sliding_window_pattern: int = 6\n\n\nclass Attention(nn.Module):\n    def __init__(self, args: ModelArgs, layer_idx: int):\n        super().__init__()\n\n        dim = args.hidden_size\n        self.n_heads = n_heads = args.num_attention_heads\n        self.n_kv_heads = n_kv_heads = args.num_key_value_heads\n        self.repeats = n_heads // n_kv_heads\n        self.head_dim = head_dim = args.head_dim\n        self.layer_idx = layer_idx\n\n        self.scale = args.query_pre_attn_scalar**-0.5\n\n        self.q_proj = nn.Linear(dim, n_heads * head_dim, bias=False)\n        self.k_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=False)\n        self.v_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=False)\n        self.o_proj = nn.Linear(n_heads * head_dim, dim, bias=False)\n\n        self.q_norm = RMSNorm(dims=head_dim, eps=args.rms_norm_eps)\n        self.k_norm = RMSNorm(dims=head_dim, eps=args.rms_norm_eps)\n        self.is_sliding = (layer_idx + 1) % args.sliding_window_pattern != 0\n\n        self.rope = nn.RoPE(\n            head_dim,\n            traditional=args.rope_traditional,\n            base=(\n                args.rope_local_base_freq\n                if self.is_sliding\n                else args.rope_global_base_freq\n            ),\n        )\n\n    def __call__(\n        self,\n        x: mx.array,\n        mask: Optional[mx.array] = None,\n        cache: Optional[Any] = None,\n    ) -> mx.array:\n        B, L, _ = x.shape\n        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)\n        queries = queries.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)\n\n        keys = keys.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)\n        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)\n\n        queries = self.q_norm(queries)\n        keys = self.k_norm(keys)\n\n        if cache is not None:\n            queries = self.rope(queries, offset=cache.offset)\n            keys = self.rope(keys, offset=cache.offset)\n            keys, values = cache.update_and_fetch(keys, values)\n        else:\n            queries = self.rope(queries)\n            keys = self.rope(keys)\n\n        # Sliding window\n        if isinstance(mask, mx.array) and mask.shape[-1] != keys.shape[-2]:\n            mask = mask[..., -keys.shape[-2] :]\n        output = scaled_dot_product_attention(\n            queries, keys, values, cache=cache, scale=self.scale, mask=mask\n        )\n        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)\n        return self.o_proj(output)\n\n\nclass RMSNorm(nn.Module):\n    def __init__(self, dims: int, eps: float = 1e-5):\n        super().__init__()\n        self.weight = mx.ones((dims,))\n        self.eps = eps\n\n    def __call__(self, x):\n        return mx.fast.rms_norm(x, 1.0 + self.weight, self.eps)\n\n\nclass MLP(nn.Module):\n    def __init__(self, dim, hidden_dim):\n        super().__init__()\n        self.gate_proj = nn.Linear(dim, hidden_dim, bias=False)\n        self.down_proj = nn.Linear(hidden_dim, dim, bias=False)\n        self.up_proj = nn.Linear(dim, hidden_dim, bias=False)\n\n    def __call__(self, x) -> mx.array:\n        return self.down_proj(nn.gelu_approx(self.gate_proj(x)) * self.up_proj(x))\n\n\n@partial(mx.compile, shapeless=True)\ndef clip_residual(x, y):\n    if x.dtype != mx.float16:\n        return x + y\n    bound = mx.finfo(mx.float16).max\n    return mx.clip(x.astype(mx.float32) + y.astype(mx.float32), -bound, bound).astype(\n        mx.float16\n    )\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, args: ModelArgs, layer_idx: int):\n        super().__init__()\n        self.num_attention_heads = args.num_attention_heads\n        self.hidden_size = args.hidden_size\n        self.self_attn = Attention(args, layer_idx)\n        self.mlp = MLP(args.hidden_size, args.intermediate_size)\n        self.input_layernorm = RMSNorm(args.hidden_size, eps=args.rms_norm_eps)\n        self.post_attention_layernorm = RMSNorm(args.hidden_size, eps=args.rms_norm_eps)\n        self.pre_feedforward_layernorm = RMSNorm(\n            args.hidden_size, eps=args.rms_norm_eps\n        )\n        self.post_feedforward_layernorm = RMSNorm(\n            args.hidden_size, eps=args.rms_norm_eps\n        )\n\n    def __call__(\n        self,\n        x: mx.array,\n        mask: Optional[mx.array] = None,\n        cache: Optional[Any] = None,\n    ) -> mx.array:\n        r = self.self_attn(self.input_layernorm(x), mask, cache)\n        h = clip_residual(x, self.post_attention_layernorm(r))\n        r = self.mlp(self.pre_feedforward_layernorm(h))\n        out = clip_residual(h, self.post_feedforward_layernorm(r))\n        return out\n\n\nclass Gemma3Model(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.args = args\n        self.vocab_size = args.vocab_size\n        self.num_hidden_layers = args.num_hidden_layers\n        assert self.vocab_size > 0\n        self.embed_tokens = nn.Embedding(args.vocab_size, args.hidden_size)\n        self.layers = [\n            TransformerBlock(args=args, layer_idx=layer_idx)\n            for layer_idx in range(args.num_hidden_layers)\n        ]\n        self.norm = RMSNorm(args.hidden_size, eps=args.rms_norm_eps)\n\n    def __call__(\n        self,\n        inputs: mx.array,\n        mask: mx.array = None,\n        cache=None,\n        input_embeddings: Optional[mx.array] = None,\n    ):\n        if input_embeddings is not None:\n            h = input_embeddings\n        else:\n            h = self.embed_tokens(inputs)\n        h *= mx.array(self.args.hidden_size**0.5, mx.bfloat16).astype(h.dtype)\n\n        if cache is None:\n            cache = [None] * len(self.layers)\n\n        if mask is None:\n            j = self.args.sliding_window_pattern\n            full_mask = create_attention_mask(h, cache[j - 1 : j])\n            sliding_window_mask = create_attention_mask(h, cache)\n\n        for i, (layer, c) in enumerate(zip(self.layers, cache)):\n            is_global = (\n                i % self.args.sliding_window_pattern\n                == self.args.sliding_window_pattern - 1\n            )\n\n            local_mask = mask\n            if mask is None and is_global:\n                local_mask = full_mask\n            elif mask is None:\n                local_mask = sliding_window_mask\n\n            h = layer(h, local_mask, c)\n\n        return self.norm(h)\n\n\nclass Model(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.args = args\n        self.model_type = args.model_type\n        self.model = Gemma3Model(args)\n        self.lm_head = nn.Linear(args.hidden_size, args.vocab_size, bias=False)\n\n    def __call__(\n        self,\n        inputs: mx.array,\n        cache=None,\n        mask: Optional[mx.array] = None,\n        input_embeddings: Optional[mx.array] = None,\n    ):\n        out = self.model(inputs, mask, cache, input_embeddings)\n        out = self.lm_head(out)\n        return out\n\n    def sanitize(self, weights):\n        weights = dict(weights)\n        if \"lm_head.weight\" not in weights:\n            weights[\"lm_head.weight\"] = weights[\"model.embed_tokens.weight\"]\n        return weights\n\n    @property\n    def layers(self):\n        return self.model.layers\n\n    def make_cache(self):\n        caches = []\n        for i in range(self.args.num_hidden_layers):\n            if (\n                i % self.args.sliding_window_pattern\n                == self.args.sliding_window_pattern - 1\n            ):\n                caches.append(KVCache())\n            else:\n                caches.append(\n                    RotatingKVCache(max_size=self.args.sliding_window, keep=0)\n                )\n        return caches"}, {"role": "assistant", "content": "//\n//  Gemma3Text.swift\n//  mlx-swift-examples\n//\n//  Created by Anthony DePasquale on 14.03.2025.\n//\n\n// Based on https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/models/gemma3_text.py\n\nimport Foundation\nimport MLX\nimport MLXFast\nimport MLXLLM\nimport MLXLMCommon\nimport MLXNN\n\npublic struct Gemma3TextConfiguration: Codable {\n    let modelType: String\n    let hiddenSize: Int\n    let hiddenLayers: Int\n    let intermediateSize: Int\n    let attentionHeads: Int\n    let headDim: Int\n    let rmsNormEps: Float\n    let vocabularySize: Int\n    let kvHeads: Int\n    let ropeGlobalBaseFreq: Float\n    let ropeLocalBaseFreq: Float\n    let ropeTraditional: Bool\n    let queryPreAttnScalar: Float\n    let slidingWindow: Int\n    let slidingWindowPattern: Int\n\n    enum CodingKeys: String, CodingKey {\n        case modelType = \"model_type\"\n        case hiddenSize = \"hidden_size\"\n        case hiddenLayers = \"num_hidden_layers\"\n        case intermediateSize = \"intermediate_size\"\n        case attentionHeads = \"num_attention_heads\"\n        case headDim = \"head_dim\"\n        case rmsNormEps = \"rms_norm_eps\"\n        case vocabularySize = \"vocab_size\"\n        case kvHeads = \"num_key_value_heads\"\n        case ropeGlobalBaseFreq = \"rope_global_base_freq\"\n        case ropeLocalBaseFreq = \"rope_local_base_freq\"\n        case ropeTraditional = \"rope_traditional\"\n        case queryPreAttnScalar = \"query_pre_attn_scalar\"\n        case slidingWindow = \"sliding_window\"\n        case slidingWindowPattern = \"sliding_window_pattern\"\n    }\n\n    enum VLMCodingKeys: String, CodingKey {\n        case textConfig = \"text_config\"\n    }\n\n    public init(from decoder: Decoder) throws {\n        let nestedContainer = try decoder.container(keyedBy: VLMCodingKeys.self)\n\n        // in the case of VLM models convertered using mlx_lm.convert\n        // the configuration will still match the VLMs and be under text_config\n        let container =\n            if nestedContainer.contains(.textConfig) {\n                try nestedContainer.nestedContainer(keyedBy: CodingKeys.self, forKey: .textConfig)\n            } else {\n                try decoder.container(keyedBy: CodingKeys.self)\n            }\n\n        modelType = try container.decode(String.self, forKey: .modelType)\n        hiddenSize = try container.decode(Int.self, forKey: .hiddenSize)\n        hiddenLayers = try container.decode(Int.self, forKey: .hiddenLayers)\n        intermediateSize = try container.decode(Int.self, forKey: .intermediateSize)\n        attentionHeads = try container.decodeIfPresent(Int.self, forKey: .attentionHeads) ?? 4\n        headDim = try container.decodeIfPresent(Int.self, forKey: .headDim) ?? 256\n        rmsNormEps = try container.decodeIfPresent(Float.self, forKey: .rmsNormEps) ?? 1.0e-6\n        vocabularySize = try container.decodeIfPresent(Int.self, forKey: .vocabularySize) ?? 262144\n        kvHeads = try container.decodeIfPresent(Int.self, forKey: .kvHeads) ?? 1\n        ropeGlobalBaseFreq =\n            try container.decodeIfPresent(Float.self, forKey: .ropeGlobalBaseFreq) ?? 1_000_000.0\n        ropeLocalBaseFreq =\n            try container.decodeIfPresent(Float.self, forKey: .ropeLocalBaseFreq) ?? 10_000.0\n        ropeTraditional =\n            try container.decodeIfPresent(Bool.self, forKey: .ropeTraditional) ?? false\n        queryPreAttnScalar =\n            try container.decodeIfPresent(Float.self, forKey: .queryPreAttnScalar) ?? 256\n        slidingWindow = try container.decodeIfPresent(Int.self, forKey: .slidingWindow) ?? 512\n        slidingWindowPattern =\n            try container.decodeIfPresent(Int.self, forKey: .slidingWindowPattern) ?? 6\n    }\n}\n\nprivate class Attention: Module {\n    let nHeads: Int\n    let nKVHeads: Int\n    let repeats: Int\n    let headDim: Int\n    let layerIdx: Int\n    let scale: Float\n    let isSliding: Bool\n    let slidingWindow: Int\n    let slidingWindowPattern: Int\n\n    @ModuleInfo(key: \"q_proj\") var queryProj: Linear\n    @ModuleInfo(key: \"k_proj\") var keyProj: Linear\n    @ModuleInfo(key: \"v_proj\") var valueProj: Linear\n    @ModuleInfo(key: \"o_proj\") var outputProj: Linear\n\n    @ModuleInfo(key: \"q_norm\") var queryNorm: Gemma.RMSNorm\n    @ModuleInfo(key: \"k_norm\") var keyNorm: Gemma.RMSNorm\n\n    @ModuleInfo var rope: RoPE\n\n    init(_ config: Gemma3TextConfiguration, layerIdx: Int) {\n        let dim = config.hiddenSize\n        self.nHeads = config.attentionHeads\n        self.nKVHeads = config.kvHeads\n        self.repeats = nHeads / nKVHeads\n        self.headDim = config.headDim\n        self.layerIdx = layerIdx\n        self.slidingWindow = config.slidingWindow\n        self.slidingWindowPattern = config.slidingWindowPattern\n\n        self.scale = pow(config.queryPreAttnScalar, -0.5)\n\n        self._queryProj.wrappedValue = Linear(dim, nHeads * headDim, bias: false)\n        self._keyProj.wrappedValue = Linear(dim, nKVHeads * headDim, bias: false)\n        self._valueProj.wrappedValue = Linear(dim, nKVHeads * headDim, bias: false)\n        self._outputProj.wrappedValue = Linear(nHeads * headDim, dim, bias: false)\n\n        self._queryNorm.wrappedValue = Gemma.RMSNorm(\n            dimensions: headDim, eps: config.rmsNormEps)\n        self._keyNorm.wrappedValue = Gemma.RMSNorm(dimensions: headDim, eps: config.rmsNormEps)\n\n        self.isSliding = (layerIdx + 1) % config.slidingWindowPattern != 0\n\n        let baseFreq = isSliding ? config.ropeLocalBaseFreq : config.ropeGlobalBaseFreq\n        self._rope.wrappedValue = RoPE(\n            dimensions: headDim,\n            traditional: config.ropeTraditional,\n            base: baseFreq\n        )\n\n        super.init()\n    }\n\n    func callAsFunction(\n        _ x: MLXArray,\n        mask: MLXFast.ScaledDotProductAttentionMaskMode,\n        cache: KVCache? = nil\n    ) -> MLXArray {\n        let (B, L, _) = (x.dim(0), x.dim(1), x.dim(2))\n\n        var queries = queryProj(x)\n        var keys = keyProj(x)\n        var values = valueProj(x)\n\n        queries = queries.reshaped(B, L, nHeads, -1).transposed(0, 2, 1, 3)\n        keys = keys.reshaped(B, L, nKVHeads, -1).transposed(0, 2, 1, 3)\n        values = values.reshaped(B, L, nKVHeads, -1).transposed(0, 2, 1, 3)\n\n        queries = queryNorm(queries)\n        keys = keyNorm(keys)\n\n        if let cache {\n            queries = rope(queries, offset: cache.offset)\n            keys = rope(keys, offset: cache.offset)\n        } else {\n            queries = rope(queries)\n            keys = rope(keys)\n        }\n\n        // Sliding window masking\n        var finalMask = mask\n        if case .array(let maskArray) = mask {\n            let keySeqLen = keys.shape[2]\n            if maskArray.shape.last! != keySeqLen {\n                let slicedMask = maskArray[.ellipsis, (-keySeqLen)...]\n                finalMask = .array(slicedMask)\n            }\n        }\n\n        let output = attentionWithCacheUpdate(\n            queries: queries,\n            keys: keys,\n            values: values,\n            cache: cache,\n            scale: scale,\n            mask: finalMask\n        )\n        .transposed(0, 2, 1, 3)\n        .reshaped(B, L, -1)\n        return outputProj(output)\n    }\n}\n\nprivate class MLP: Module {\n    @ModuleInfo(key: \"gate_proj\") var gateProj: Linear\n    @ModuleInfo(key: \"down_proj\") var downProj: Linear\n    @ModuleInfo(key: \"up_proj\") var upProj: Linear\n\n    init(dimensions: Int, hiddenDimensions: Int) {\n        self._gateProj.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)\n        self._downProj.wrappedValue = Linear(hiddenDimensions, dimensions, bias: false)\n        self._upProj.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)\n        super.init()\n    }\n\n    func callAsFunction(_ x: MLXArray) -> MLXArray {\n        return downProj(geluApproximate(gateProj(x)) * upProj(x))\n    }\n}\n\nprivate class TransformerBlock: Module {\n    @ModuleInfo(key: \"self_attn\") var selfAttention: Attention\n    @ModuleInfo var mlp: MLP\n    @ModuleInfo(key: \"input_layernorm\") var inputLayerNorm: Gemma.RMSNorm\n    @ModuleInfo(key: \"post_attention_layernorm\") var postAttentionLayerNorm: Gemma.RMSNorm\n    @ModuleInfo(key: \"pre_feedforward_layernorm\") var preFeedforwardLayerNorm: Gemma.RMSNorm\n    @ModuleInfo(key: \"post_feedforward_layernorm\") var postFeedforwardLayerNorm: Gemma.RMSNorm\n\n    let numAttentionHeads: Int\n    let hiddenSize: Int\n    let layerIdx: Int\n\n    init(_ config: Gemma3TextConfiguration, layerIdx: Int) {\n        self.numAttentionHeads = config.attentionHeads\n        self.hiddenSize = config.hiddenSize\n        self.layerIdx = layerIdx\n\n        self._selfAttention.wrappedValue = Attention(config, layerIdx: layerIdx)\n        self.mlp = MLP(dimensions: config.hiddenSize, hiddenDimensions: config.intermediateSize)\n\n        self._inputLayerNorm.wrappedValue = Gemma.RMSNorm(\n            dimensions: config.hiddenSize, eps: config.rmsNormEps)\n        self._postAttentionLayerNorm.wrappedValue = Gemma.RMSNorm(\n            dimensions: config.hiddenSize, eps: config.rmsNormEps)\n        self._preFeedforwardLayerNorm.wrappedValue = Gemma.RMSNorm(\n            dimensions: config.hiddenSize, eps: config.rmsNormEps)\n        self._postFeedforwardLayerNorm.wrappedValue = Gemma.RMSNorm(\n            dimensions: config.hiddenSize, eps: config.rmsNormEps)\n\n        super.init()\n    }\n\n    func callAsFunction(\n        _ x: MLXArray,\n        mask: MLXFast.ScaledDotProductAttentionMaskMode,\n        cache: KVCache? = nil\n    ) -> MLXArray {\n        let inputNorm = inputLayerNorm(x)\n        let r = selfAttention(inputNorm, mask: mask, cache: cache)\n        let attnNorm = postAttentionLayerNorm(r)\n        let h = Gemma.clipResidual(x, attnNorm)\n        let preMLPNorm = preFeedforwardLayerNorm(h)\n        let r2 = mlp(preMLPNorm)\n        let postMLPNorm = postFeedforwardLayerNorm(r2)\n        let out = Gemma.clipResidual(h, postMLPNorm)\n        return out\n    }\n}\n\nprivate class Gemma3Model: Module {\n    @ModuleInfo(key: \"embed_tokens\") var embedTokens: Embedding\n    @ModuleInfo var layers: [TransformerBlock]\n    @ModuleInfo var norm: Gemma.RMSNorm\n\n    let config: Gemma3TextConfiguration\n\n    init(_ config: Gemma3TextConfiguration) {\n        self.config = config\n\n        self._embedTokens.wrappedValue = Embedding(\n            embeddingCount: config.vocabularySize,\n            dimensions: config.hiddenSize\n        )\n\n        self._layers.wrappedValue = (0 ..< config.hiddenLayers).map { layerIdx in\n            TransformerBlock(config, layerIdx: layerIdx)\n        }\n\n        self.norm = Gemma.RMSNorm(dimensions: config.hiddenSize, eps: config.rmsNormEps)\n\n        super.init()\n    }\n\n    func callAsFunction(\n        _ inputs: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode? = nil,\n        cache: [KVCache?]? = nil\n    )\n        -> MLXArray\n    {\n        var h: MLXArray\n        h = embedTokens(inputs)\n        let scale = MLXArray(sqrt(Float(config.hiddenSize)), dtype: .bfloat16)\n        h = h * scale.asType(h.dtype)\n        var layerCache = cache\n        if layerCache == nil {\n            layerCache = Array(repeating: nil as KVCache?, count: layers.count)\n        }\n        // Create attention masks\n        var fullMask: MLXFast.ScaledDotProductAttentionMaskMode = .none\n        var slidingWindowMask: MLXFast.ScaledDotProductAttentionMaskMode = .none\n        if mask == nil {\n            let j = config.slidingWindowPattern\n            let globalLayerCache: [KVCache]\n            if j > 0 && j <= (layerCache?.count ?? 0), let globalCache = layerCache?[j - 1] {\n                globalLayerCache = [globalCache]\n            } else {\n                globalLayerCache = []\n            }\n            fullMask = createAttentionMask(h: h, cache: globalLayerCache)\n            let allCaches = layerCache?.compactMap { $0 } ?? []\n            slidingWindowMask = createAttentionMask(h: h, cache: allCaches)\n        }\n        for (i, layer) in layers.enumerated() {\n            let isGlobal = (i % config.slidingWindowPattern == config.slidingWindowPattern - 1)\n\n            let localMask: MLXFast.ScaledDotProductAttentionMaskMode\n            if let mask {\n                localMask = mask\n            } else if isGlobal {\n                localMask = fullMask\n            } else {\n                localMask = slidingWindowMask\n            }\n            h = layer(h, mask: localMask, cache: layerCache?[i])\n        }\n        return norm(h)\n    }\n}\n\npublic class Gemma3TextModel: Module, LLMModel {\n\n    @ModuleInfo private var model: Gemma3Model\n    @ModuleInfo(key: \"lm_head\") var lmHead: Linear\n\n    public let config: Gemma3TextConfiguration\n    public var vocabularySize: Int { config.vocabularySize }\n\n    public init(_ config: Gemma3TextConfiguration) {\n        self.config = config\n        self.model = Gemma3Model(config)\n        self._lmHead.wrappedValue = Linear(config.hiddenSize, config.vocabularySize, bias: false)\n        super.init()\n    }\n\n    public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]? = nil) -> MLXArray {\n        var out = model(inputs, mask: nil, cache: cache)\n        out = lmHead(out)\n        return out\n    }\n\n    public func sanitize(weights: [String: MLXArray])\n        -> [String: MLXArray]\n    {\n        var processedWeights = weights\n\n        // VLM models converted using mlx_vlm.convert will still have\n        // the weights are under a language_model key\n        let unflattened = ModuleParameters.unflattened(weights)\n        if let lm = unflattened[\"language_model\"] {\n            processedWeights = Dictionary(uniqueKeysWithValues: lm.flattened())\n        }\n\n        if processedWeights[\"lm_head.weight\"] == nil {\n            if let embedWeight = processedWeights[\"model.embed_tokens.weight\"] {\n                processedWeights[\"lm_head.weight\"] = embedWeight\n            }\n        }\n        return processedWeights\n    }\n\n    public func newCache(parameters: GenerateParameters? = nil) -> [KVCache] {\n        var caches = [KVCache]()\n        let slidingWindow = config.slidingWindow\n        let slidingWindowPattern = config.slidingWindowPattern\n\n        for i in 0 ..< config.hiddenLayers {\n            let isGlobalLayer = (i % slidingWindowPattern == slidingWindowPattern - 1)\n\n            if isGlobalLayer {\n                // For global layers, use standard cache but with reasonable step size for long sequences\n                let cache = StandardKVCache()\n                cache.step = 1024  // Larger step size for efficiency with long sequences\n                caches.append(cache)\n            } else {\n                // For sliding window layers, use rotating cache\n                caches.append(\n                    RotatingKVCache(maxSize: slidingWindow, keep: 0)\n                )\n            }\n        }\n\n        return caches\n    }\n\n    /// Handles prompt processing for sequences\n    public func prepare(\n        _ input: LMInput, cache: [KVCache], windowSize: Int? = nil\n    ) throws -> PrepareResult {\n        let promptTokens = input.text.tokens\n        let promptCount = promptTokens.shape[0]\n\n        guard promptCount > 0 else {\n            print(\"Warning: Preparing with empty prompt tokens.\")\n            let emptyToken = MLXArray(Int32(0))[0 ..< 0]\n            return .tokens(.init(tokens: emptyToken))\n        }\n\n        return .tokens(input.text)\n    }\n}\n\nextension Gemma3TextModel: LoRAModel {\n    public func loraLinearLayers() -> LoRALinearLayers {\n        model.layers.map { ($0.selfAttention, [\"q_proj\", \"v_proj\"]) }\n    }\n}"}]}
{"messages": [{"role": "user", "content": "convert the following MLX Python code into MLX Swift: \n\n# Copyright \u00a9 2023-2024 Apple Inc.\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, Optional, Union\n\nimport mlx.core as mx\nimport mlx.nn as nn\n\nfrom .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention\nfrom .rope_utils import initialize_rope\n\n\n@dataclass\nclass ModelArgs(BaseModelArgs):\n    model_type: str\n    hidden_size: int\n    num_hidden_layers: int\n    intermediate_size: int\n    num_attention_heads: int\n    rms_norm_eps: float\n    vocab_size: int\n    logits_scaling: float\n    attention_multiplier: float\n    embedding_multiplier: float\n    residual_multiplier: float\n    max_position_embeddings: int\n    num_key_value_heads: int\n    attention_bias: bool\n    mlp_bias: bool\n    rope_theta: float\n    rope_scaling: Optional[Dict[str, Union[float, str]]] = None\n    tie_word_embeddings: bool = True\n\n\nclass Attention(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n\n        dim = args.hidden_size\n        self.n_heads = n_heads = args.num_attention_heads\n        self.n_kv_heads = n_kv_heads = args.num_key_value_heads\n\n        self.head_dim = head_dim = args.hidden_size // n_heads\n\n        self.scale = args.attention_multiplier\n        attention_bias = args.attention_bias\n        self.q_proj = nn.Linear(dim, n_heads * head_dim, bias=attention_bias)\n        self.k_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=attention_bias)\n        self.v_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=attention_bias)\n        self.o_proj = nn.Linear(n_heads * head_dim, dim, bias=attention_bias)\n\n        self.rope = initialize_rope(\n            self.head_dim,\n            args.rope_theta,\n            False,\n            args.rope_scaling,\n            args.max_position_embeddings,\n        )\n\n    def __call__(\n        self,\n        x: mx.array,\n        mask: Optional[mx.array] = None,\n        cache: Optional[Any] = None,\n    ) -> mx.array:\n        B, L, D = x.shape\n\n        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)\n\n        # Prepare the queries, keys and values for the attention computation\n        queries = queries.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)\n        keys = keys.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)\n        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)\n\n        if cache is not None:\n            queries = self.rope(queries, offset=cache.offset)\n            keys = self.rope(keys, offset=cache.offset)\n            keys, values = cache.update_and_fetch(keys, values)\n        else:\n            queries = self.rope(queries)\n            keys = self.rope(keys)\n\n        output = scaled_dot_product_attention(\n            queries, keys, values, cache=cache, scale=self.scale, mask=mask\n        )\n\n        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)\n        return self.o_proj(output)\n\n\nclass MLP(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n\n        dim = args.hidden_size\n        hidden_dim = args.intermediate_size\n        if hasattr(args, \"mlp_bias\"):\n            mlp_bias = args.mlp_bias\n        else:\n            mlp_bias = False\n\n        self.gate_proj = nn.Linear(dim, hidden_dim, bias=mlp_bias)\n        self.down_proj = nn.Linear(hidden_dim, dim, bias=mlp_bias)\n        self.up_proj = nn.Linear(dim, hidden_dim, bias=mlp_bias)\n\n    def __call__(self, x) -> mx.array:\n        return self.down_proj(nn.silu(self.gate_proj(x)) * self.up_proj(x))\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.num_attention_heads = args.num_attention_heads\n        self.hidden_size = args.hidden_size\n        self.self_attn = Attention(args)\n        self.mlp = MLP(args)\n        self.input_layernorm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)\n        self.post_attention_layernorm = nn.RMSNorm(\n            args.hidden_size, eps=args.rms_norm_eps\n        )\n        self.residual_multiplier = args.residual_multiplier\n\n    def __call__(\n        self,\n        x: mx.array,\n        mask: Optional[mx.array] = None,\n        cache: Optional[Any] = None,\n    ) -> mx.array:\n        r = self.self_attn(self.input_layernorm(x), mask, cache)\n        h = x + r * self.residual_multiplier\n        r = self.mlp(self.post_attention_layernorm(h))\n        out = h + r * self.residual_multiplier\n        return out\n\n\nclass GraniteModel(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.args = args\n        self.vocab_size = args.vocab_size\n        self.num_hidden_layers = args.num_hidden_layers\n        assert self.vocab_size > 0\n        self.embed_tokens = nn.Embedding(args.vocab_size, args.hidden_size)\n        self.layers = [\n            TransformerBlock(args=args) for _ in range(args.num_hidden_layers)\n        ]\n        self.norm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)\n        self.embedding_multiplier = args.embedding_multiplier\n\n    def __call__(\n        self,\n        inputs: mx.array,\n        mask: mx.array = None,\n        cache=None,\n    ):\n        h = self.embed_tokens(inputs) * self.embedding_multiplier\n\n        if mask is None:\n            mask = create_attention_mask(h, cache)\n\n        if cache is None:\n            cache = [None] * len(self.layers)\n\n        for layer, c in zip(self.layers, cache):\n            h = layer(h, mask, cache=c)\n\n        return self.norm(h)\n\n\nclass Model(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.args = args\n        self.model_type = args.model_type\n        self.model = GraniteModel(args)\n        if not args.tie_word_embeddings:\n            self.lm_head = nn.Linear(args.hidden_size, args.vocab_size, bias=False)\n        self.logits_scaling = args.logits_scaling\n\n    def __call__(\n        self,\n        inputs: mx.array,\n        mask: mx.array = None,\n        cache=None,\n    ):\n        out = self.model(inputs, mask, cache)\n        if self.args.tie_word_embeddings:\n            out = self.model.embed_tokens.as_linear(out)\n        else:\n            out = self.lm_head(out)\n        return out / self.logits_scaling\n\n    @property\n    def layers(self):\n        return self.model.layers"}, {"role": "assistant", "content": "//\n//  Granite.swift\n//  mlx-swift-examples\n//\n//  Created by Sachin Desai on 4/25/25.\n//\n\n// Port of https://github.com/ml-explore/mlx-lm/blob/main/mlx_lm/models/granite.py\n\nimport Foundation\nimport MLX\nimport MLXLMCommon\nimport MLXNN\n\nprivate class Attention: Module {\n    let args: GraniteConfiguration\n    let scale: Float\n\n    @ModuleInfo(key: \"q_proj\") var wq: Linear\n    @ModuleInfo(key: \"k_proj\") var wk: Linear\n    @ModuleInfo(key: \"v_proj\") var wv: Linear\n    @ModuleInfo(key: \"o_proj\") var wo: Linear\n\n    let rope: RoPE\n\n    public init(_ args: GraniteConfiguration) {\n        self.args = args\n\n        let dim = args.hiddenSize\n        let nHeads = args.attentionHeads\n        let nKvHeads = args.kvHeads\n        let headDim = dim / nHeads\n\n        self.scale = args.attentionMultiplier\n        let attentionBias = args.attentionBias\n\n        self._wq.wrappedValue = Linear(dim, nHeads * headDim, bias: attentionBias)\n        self._wk.wrappedValue = Linear(dim, nKvHeads * headDim, bias: attentionBias)\n        self._wv.wrappedValue = Linear(dim, nKvHeads * headDim, bias: attentionBias)\n        self._wo.wrappedValue = Linear(nHeads * headDim, dim, bias: attentionBias)\n\n        let ropeScale: Float\n        if let ropeScaling = args.ropeScaling, ropeScaling[\"type\"] == .string(\"linear\"),\n            let factor = ropeScaling[\"factor\"]\n        {\n            if let v = factor.asFloat() {\n                ropeScale = 1 / v\n            } else {\n                fatalError(\"ropeScaling.factor must be a float\")\n            }\n        } else {\n            ropeScale = 1\n        }\n        rope = RoPE(dimensions: headDim, traditional: false, base: args.ropeTheta, scale: ropeScale)\n    }\n\n    public func callAsFunction(\n        _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?\n    ) -> MLXArray {\n        let (B, L) = (x.dim(0), x.dim(1))\n\n        var queries = wq(x)\n        var keys = wk(x)\n        var values = wv(x)\n\n        // prepare the queries, keys and values for the attention computation\n        queries = queries.reshaped(B, L, args.attentionHeads, -1).transposed(0, 2, 1, 3)\n        keys = keys.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)\n        values = values.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)\n\n        if let cache {\n            queries = rope(queries, offset: cache.offset)\n            keys = rope(keys, offset: cache.offset)\n        } else {\n            queries = rope(queries)\n            keys = rope(keys)\n        }\n\n        let output = attentionWithCacheUpdate(\n            queries: queries,\n            keys: keys,\n            values: values,\n            cache: cache,\n            scale: scale,\n            mask: mask\n        )\n        .transposed(0, 2, 1, 3)\n        .reshaped(B, L, -1)\n\n        return wo(output)\n    }\n}\n\nprivate class MLP: Module, UnaryLayer {\n    @ModuleInfo(key: \"gate_proj\") var gate: Linear\n    @ModuleInfo(key: \"down_proj\") var down: Linear\n    @ModuleInfo(key: \"up_proj\") var up: Linear\n\n    public init(_ args: GraniteConfiguration) {\n        let dim = args.hiddenSize\n        let hiddenDim = args.intermediateSize\n        let mlpBias = args.mlpBias\n\n        self._gate.wrappedValue = Linear(dim, hiddenDim, bias: mlpBias)\n        self._down.wrappedValue = Linear(hiddenDim, dim, bias: mlpBias)\n        self._up.wrappedValue = Linear(dim, hiddenDim, bias: mlpBias)\n    }\n\n    public func callAsFunction(_ x: MLXArray) -> MLXArray {\n        down(silu(gate(x)) * up(x))\n    }\n}\n\nprivate class TransformerBlock: Module {\n    @ModuleInfo(key: \"self_attn\") var attention: Attention\n    let mlp: MLP\n\n    @ModuleInfo(key: \"input_layernorm\") var inputLayerNorm: RMSNorm\n    @ModuleInfo(key: \"post_attention_layernorm\") var postAttentionLayerNorm: RMSNorm\n\n    let residualMultiplier: Float\n\n    public init(_ args: GraniteConfiguration) {\n        let attentionHeads = args.attentionHeads\n        let hiddenSize = args.hiddenSize\n\n        self._attention.wrappedValue = Attention(args)\n        self.mlp = MLP(args)\n\n        self._inputLayerNorm.wrappedValue = RMSNorm(\n            dimensions: hiddenSize, eps: args.rmsNormEps)\n        self._postAttentionLayerNorm.wrappedValue = RMSNorm(\n            dimensions: hiddenSize, eps: args.rmsNormEps)\n\n        self.residualMultiplier = args.residualMultiplier\n    }\n\n    public func callAsFunction(\n        _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?\n    ) -> MLXArray {\n        var r = attention(inputLayerNorm(x), mask: mask, cache: cache)\n        let h = x + r * residualMultiplier\n        r = mlp(postAttentionLayerNorm(h))\n        let out = h + r * residualMultiplier\n        return out\n    }\n}\n\nprivate class GraniteModelInner: Module {\n    @ModuleInfo(key: \"embed_tokens\") var embedTokens: Embedding\n    fileprivate let layers: [TransformerBlock]\n    let norm: RMSNorm\n    let embeddingMultiplier: Float\n\n    public init(_ args: GraniteConfiguration) {\n        precondition(args.vocabularySize > 0)\n\n        self._embedTokens.wrappedValue = Embedding(\n            embeddingCount: args.vocabularySize, dimensions: args.hiddenSize)\n        self.layers = (0 ..< args.hiddenLayers)\n            .map { _ in\n                TransformerBlock(args)\n            }\n\n        self.norm = RMSNorm(dimensions: args.hiddenSize, eps: args.rmsNormEps)\n        self.embeddingMultiplier = args.embeddingMultiplier\n    }\n\n    public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]? = nil) -> MLXArray {\n        var h = embedTokens(inputs) * embeddingMultiplier\n\n        let mask = createAttentionMask(h: h, cache: cache)\n\n        for (i, layer) in layers.enumerated() {\n            h = layer(h, mask: mask, cache: cache?[i])\n        }\n\n        return norm(h)\n    }\n}\n\npublic class GraniteModel: Module, LLMModel, KVCacheDimensionProvider {\n    public let vocabularySize: Int\n    public let kvHeads: [Int]\n    let logitsScaling: Float\n\n    private let model: GraniteModelInner\n    let configuration: GraniteConfiguration\n\n    @ModuleInfo(key: \"lm_head\") var lmHead: Linear?\n\n    public init(_ args: GraniteConfiguration) {\n        self.configuration = args\n        self.vocabularySize = args.vocabularySize\n        self.kvHeads = (0 ..< args.hiddenLayers).map { _ in args.kvHeads }\n\n        self.model = GraniteModelInner(args)\n\n        if !args.tieWordEmbeddings {\n            self._lmHead.wrappedValue = Linear(args.hiddenSize, args.vocabularySize, bias: false)\n        }\n        self.logitsScaling = args.logitsScaling\n    }\n\n    public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]?) -> MLXArray {\n        var out = model(inputs, cache: cache)\n        if let lmHead {\n            out = lmHead(out)\n        } else {\n            out = model.embedTokens.asLinear(out)\n        }\n\n        return out / logitsScaling\n    }\n}\n\npublic struct GraniteConfiguration: Codable, Sendable {\n    var hiddenSize: Int\n    var hiddenLayers: Int\n    var intermediateSize: Int\n    var attentionHeads: Int\n    var rmsNormEps: Float\n    var vocabularySize: Int\n    var logitsScaling: Float\n    var attentionMultiplier: Float\n    var embeddingMultiplier: Float\n    var residualMultiplier: Float\n    var maxPositionEmbeddings: Int\n    var kvHeads: Int\n    var attentionBias: Bool\n    var mlpBias: Bool\n    var ropeTheta: Float\n    var ropeTraditional: Bool = false\n    var ropeScaling: [String: StringOrNumber]? = nil\n    var tieWordEmbeddings: Bool = true\n\n    enum CodingKeys: String, CodingKey {\n        case hiddenSize = \"hidden_size\"\n        case hiddenLayers = \"num_hidden_layers\"\n        case intermediateSize = \"intermediate_size\"\n        case attentionHeads = \"num_attention_heads\"\n        case rmsNormEps = \"rms_norm_eps\"\n        case vocabularySize = \"vocab_size\"\n        case logitsScaling = \"logits_scaling\"\n        case attentionMultiplier = \"attention_multiplier\"\n        case embeddingMultiplier = \"embedding_multiplier\"\n        case residualMultiplier = \"residual_multiplier\"\n        case maxPositionEmbeddings = \"max_position_embeddings\"\n        case kvHeads = \"num_key_value_heads\"\n        case attentionBias = \"attention_bias\"\n        case mlpBias = \"mlp_bias\"\n        case ropeTheta = \"rope_theta\"\n        case ropeScaling = \"rope_scaling\"\n        case tieWordEmbeddings = \"tie_word_embeddings\"\n    }\n\n    public init(from decoder: Decoder) throws {\n        let container: KeyedDecodingContainer<GraniteConfiguration.CodingKeys> =\n            try decoder.container(keyedBy: GraniteConfiguration.CodingKeys.self)\n\n        self.hiddenSize = try container.decode(Int.self, forKey: .hiddenSize)\n        self.hiddenLayers = try container.decode(Int.self, forKey: .hiddenLayers)\n        self.intermediateSize = try container.decode(Int.self, forKey: .intermediateSize)\n        self.attentionHeads = try container.decode(Int.self, forKey: .attentionHeads)\n        self.rmsNormEps = try container.decode(Float.self, forKey: .rmsNormEps)\n        self.vocabularySize = try container.decode(Int.self, forKey: .vocabularySize)\n        self.logitsScaling = try container.decode(Float.self, forKey: .logitsScaling)\n        self.attentionMultiplier = try container.decode(Float.self, forKey: .attentionMultiplier)\n        self.embeddingMultiplier = try container.decode(Float.self, forKey: .embeddingMultiplier)\n        self.residualMultiplier = try container.decode(Float.self, forKey: .residualMultiplier)\n        self.maxPositionEmbeddings = try container.decode(Int.self, forKey: .maxPositionEmbeddings)\n        self.kvHeads = try container.decode(Int.self, forKey: .kvHeads)\n        self.attentionBias = try container.decode(Bool.self, forKey: .attentionBias)\n        self.mlpBias = try container.decode(Bool.self, forKey: .mlpBias) ?? false\n        self.ropeTheta = try container.decodeIfPresent(Float.self, forKey: .ropeTheta) ?? 10000000.0\n        self.ropeScaling = try container.decodeIfPresent(\n            [String: StringOrNumber].self, forKey: .ropeScaling)\n        self.tieWordEmbeddings = try container.decode(Bool.self, forKey: .tieWordEmbeddings)\n    }\n}\n\n// MARK: - LoRA\n\nextension GraniteModel: LoRAModel {\n    public func loraLinearLayers() -> LoRALinearLayers {\n        model.layers.map { ($0.attention, [\"q_proj\", \"v_proj\"]) }\n    }\n}"}]}
{"messages": [{"role": "user", "content": "convert the following MLX Python code into MLX Swift: \n\n# Copyright \u00a9 2023-2024 Apple Inc.\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, Optional, Union\n\nimport mlx.core as mx\nimport mlx.nn as nn\n\nfrom .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention\n\n\n@dataclass\nclass ModelArgs(BaseModelArgs):\n    model_type: str\n    hidden_size: int\n    num_hidden_layers: int\n    intermediate_size: int\n    num_attention_heads: int\n    rms_norm_eps: float\n    vocab_size: int\n    bias: bool = True\n    max_position_embeddings: int = 32768\n    num_key_value_heads: int = None\n    rope_theta: float = 10000\n    rope_traditional: bool = False\n    rope_scaling: Optional[Dict[str, Union[float, str]]] = None\n    tie_word_embeddings: bool = False\n\n    def __post_init__(self):\n        if self.num_key_value_heads is None:\n            self.num_key_value_heads = self.num_attention_heads\n\n        if self.rope_scaling:\n            required_keys = {\"factor\", \"type\"}\n            if not all(key in self.rope_scaling for key in required_keys):\n                raise ValueError(f\"rope_scaling must contain keys {required_keys}\")\n\n            if self.rope_scaling[\"type\"] not in [\"linear\", \"dynamic\"]:\n                raise ValueError(\n                    \"rope_scaling 'type' currently only supports 'linear' or 'dynamic\"\n                )\n\n\nclass DynamicNTKScalingRoPE(nn.Module):\n    \"\"\"Implements the rotary positional encoding with Dynamic NTK scaling.\"\"\"\n\n    def __init__(\n        self,\n        dims: int,\n        max_position_embeddings: int = 2048,\n        traditional: bool = False,\n        base: float = 10000,\n        scale: float = 1.0,\n    ):\n        super().__init__()\n        self.max_position_embeddings = max_position_embeddings\n        self.original_base = base\n        self.dims = dims\n        self.traditional = traditional\n        self.scale = scale\n\n    def extra_repr(self):\n        return f\"{self.dims}, traditional={self.traditional}, max_position_embeddings={self.max_position_embeddings}, scaling_factor={self.scaling_factor}\"\n\n    def __call__(self, x, offset: int = 0):\n        seq_len = x.shape[1] + offset\n        if seq_len > self.max_position_embeddings:\n            base = self.original_base * (\n                (self.scale * seq_len / self.max_position_embeddings) - (self.scale - 1)\n            ) ** (self.dims / (self.dims - 2))\n        else:\n            base = self.original_base\n\n        return mx.fast.rope(\n            x,\n            self.dims,\n            traditional=self.traditional,\n            base=base,\n            scale=self.scale,\n            offset=offset,\n        )\n\n\nclass Attention(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n\n        dim = args.hidden_size\n        self.n_heads = n_heads = args.num_attention_heads\n        self.n_kv_heads = n_kv_heads = args.num_key_value_heads\n        self.n_kv_groups = n_heads // args.num_key_value_heads\n\n        self.head_dim = head_dim = args.hidden_size // n_heads\n        self.scale = head_dim**-0.5\n\n        self.wqkv = nn.Linear(\n            dim, (n_heads + 2 * n_kv_heads) * head_dim, bias=args.bias\n        )\n        self.wo = nn.Linear(n_heads * head_dim, dim, bias=args.bias)\n\n        rope_scale = (\n            1 / args.rope_scaling[\"factor\"]\n            if args.rope_scaling is not None and args.rope_scaling[\"type\"] == \"linear\"\n            else 2.0\n        )\n\n        self.rope = DynamicNTKScalingRoPE(\n            head_dim,\n            max_position_embeddings=args.max_position_embeddings,\n            traditional=args.rope_traditional,\n            base=args.rope_theta,\n            scale=rope_scale,\n        )\n\n    def __call__(\n        self,\n        x: mx.array,\n        mask: Optional[mx.array] = None,\n        cache: Optional[Any] = None,\n    ) -> mx.array:\n        B, L, D = x.shape\n\n        qkv_states = self.wqkv(x)\n        qkv_states = qkv_states.reshape(B, L, -1, 2 + self.n_kv_groups, self.head_dim)\n\n        queries = qkv_states[..., : self.n_kv_groups, :]\n        queries = queries.reshape(B, L, -1, self.head_dim)\n        keys = qkv_states[..., -2, :]\n        values = qkv_states[..., -1, :]\n\n        # Prepare the queries, keys and values for the attention computation\n        queries = queries.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)\n        keys = keys.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)\n        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)\n\n        if cache is not None:\n            queries = self.rope(queries, offset=cache.offset)\n            keys = self.rope(keys, offset=cache.offset)\n            keys, values = cache.update_and_fetch(keys, values)\n        else:\n            queries = self.rope(queries)\n            keys = self.rope(keys)\n\n        output = scaled_dot_product_attention(\n            queries, keys, values, cache=cache, scale=self.scale, mask=mask\n        )\n        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)\n        return self.wo(output)\n\n\nclass MLP(nn.Module):\n    def __init__(self, dim, hidden_dim):\n        super().__init__()\n        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n\n    def __call__(self, x) -> mx.array:\n        return self.w2(nn.silu(self.w1(x)) * self.w3(x))\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.attention = Attention(args)\n        self.feed_forward = MLP(args.hidden_size, args.intermediate_size)\n        self.attention_norm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)\n        self.ffn_norm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)\n\n    def __call__(\n        self,\n        x: mx.array,\n        mask: Optional[mx.array] = None,\n        cache: Optional[Any] = None,\n    ) -> mx.array:\n        r = self.attention(self.attention_norm(x), mask, cache)\n        h = x + r\n        r = self.feed_forward(self.ffn_norm(h))\n        out = h + r\n        return out\n\n\nclass InternLM2Model(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        assert args.vocab_size > 0\n        self.tok_embeddings = nn.Embedding(args.vocab_size, args.hidden_size)\n        self.layers = [\n            TransformerBlock(args=args) for _ in range(args.num_hidden_layers)\n        ]\n        self.norm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)\n\n    def __call__(\n        self,\n        inputs: mx.array,\n        mask: mx.array = None,\n        cache=None,\n    ):\n        h = self.tok_embeddings(inputs)\n\n        if mask is None:\n            mask = create_attention_mask(h, cache)\n\n        if cache is None:\n            cache = [None] * len(self.layers)\n\n        for layer, c in zip(self.layers, cache):\n            h = layer(h, mask, cache=c)\n\n        return self.norm(h)\n\n\nclass Model(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.args = args\n        self.model_type = args.model_type\n        self.model = InternLM2Model(args)\n        if not args.tie_word_embeddings:\n            self.output = nn.Linear(args.hidden_size, args.vocab_size, bias=False)\n\n    def __call__(\n        self,\n        inputs: mx.array,\n        mask: mx.array = None,\n        cache=None,\n    ):\n        out = self.model(inputs, mask, cache)\n        if self.args.tie_word_embeddings:\n            out = self.model.tok_embeddings.as_linear(out)\n        else:\n            out = self.output(out)\n        return out\n\n    def sanitize(self, weights):\n        # Remove unused precomputed rotary freqs\n        return {k: v for k, v in weights.items() if \"attention.rope.inv_freq\" not in k}\n\n    @property\n    def layers(self):\n        return self.model.layers"}, {"role": "assistant", "content": "// Copyright \u00a9 2024 Apple Inc.\n\nimport Foundation\nimport MLX\nimport MLXLMCommon\nimport MLXNN\n\n// Port of https://github.com/maiqingqiang/mlx-examples/blob/main/llms/mlx_lm/models/internlm2.py\n\nprivate class DynamicNTKScalingRoPE: Module {\n    let dims: Int\n    let maxPositionEmbeddings: Int\n    let traditional: Bool\n    let originalBase: Float\n    var scale: Float\n\n    init(\n        dims: Int, maxPositionEmbeddings: Int = 2048, traditional: Bool = false,\n        base: Float = 10000, scale: Float = 1.0\n    ) {\n        self.dims = dims\n        self.maxPositionEmbeddings = maxPositionEmbeddings\n        self.traditional = traditional\n        self.originalBase = base\n        self.scale = scale\n    }\n\n    func callAsFunction(_ x: MLXArray, offset: Int = 0) -> MLXArray {\n        let seqLen = x.dim(1) + offset\n        var base = originalBase\n        if seqLen > maxPositionEmbeddings {\n            base *= pow(\n                (scale * Float(seqLen) / Float(maxPositionEmbeddings)) - (scale - 1),\n                Float(dims) / Float(dims - 2))\n        }\n        return MLXFast.RoPE(\n            x, dimensions: dims, traditional: traditional, base: base, scale: scale, offset: offset)\n    }\n}\n\nprivate class Attention: Module {\n    let args: InternLM2Configuration\n    let scale: Float\n\n    let heads: Int\n    let kvHeads: Int\n    let kvGroups: Int\n    let headDim: Int\n\n    @ModuleInfo(key: \"wqkv\") var wqkv: Linear\n    @ModuleInfo(key: \"wo\") var wo: Linear\n\n    let rope: DynamicNTKScalingRoPE\n\n    init(_ args: InternLM2Configuration) {\n        self.args = args\n\n        let dim = args.hiddenSize\n        self.heads = args.attentionHeads\n        self.kvHeads = args.kvHeads\n        self.kvGroups = args.kvGroups\n\n        self.headDim = args.hiddenSize / self.heads\n        self.scale = pow(Float(headDim), -0.5)\n\n        self._wqkv.wrappedValue = Linear(\n            dim, (self.heads + 2 * self.kvHeads) * self.headDim, bias: args.bias)\n        self._wo.wrappedValue = Linear(self.heads * self.headDim, dim, bias: args.bias)\n\n        let ropeScale: Float\n        if let ropeScaling = args.ropeScaling, ropeScaling[\"type\"] == .string(\"linear\"),\n            let factor = ropeScaling[\"factor\"]\n        {\n            if let v = factor.asFloat() {\n                ropeScale = 1 / v\n            } else {\n                fatalError(\"ropeScaling.factor must be a float\")\n            }\n        } else {\n            ropeScale = 1\n        }\n\n        self.rope = DynamicNTKScalingRoPE(\n            dims: self.headDim,\n            maxPositionEmbeddings: args.maxPositionEmbeddings,\n            traditional: args.ropeTraditional,\n            base: args.ropeTheta,\n            scale: ropeScale\n        )\n    }\n\n    func callAsFunction(\n        _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?\n    ) -> MLXArray {\n        let (B, L) = (x.dim(0), x.dim(1))\n\n        var qkvStates = wqkv(x)\n        qkvStates = qkvStates.reshaped(B, L, -1, 2 + self.kvGroups, self.headDim)\n        var queries = qkvStates[.ellipsis, ..<self.kvGroups, 0...]\n        queries = queries.reshaped(B, L, -1, self.headDim)\n        var keys = qkvStates[.ellipsis, -2, 0...]\n        var values = qkvStates[.ellipsis, -1, 0...]\n\n        // prepare the queries, keys and values for the attention computation\n        queries = queries.reshaped(B, L, args.attentionHeads, -1).transposed(0, 2, 1, 3)\n        keys = keys.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)\n        values = values.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)\n\n        if let cache {\n            queries = rope(queries, offset: cache.offset)\n            keys = rope(keys, offset: cache.offset)\n        } else {\n            queries = rope(queries)\n            keys = rope(keys)\n        }\n\n        let output = attentionWithCacheUpdate(\n            queries: queries,\n            keys: keys,\n            values: values,\n            cache: cache,\n            scale: scale,\n            mask: mask\n        )\n        .transposed(0, 2, 1, 3)\n        .reshaped(B, L, -1)\n\n        return wo(output)\n    }\n}\n\nprivate class MLP: Module, UnaryLayer {\n    @ModuleInfo(key: \"w1\") var w1: Linear\n    @ModuleInfo(key: \"w2\") var w2: Linear\n    @ModuleInfo(key: \"w3\") var w3: Linear\n\n    init(dim: Int, hiddenDim: Int) {\n        self._w1.wrappedValue = Linear(dim, hiddenDim, bias: false)\n        self._w2.wrappedValue = Linear(hiddenDim, dim, bias: false)\n        self._w3.wrappedValue = Linear(dim, hiddenDim, bias: false)\n    }\n\n    func callAsFunction(_ x: MLXArray) -> MLXArray {\n        return w2(silu(w1(x)) * w3(x))\n    }\n}\n\nprivate class TransformerBlock: Module {\n    @ModuleInfo(key: \"attention\") var attention: Attention\n    @ModuleInfo(key: \"feed_forward\") var feedForward: MLP\n\n    @ModuleInfo(key: \"attention_norm\") var attentionNorm: RMSNorm\n    @ModuleInfo(key: \"ffn_norm\") var ffnNorm: RMSNorm\n\n    init(_ args: InternLM2Configuration) {\n        self._attention.wrappedValue = Attention(args)\n        self._feedForward.wrappedValue = MLP(dim: args.hiddenSize, hiddenDim: args.intermediateSize)\n        self._attentionNorm.wrappedValue = RMSNorm(\n            dimensions: args.hiddenSize, eps: args.rmsNormEps)\n        self._ffnNorm.wrappedValue = RMSNorm(dimensions: args.hiddenSize, eps: args.rmsNormEps)\n    }\n\n    func callAsFunction(\n        _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?\n    ) -> MLXArray {\n        var r = attention(attentionNorm(x), mask: mask, cache: cache)\n        let h = x + r\n        r = feedForward(ffnNorm(h))\n        let out = h + r\n        return out\n    }\n}\n\nprivate class InternLM2ModelInner: Module {\n    @ModuleInfo(key: \"tok_embeddings\") var tokEmbeddings: Embedding\n\n    let layers: [TransformerBlock]\n    let norm: RMSNorm\n\n    init(_ args: InternLM2Configuration) {\n        precondition(args.vocabularySize > 0)\n\n        self._tokEmbeddings.wrappedValue = Embedding(\n            embeddingCount: args.vocabularySize, dimensions: args.hiddenSize)\n\n        self.layers = (0 ..< args.hiddenLayers).map { _ in TransformerBlock(args) }\n        self.norm = RMSNorm(dimensions: args.hiddenSize, eps: args.rmsNormEps)\n    }\n\n    func callAsFunction(_ inputs: MLXArray, cache: [KVCache]? = nil) -> MLXArray {\n        var h = tokEmbeddings(inputs)\n\n        let mask = createAttentionMask(h: h, cache: cache)\n\n        for (i, layer) in layers.enumerated() {\n            h = layer(h, mask: mask, cache: cache?[i])\n        }\n\n        return norm(h)\n    }\n}\n\npublic class InternLM2Model: Module, LLMModel, KVCacheDimensionProvider {\n    public let vocabularySize: Int\n    public let kvHeads: [Int]\n\n    fileprivate let model: InternLM2ModelInner\n\n    @ModuleInfo(key: \"output\") var output: Linear?\n\n    public init(_ args: InternLM2Configuration) {\n        self.vocabularySize = args.vocabularySize\n        self.kvHeads = (0 ..< args.hiddenLayers).map { _ in args.kvHeads }\n        self.model = InternLM2ModelInner(args)\n        if !args.tieWordEmbeddings {\n            self._output.wrappedValue = Linear(args.hiddenSize, args.vocabularySize, bias: false)\n        }\n    }\n\n    public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]?) -> MLXArray {\n        let out = model(inputs, cache: cache)\n        if let output {\n            return output(out)\n        } else {\n            return model.tokEmbeddings.asLinear(out)\n        }\n    }\n\n    public func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {\n        // Remove unused precomputed rotary frequencies\n        weights.filter {\n            !$0.key.contains(\"attention.rope.inv_freq\")\n        }\n    }\n}\n\nextension InternLM2Model: LoRAModel {\n    public func loraLinearLayers() -> LoRALinearLayers {\n        model.layers.map { ($0.attention, [\"q_proj\", \"v_proj\"]) }\n    }\n}\n\npublic struct InternLM2Configuration: Codable, Sendable {\n    var hiddenSize: Int\n    var hiddenLayers: Int\n    var intermediateSize: Int\n    var attentionHeads: Int\n    var rmsNormEps: Float\n    var vocabularySize: Int\n    var kvHeads: Int\n    var maxPositionEmbeddings: Int = 32768\n    var ropeTheta: Float = 10000\n    var ropeTraditional: Bool = false\n    var ropeScaling: [String: StringOrNumber]?\n    var tieWordEmbeddings: Bool = false\n    var bias: Bool = true\n\n    var kvGroups: Int {\n        attentionHeads / kvHeads\n    }\n\n    enum CodingKeys: String, CodingKey {\n        case hiddenSize = \"hidden_size\"\n        case hiddenLayers = \"num_hidden_layers\"\n        case intermediateSize = \"intermediate_size\"\n        case attentionHeads = \"num_attention_heads\"\n        case rmsNormEps = \"rms_norm_eps\"\n        case vocabularySize = \"vocab_size\"\n        case kvHeads = \"num_key_value_heads\"\n        case maxPositionEmbeddings = \"max_position_embeddings\"\n        case ropeTheta = \"rope_theta\"\n        case ropeTraditional = \"rope_traditional\"\n        case ropeScaling = \"rope_scaling\"\n        case tieWordEmbeddings = \"tie_word_embeddings\"\n        case bias = \"bias\"\n    }\n\n    public init(from decoder: Decoder) throws {\n        let container = try decoder.container(keyedBy: CodingKeys.self)\n\n        hiddenSize = try container.decode(Int.self, forKey: .hiddenSize)\n        hiddenLayers = try container.decode(Int.self, forKey: .hiddenLayers)\n        intermediateSize = try container.decode(Int.self, forKey: .intermediateSize)\n        attentionHeads = try container.decode(Int.self, forKey: .attentionHeads)\n        rmsNormEps = try container.decode(Float.self, forKey: .rmsNormEps)\n        vocabularySize = try container.decode(Int.self, forKey: .vocabularySize)\n        kvHeads = try container.decodeIfPresent(Int.self, forKey: .kvHeads) ?? attentionHeads\n        maxPositionEmbeddings = try container.decode(Int.self, forKey: .maxPositionEmbeddings)\n        if let ropeTheta = try container.decodeIfPresent(Float.self, forKey: .ropeTheta) {\n            self.ropeTheta = ropeTheta\n        }\n        if let ropeTraditional = try container.decodeIfPresent(Bool.self, forKey: .ropeTraditional)\n        {\n            self.ropeTraditional = ropeTraditional\n        }\n        ropeScaling = try container.decodeIfPresent(\n            [String: StringOrNumber].self, forKey: .ropeScaling)\n        if let tieWordEmbeddings = try container.decodeIfPresent(\n            Bool.self, forKey: .tieWordEmbeddings)\n        {\n            self.tieWordEmbeddings = tieWordEmbeddings\n        }\n        if let bias = try container.decodeIfPresent(Bool.self, forKey: .bias) {\n            self.bias = bias\n        }\n\n        if let ropeScaling {\n            let requiredKeys: Set<String> = [\"factor\", \"type\"]\n            let keys = Set(ropeScaling.keys)\n            if !requiredKeys.isSubset(of: keys) {\n                throw DecodingError.dataCorruptedError(\n                    forKey: .ropeScaling, in: container,\n                    debugDescription: \"rope_scaling must contain keys \\(requiredKeys)\"\n                )\n            }\n            if let type = ropeScaling[\"type\"],\n                type != .string(\"linear\") && type != .string(\"dynamic\")\n            {\n                throw DecodingError.dataCorruptedError(\n                    forKey: .ropeScaling, in: container,\n                    debugDescription:\n                        \"rope_scaling 'type' currently only supports 'linear' or 'dynamic'\"\n                )\n            }\n        }\n    }\n}"}]}
{"messages": [{"role": "user", "content": "convert the following MLX Python code into MLX Swift: \n\n# Copyright \u00a9 2023-2024 Apple Inc.\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, Optional, Union\n\nimport mlx.core as mx\nimport mlx.nn as nn\n\nfrom .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention\nfrom .rope_utils import initialize_rope\n\n\n@dataclass\nclass ModelArgs(BaseModelArgs):\n    model_type: str\n    hidden_size: int\n    num_hidden_layers: int\n    intermediate_size: int\n    num_attention_heads: int\n    rms_norm_eps: float\n    vocab_size: int\n    head_dim: Optional[int] = None\n    max_position_embeddings: Optional[int] = None\n    num_key_value_heads: Optional[int] = None\n    attention_bias: bool = False\n    mlp_bias: bool = False\n    rope_theta: float = 10000\n    rope_traditional: bool = False\n    rope_scaling: Optional[Dict[str, Union[float, str]]] = None\n    tie_word_embeddings: bool = True\n\n    def __post_init__(self):\n        if self.num_key_value_heads is None:\n            self.num_key_value_heads = self.num_attention_heads\n\n\nclass Attention(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n\n        dim = args.hidden_size\n        self.n_heads = n_heads = args.num_attention_heads\n        self.n_kv_heads = n_kv_heads = args.num_key_value_heads\n\n        self.head_dim = head_dim = args.head_dim or args.hidden_size // n_heads\n\n        self.scale = head_dim**-0.5\n        if hasattr(args, \"attention_bias\"):\n            attention_bias = args.attention_bias\n        else:\n            attention_bias = False\n\n        self.q_proj = nn.Linear(dim, n_heads * head_dim, bias=attention_bias)\n        self.k_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=attention_bias)\n        self.v_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=attention_bias)\n        self.o_proj = nn.Linear(n_heads * head_dim, dim, bias=attention_bias)\n\n        self.rope = initialize_rope(\n            self.head_dim,\n            args.rope_theta,\n            args.rope_traditional,\n            args.rope_scaling,\n            args.max_position_embeddings,\n        )\n\n    def __call__(\n        self,\n        x: mx.array,\n        mask: Optional[mx.array] = None,\n        cache: Optional[Any] = None,\n    ) -> mx.array:\n        B, L, D = x.shape\n\n        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)\n\n        # Prepare the queries, keys and values for the attention computation\n        queries = queries.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)\n        keys = keys.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)\n        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)\n\n        if cache is not None:\n            queries = self.rope(queries, offset=cache.offset)\n            keys = self.rope(keys, offset=cache.offset)\n            keys, values = cache.update_and_fetch(keys, values)\n        else:\n            queries = self.rope(queries)\n            keys = self.rope(keys)\n\n        output = scaled_dot_product_attention(\n            queries, keys, values, cache=cache, scale=self.scale, mask=mask\n        )\n\n        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)\n        return self.o_proj(output)\n\n\nclass MLP(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n\n        dim = args.hidden_size\n        hidden_dim = args.intermediate_size\n        if hasattr(args, \"mlp_bias\"):\n            mlp_bias = args.mlp_bias\n        else:\n            mlp_bias = False\n\n        self.gate_proj = nn.Linear(dim, hidden_dim, bias=mlp_bias)\n        self.down_proj = nn.Linear(hidden_dim, dim, bias=mlp_bias)\n        self.up_proj = nn.Linear(dim, hidden_dim, bias=mlp_bias)\n\n    def __call__(self, x) -> mx.array:\n        return self.down_proj(nn.silu(self.gate_proj(x)) * self.up_proj(x))\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.num_attention_heads = args.num_attention_heads\n        self.hidden_size = args.hidden_size\n        self.self_attn = Attention(args)\n        self.mlp = MLP(args)\n        self.input_layernorm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)\n        self.post_attention_layernorm = nn.RMSNorm(\n            args.hidden_size, eps=args.rms_norm_eps\n        )\n        self.args = args\n\n    def __call__(\n        self,\n        x: mx.array,\n        mask: Optional[mx.array] = None,\n        cache: Optional[Any] = None,\n    ) -> mx.array:\n        r = self.self_attn(self.input_layernorm(x), mask, cache)\n        h = x + r\n        r = self.mlp(self.post_attention_layernorm(h))\n        out = h + r\n        return out\n\n\nclass LlamaModel(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.args = args\n        self.vocab_size = args.vocab_size\n        self.num_hidden_layers = args.num_hidden_layers\n        assert self.vocab_size > 0\n        self.embed_tokens = nn.Embedding(args.vocab_size, args.hidden_size)\n        self.layers = [\n            TransformerBlock(args=args) for _ in range(args.num_hidden_layers)\n        ]\n        self.norm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)\n\n    def __call__(\n        self,\n        inputs: mx.array,\n        mask: mx.array = None,\n        cache=None,\n        input_embeddings: Optional[mx.array] = None,\n    ):\n        if input_embeddings is not None:\n            h = input_embeddings\n        else:\n            h = self.embed_tokens(inputs)\n\n        if mask is None:\n            mask = create_attention_mask(h, cache)\n\n        if cache is None:\n            cache = [None] * len(self.layers)\n\n        for layer, c in zip(self.layers, cache):\n            h = layer(h, mask, cache=c)\n\n        return self.norm(h)\n\n\nclass Model(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.args = args\n        self.model_type = args.model_type\n        self.model = LlamaModel(args)\n        if not args.tie_word_embeddings:\n            self.lm_head = nn.Linear(args.hidden_size, args.vocab_size, bias=False)\n\n    def __call__(\n        self,\n        inputs: mx.array,\n        mask: mx.array = None,\n        cache=None,\n        input_embeddings: Optional[mx.array] = None,\n    ):\n        out = self.model(inputs, mask, cache, input_embeddings)\n        if self.args.tie_word_embeddings:\n            out = self.model.embed_tokens.as_linear(out)\n        else:\n            out = self.lm_head(out)\n        return out\n\n    def sanitize(self, weights):\n        # Remove unused precomputed rotary freqs\n        weights = {\n            k: v for k, v in weights.items() if \"self_attn.rotary_emb.inv_freq\" not in k\n        }\n        if self.args.tie_word_embeddings:\n            weights.pop(\"lm_head.weight\", None)\n        return weights\n\n    @property\n    def layers(self):\n        return self.model.layers"}, {"role": "assistant", "content": "// Copyright \u00a9 2024 Apple Inc.\n\nimport Foundation\nimport MLX\nimport MLXLMCommon\nimport MLXNN\nimport Tokenizers\n\n// port of https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/models/llama.py\n\nfunc computeBaseFrequency(\n    base: Float, dims: Int, ropeType: String, ropeScaling: [String: StringOrNumber]?\n)\n    -> Float\n{\n    if ropeType != \"llama3\" {\n        return base\n    }\n\n    guard let ropeScaling = ropeScaling else {\n        return base\n    }\n\n    guard case .float(let factor) = ropeScaling[\"factor\"],\n        case .float(let lowFreqFactor) = ropeScaling[\"low_freq_factor\"] ?? .float(1.0),\n        case .float(let highFreqFactor) = ropeScaling[\"high_freq_factor\"] ?? .float(4.0),\n        case .float(let oldContextLen) = ropeScaling[\"original_max_position_embeddings\"]\n            ?? .float(8192)\n    else {\n        return base\n    }\n\n    let lowFreqWavelen = oldContextLen / lowFreqFactor\n    let highFreqWavelen = oldContextLen / highFreqFactor\n\n    let freqs = (0 ..< dims).compactMap { index -> Float? in\n        if index % 2 == 0 {\n            return pow(base, Float(index) / Float(dims))\n        }\n        return nil\n    }\n\n    let newBaseFreqs = freqs.map { freq -> Float in\n        let wavelen = 2 * .pi / freq\n        let smooth = max(\n            0, min(1, (wavelen - highFreqWavelen) / (lowFreqWavelen - highFreqWavelen)))\n        return freq * ((1 - smooth) * factor + smooth)\n    }\n\n    return newBaseFreqs.reduce(0, +) / Float(newBaseFreqs.count)\n}\n\nprivate class DynamicNTKScalingRoPE: Module {\n    let dims: Int\n    let maxPositionEmbeddings: Int\n    let traditional: Bool\n    var base: Float?\n    let scale: Float\n    let ropeType: String\n    let ropeScaling: [String: StringOrNumber]?\n    var freqs: MLXArray?\n\n    init(\n        dims: Int,\n        maxPositionEmbeddings: Int?,\n        traditional: Bool = false,\n        base: Float = 10000,\n        scale: Float = 1.0,\n        ropeType: String = \"default\",\n        ropeScaling: [String: StringOrNumber]? = nil\n    ) {\n        self.dims = dims\n        self.maxPositionEmbeddings = maxPositionEmbeddings ?? 2048\n        self.traditional = traditional\n        self.base = base\n        self.scale = scale\n        self.ropeType = ropeType\n        self.ropeScaling = ropeScaling\n        super.init()\n        computeFreqs()\n    }\n\n    private func computeFreqs() {\n        if ropeType != \"llama3\" {\n            freqs = nil\n            return\n        }\n\n        guard let ropeScaling = ropeScaling,\n            case .float(let factor) = ropeScaling[\"factor\"],\n            case .float(let lowFreqFactor) = ropeScaling[\"low_freq_factor\"] ?? .float(1.0),\n            case .float(let highFreqFactor) = ropeScaling[\"high_freq_factor\"] ?? .float(4.0),\n            case .float(let oldContextLen) = ropeScaling[\"original_max_position_embeddings\"]\n                ?? .float(8192),\n            let base\n        else {\n            freqs = nil\n            return\n        }\n\n        let lowFreqWavelen = oldContextLen / lowFreqFactor\n        let highFreqWavelen = oldContextLen / highFreqFactor\n\n        let indices = MLXArray(stride(from: 0, to: dims, by: 2))\n        var frequencies = MLX.pow(base, indices / Float(dims))\n        let wavelens = 2 * Float.pi * frequencies\n\n        frequencies = MLX.where(\n            wavelens .> MLXArray(lowFreqWavelen), frequencies * factor, frequencies)\n        let isMediumFreq = MLX.logicalAnd(\n            wavelens .> MLXArray(highFreqWavelen),\n            wavelens .< MLXArray(lowFreqWavelen)\n        )\n        let smoothFactors =\n            (oldContextLen / wavelens - lowFreqFactor) / (highFreqFactor - lowFreqFactor)\n        let smoothFreqs = frequencies / ((1 - smoothFactors) / factor + smoothFactors)\n\n        freqs = MLX.where(isMediumFreq, smoothFreqs, frequencies)\n        self.base = nil\n    }\n\n    func callAsFunction(_ x: MLXArray, offset: Int = 0) -> MLXArray {\n        MLXFast.RoPE(\n            x,\n            dimensions: dims,\n            traditional: traditional,\n            base: base,\n            scale: scale,\n            offset: offset,\n            freqs: freqs\n        )\n    }\n}\n\nprivate class Attention: Module {\n\n    let args: LlamaConfiguration\n    let scale: Float\n\n    @ModuleInfo(key: \"q_proj\") var wq: Linear\n    @ModuleInfo(key: \"k_proj\") var wk: Linear\n    @ModuleInfo(key: \"v_proj\") var wv: Linear\n    @ModuleInfo(key: \"o_proj\") var wo: Linear\n\n    let rope: DynamicNTKScalingRoPE\n\n    init(_ args: LlamaConfiguration) {\n        self.args = args\n\n        let dim = args.hiddenSize\n        let heads = args.attentionHeads\n        let kvHeads = args.kvHeads\n\n        let headDim = args.resolvedHeadDimensions\n        self.scale = pow(Float(headDim), -0.5)\n\n        self._wq.wrappedValue = Linear(dim, heads * headDim, bias: args.attentionBias)\n        self._wk.wrappedValue = Linear(dim, kvHeads * headDim, bias: args.attentionBias)\n        self._wv.wrappedValue = Linear(dim, kvHeads * headDim, bias: args.attentionBias)\n        self._wo.wrappedValue = Linear(heads * headDim, dim, bias: args.attentionBias)\n\n        self.rope = DynamicNTKScalingRoPE(\n            dims: headDim,\n            maxPositionEmbeddings: args.maxPositionEmbeddings,\n            traditional: args.ropeTraditional,\n            base: args.ropeTheta,\n            scale: 1.0,\n            ropeType: {\n                if case .string(let value) = args.ropeScaling?[\"type\"] {\n                    return value\n                } else {\n                    return \"default\"\n                }\n            }(),\n            ropeScaling: args.ropeScaling)\n    }\n\n    func callAsFunction(\n        _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?\n    ) -> MLXArray {\n        let (B, L) = (x.dim(0), x.dim(1))\n\n        var queries = wq(x)\n        var keys = wk(x)\n        var values = wv(x)\n\n        // Prepare the queries, keys and values for the attention computation\n        queries = queries.reshaped(B, L, args.attentionHeads, -1).transposed(0, 2, 1, 3)\n        keys = keys.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)\n        values = values.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)\n\n        if let cache {\n            queries = rope(queries, offset: cache.offset)\n            keys = rope(keys, offset: cache.offset)\n        } else {\n            queries = rope(queries)\n            keys = rope(keys)\n        }\n\n        let output = attentionWithCacheUpdate(\n            queries: queries,\n            keys: keys,\n            values: values,\n            cache: cache,\n            scale: scale,\n            mask: mask\n        )\n        .transposed(0, 2, 1, 3)\n        .reshaped(B, L, -1)\n\n        return wo(output)\n    }\n}\n\nprivate class MLP: Module, UnaryLayer {\n\n    @ModuleInfo(key: \"gate_proj\") var gate: Linear\n    @ModuleInfo(key: \"down_proj\") var down: Linear\n    @ModuleInfo(key: \"up_proj\") var up: Linear\n\n    init(_ args: LlamaConfiguration) {\n        self._gate.wrappedValue = Linear(args.hiddenSize, args.intermediateSize, bias: args.mlpBias)\n        self._down.wrappedValue = Linear(args.intermediateSize, args.hiddenSize, bias: args.mlpBias)\n        self._up.wrappedValue = Linear(args.hiddenSize, args.intermediateSize, bias: args.mlpBias)\n    }\n\n    func callAsFunction(_ x: MLXArray) -> MLXArray {\n        let activation = silu(gate(x))\n        return down(activation * up(x))\n    }\n}\n\nprivate class TransformerBlock: Module {\n    @ModuleInfo(key: \"self_attn\") var attention: Attention\n    @ModuleInfo(key: \"mlp\") var mlp: MLP\n\n    @ModuleInfo(key: \"input_layernorm\") var inputLayerNorm: RMSNorm\n    @ModuleInfo(key: \"post_attention_layernorm\") var postAttentionLayerNorm: RMSNorm\n\n    init(_ args: LlamaConfiguration) {\n        self._attention.wrappedValue = Attention(args)\n        self._mlp.wrappedValue = MLP(args)\n        self._inputLayerNorm.wrappedValue = RMSNorm(\n            dimensions: args.hiddenSize, eps: args.rmsNormEps)\n        self._postAttentionLayerNorm.wrappedValue = RMSNorm(\n            dimensions: args.hiddenSize, eps: args.rmsNormEps)\n    }\n\n    func callAsFunction(\n        _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?\n    ) -> MLXArray {\n        var r = attention(inputLayerNorm(x), mask: mask, cache: cache)\n        let h = x + r\n        r = mlp(postAttentionLayerNorm(h))\n        let out = h + r\n        return out\n    }\n}\n\nprivate class LlamaModelInner: Module {\n\n    @ModuleInfo(key: \"embed_tokens\") var embedTokens: Embedding\n\n    let layers: [TransformerBlock]\n    let norm: RMSNorm\n\n    init(_ args: LlamaConfiguration) {\n        precondition(args.vocabularySize > 0)\n\n        self._embedTokens.wrappedValue = Embedding(\n            embeddingCount: args.vocabularySize, dimensions: args.hiddenSize)\n\n        self.layers = (0 ..< args.hiddenLayers).map { _ in TransformerBlock(args) }\n        self.norm = RMSNorm(dimensions: args.hiddenSize, eps: args.rmsNormEps)\n    }\n\n    func callAsFunction(_ inputs: MLXArray, cache: [KVCache]? = nil) -> MLXArray {\n        var h = embedTokens(inputs)\n\n        let mask = createAttentionMask(h: h, cache: cache)\n\n        for (i, layer) in layers.enumerated() {\n            h = layer(h, mask: mask, cache: cache?[i])\n        }\n\n        return norm(h)\n    }\n}\n\n/// Model for Llama and Mistral model types.\npublic class LlamaModel: Module, LLMModel, KVCacheDimensionProvider {\n\n    public let vocabularySize: Int\n    public let kvHeads: [Int]\n\n    fileprivate let model: LlamaModelInner\n\n    @ModuleInfo(key: \"lm_head\") var lmHead: Linear?\n\n    public init(_ args: LlamaConfiguration) {\n        self.vocabularySize = args.vocabularySize\n        self.kvHeads = (0 ..< args.hiddenLayers).map { _ in args.kvHeads }\n        self.model = LlamaModelInner(args)\n        if !args.tieWordEmbeddings {\n            self._lmHead.wrappedValue = Linear(args.hiddenSize, args.vocabularySize, bias: false)\n        }\n    }\n\n    public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]?) -> MLXArray {\n        let out = model(inputs, cache: cache)\n        if let lmHead {\n            return lmHead(out)\n        } else {\n            return model.embedTokens.asLinear(out)\n        }\n    }\n\n    public func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {\n        // Remove unused precomputed rotary frequencies\n        weights.filter {\n            !$0.key.contains(\"self_attn.rotary_emb.inv_freq\")\n        }\n    }\n\n    public func messageGenerator(tokenizer: any Tokenizer) -> any MessageGenerator {\n        // some models allow the system role and some do not -- this is enforced\n        // by the chat template (code).\n        do {\n            let probe = [\n                [\n                    \"role\": \"system\",\n                    \"content\": \"test\",\n                ]\n            ]\n            _ = try tokenizer.applyChatTemplate(messages: probe)\n            return DefaultMessageGenerator()\n        } catch {\n            return NoSystemMessageGenerator()\n        }\n    }\n}\n\npublic struct LlamaConfiguration: Codable, Sendable {\n\n    var hiddenSize: Int\n    var hiddenLayers: Int\n    var intermediateSize: Int\n    var attentionHeads: Int\n    var headDimensions: Int?\n    var rmsNormEps: Float\n    var vocabularySize: Int\n    var kvHeads: Int\n    var maxPositionEmbeddings: Int?\n    var ropeTheta: Float = 10_000\n    var ropeTraditional: Bool = false\n    var ropeScaling: [String: StringOrNumber]?\n    var tieWordEmbeddings: Bool = true\n    var attentionBias: Bool = false\n    var mlpBias: Bool = false\n\n    public init(\n        hiddenSize: Int, hiddenLayers: Int, intermediateSize: Int, attentionHeads: Int,\n        headDimensions: Int? = nil, rmsNormEps: Float, vocabularySize: Int, kvHeads: Int,\n        maxPositionEmbeddings: Int? = nil, ropeTheta: Float = 10_000, ropeTraditional: Bool = false,\n        ropeScaling: [String: StringOrNumber]? = nil, tieWordEmbeddings: Bool = true,\n        attentionBias: Bool = false, mlpBias: Bool = false\n    ) {\n        self.hiddenSize = hiddenSize\n        self.hiddenLayers = hiddenLayers\n        self.intermediateSize = intermediateSize\n        self.attentionHeads = attentionHeads\n        self.headDimensions = headDimensions\n        self.rmsNormEps = rmsNormEps\n        self.vocabularySize = vocabularySize\n        self.kvHeads = kvHeads\n        self.maxPositionEmbeddings = maxPositionEmbeddings\n        self.ropeTheta = ropeTheta\n        self.ropeTraditional = ropeTraditional\n        self.ropeScaling = ropeScaling\n        self.tieWordEmbeddings = tieWordEmbeddings\n        self.attentionBias = attentionBias\n        self.mlpBias = mlpBias\n    }\n\n    var resolvedHeadDimensions: Int {\n        headDimensions ?? (hiddenSize / attentionHeads)\n    }\n\n    enum CodingKeys: String, CodingKey {\n        case hiddenSize = \"hidden_size\"\n        case hiddenLayers = \"num_hidden_layers\"\n        case intermediateSize = \"intermediate_size\"\n        case attentionHeads = \"num_attention_heads\"\n        case headDimensions = \"head_dim\"\n        case rmsNormEps = \"rms_norm_eps\"\n        case vocabularySize = \"vocab_size\"\n        case kvHeads = \"num_key_value_heads\"\n        case maxPositionEmbeddings = \"max_position_embeddings\"\n        case ropeTheta = \"rope_theta\"\n        case ropeTraditional = \"rope_traditional\"\n        case ropeScaling = \"rope_scaling\"\n        case tieWordEmbeddings = \"tie_word_embeddings\"\n        case attentionBias = \"attention_bias\"\n        case mlpBias = \"mlp_bias\"\n    }\n\n    public init(from decoder: Swift.Decoder) throws {\n        let container = try decoder.container(keyedBy: CodingKeys.self)\n\n        hiddenSize = try container.decode(Int.self, forKey: .hiddenSize)\n        hiddenLayers = try container.decode(Int.self, forKey: .hiddenLayers)\n        intermediateSize = try container.decode(Int.self, forKey: .intermediateSize)\n        attentionHeads = try container.decode(Int.self, forKey: .attentionHeads)\n        headDimensions = try container.decodeIfPresent(Int.self, forKey: .headDimensions)\n        rmsNormEps = try container.decode(Float.self, forKey: .rmsNormEps)\n        vocabularySize = try container.decode(Int.self, forKey: .vocabularySize)\n        kvHeads = try container.decodeIfPresent(Int.self, forKey: .kvHeads) ?? attentionHeads\n        maxPositionEmbeddings = try container.decodeIfPresent(\n            Int.self, forKey: .maxPositionEmbeddings)\n        if let ropeTheta = try container.decodeIfPresent(Float.self, forKey: .ropeTheta) {\n            self.ropeTheta = ropeTheta\n        }\n        if let ropeTraditional = try container.decodeIfPresent(Bool.self, forKey: .ropeTraditional)\n        {\n            self.ropeTraditional = ropeTraditional\n        }\n        ropeScaling = try container.decodeIfPresent(\n            [String: StringOrNumber].self, forKey: .ropeScaling)\n        if let tieWordEmbeddings = try container.decodeIfPresent(\n            Bool.self, forKey: .tieWordEmbeddings)\n        {\n            self.tieWordEmbeddings = tieWordEmbeddings\n        }\n        if let attentionBias = try container.decodeIfPresent(Bool.self, forKey: .attentionBias) {\n            self.attentionBias = attentionBias\n        }\n        if let mlpBias = try container.decodeIfPresent(Bool.self, forKey: .mlpBias) {\n            self.mlpBias = mlpBias\n        }\n\n        if let ropeScaling {\n            if ropeScaling[\"factor\"] == nil {\n                throw DecodingError.dataCorruptedError(\n                    forKey: .ropeScaling, in: container,\n                    debugDescription: \"rope_scaling must contain 'factor'\")\n            }\n            if let ropeType = ropeScaling[\"type\"] ?? ropeScaling[\"rope_type\"] {\n                if case .string = ropeType {\n                    let options = [\n                        StringOrNumber.string(\"linear\"), StringOrNumber.string(\"dynamic\"),\n                        StringOrNumber.string(\"llama3\"),\n                    ]\n                    if !options.contains(ropeType) {\n                        throw DecodingError.dataCorruptedError(\n                            forKey: .ropeScaling, in: container,\n                            debugDescription:\n                                \"rope_scaling 'type' currently only supports 'linear', 'dynamic', or 'llama3'\"\n                        )\n                    }\n                }\n            } else {\n                throw DecodingError.dataCorruptedError(\n                    forKey: .ropeScaling, in: container,\n                    debugDescription: \"rope_scaling must contain either 'type' or 'rope_type'\")\n            }\n        }\n    }\n}\n\n// MARK: - LoRA\n\nextension LlamaModel: LoRAModel {\n    public func loraLinearLayers() -> LoRALinearLayers {\n        model.layers.map { ($0.attention, [\"q_proj\", \"v_proj\"]) }\n    }\n}"}]}
{"messages": [{"role": "user", "content": "convert the following MLX Python code into MLX Swift: \n\n# Copyright \u00a9 2023-2025 Apple Inc.\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, Optional, Union\n\nimport mlx.core as mx\nimport mlx.nn as nn\n\nfrom .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention\nfrom .rope_utils import initialize_rope\n\n\n@dataclass\nclass ModelArgs(BaseModelArgs):\n    model_type: str\n    hidden_size: int\n    num_hidden_layers: int\n    intermediate_size: int\n    num_attention_heads: int\n    rms_norm_eps: float\n    vocab_size: int\n    num_key_value_heads: int\n    max_position_embeddings: int = 32768\n    rope_theta: float = 10000.0\n    rope_traditional: bool = False\n    rope_scaling: Optional[Dict[str, Union[float, str]]] = None\n    tie_word_embeddings: bool = False\n    num_nextn_predict_layers: int = 2\n\n\nclass Attention(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n\n        dim = args.hidden_size\n        self.n_heads = n_heads = args.num_attention_heads\n        assert args.num_key_value_heads is not None\n        self.n_kv_heads = n_kv_heads = args.num_key_value_heads\n\n        head_dim = args.hidden_size // n_heads\n        self.scale = head_dim**-0.5\n\n        self.q_proj = nn.Linear(dim, n_heads * head_dim, bias=True)\n        self.k_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=True)\n        self.v_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=True)\n        self.o_proj = nn.Linear(n_heads * head_dim, dim, bias=False)\n\n        self.rope = initialize_rope(\n            head_dim,\n            base=args.rope_theta,\n            traditional=args.rope_traditional,\n            scaling_config=args.rope_scaling,\n            max_position_embeddings=args.max_position_embeddings,\n        )\n\n    def __call__(\n        self,\n        x: mx.array,\n        mask: Optional[mx.array] = None,\n        cache: Optional[Any] = None,\n    ) -> mx.array:\n        B, L, D = x.shape\n\n        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)\n\n        queries = queries.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)\n        keys = keys.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)\n        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)\n\n        if cache is not None:\n            queries = self.rope(queries, offset=cache.offset)\n            keys = self.rope(keys, offset=cache.offset)\n            keys, values = cache.update_and_fetch(keys, values)\n        else:\n            queries = self.rope(queries)\n            keys = self.rope(keys)\n\n        output = scaled_dot_product_attention(\n            queries, keys, values, cache=cache, scale=self.scale, mask=mask\n        )\n        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)\n        return self.o_proj(output)\n\n\nclass MLP(nn.Module):\n    def __init__(self, dim, hidden_dim):\n        super().__init__()\n        self.gate_proj = nn.Linear(dim, hidden_dim, bias=False)\n        self.down_proj = nn.Linear(hidden_dim, dim, bias=False)\n        self.up_proj = nn.Linear(dim, hidden_dim, bias=False)\n\n    def __call__(self, x) -> mx.array:\n        return self.down_proj(nn.silu(self.gate_proj(x)) * self.up_proj(x))\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.num_attention_heads = args.num_attention_heads\n        self.hidden_size = args.hidden_size\n        self.self_attn = Attention(args)\n        self.mlp = MLP(args.hidden_size, args.intermediate_size)\n        self.input_layernorm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)\n        self.post_attention_layernorm = nn.RMSNorm(\n            args.hidden_size, eps=args.rms_norm_eps\n        )\n        self.args = args\n\n    def __call__(\n        self,\n        x: mx.array,\n        mask: Optional[mx.array] = None,\n        cache: Optional[Any] = None,\n    ) -> mx.array:\n        r = self.self_attn(self.input_layernorm(x), mask, cache)\n        h = x + r\n        r = self.mlp(self.post_attention_layernorm(h))\n        out = h + r\n        return out\n\n\nclass MiMoModel(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.args = args\n        self.vocab_size = args.vocab_size\n        self.num_hidden_layers = args.num_hidden_layers\n        self.num_nextn_predict_layers = args.num_nextn_predict_layers\n\n        assert self.vocab_size > 0\n        self.embed_tokens = nn.Embedding(args.vocab_size, args.hidden_size)\n        self.layers = [\n            TransformerBlock(args=args) for _ in range(args.num_hidden_layers)\n        ]\n        self.norm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)\n\n    def __call__(\n        self,\n        inputs: mx.array,\n        mask: mx.array = None,\n        cache=None,\n    ):\n        h = self.embed_tokens(inputs)\n\n        if mask is None:\n            mask = create_attention_mask(h, cache)\n\n        if cache is None:\n            cache = [None] * len(self.layers)\n\n        for layer, c in zip(self.layers, cache):\n            h = layer(h, mask, c)\n\n        h = self.norm(h)\n\n        return h\n\n\nclass Model(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.args = args\n        self.model_type = args.model_type\n        self.model = MiMoModel(args)\n        if not args.tie_word_embeddings:\n            self.lm_head = nn.Linear(args.hidden_size, args.vocab_size, bias=False)\n\n    def __call__(\n        self,\n        inputs: mx.array,\n        mask: mx.array = None,\n        cache=None,\n    ):\n        out = self.model(inputs, mask, cache)\n\n        if self.args.tie_word_embeddings:\n            out = self.model.embed_tokens.as_linear(out)\n        else:\n            out = self.lm_head(out)\n\n        return out\n\n    def sanitize(self, weights):\n        if self.args.tie_word_embeddings:\n            weights.pop(\"lm_head.weight\", None)\n\n        return {\n            k: v\n            for k, v in weights.items()\n            if \"self_attn.rotary_emb.inv_freq\" not in k\n            and not k.startswith(\"model.mtp_layers.\")\n        }\n\n    @property\n    def layers(self):\n        return self.model.layers"}, {"role": "assistant", "content": "//\n//  MiMo.swift\n//  LLM\n//\n//  Created by John Mai on 2025/5/3.\n//\n\nimport Foundation\nimport MLX\nimport MLXLMCommon\nimport MLXNN\n\nprivate class Attention: Module {\n    let args: MiMoConfiguration\n    let scale: Float\n\n    @ModuleInfo(key: \"q_proj\") var wq: Linear\n    @ModuleInfo(key: \"k_proj\") var wk: Linear\n    @ModuleInfo(key: \"v_proj\") var wv: Linear\n    @ModuleInfo(key: \"o_proj\") var wo: Linear\n\n    let rope: RoPE\n\n    public init(_ args: MiMoConfiguration) {\n        self.args = args\n\n        let dim = args.hiddenSize\n        let heads = args.attentionHeads\n        let kvHeads = args.kvHeads\n\n        let headDim = args.hiddenSize / heads\n        self.scale = pow(Float(headDim), -0.5)\n\n        _wq.wrappedValue = Linear(dim, heads * headDim, bias: true)\n        _wk.wrappedValue = Linear(dim, kvHeads * headDim, bias: true)\n        _wv.wrappedValue = Linear(dim, kvHeads * headDim, bias: true)\n        _wo.wrappedValue = Linear(heads * headDim, dim, bias: false)\n\n        let ropeScale: Float\n        if let ropeScaling = args.ropeScaling, ropeScaling[\"type\"] == .string(\"linear\"),\n            let factor = ropeScaling[\"factor\"]\n        {\n            if let v = factor.asFloat() {\n                ropeScale = 1 / v\n            } else {\n                fatalError(\"ropeScaling.factor must be a float\")\n            }\n        } else {\n            ropeScale = 1\n        }\n\n        self.rope = RoPE(\n            dimensions: headDim, traditional: args.ropeTraditional, base: args.ropeTheta,\n            scale: ropeScale)\n    }\n\n    public func callAsFunction(\n        _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?\n    ) -> MLXArray {\n        let (B, L) = (x.dim(0), x.dim(1))\n\n        var queries = wq(x)\n        var keys = wk(x)\n        var values = wv(x)\n\n        // prepare the queries, keys and values for the attention computation\n        queries = queries.reshaped(B, L, args.attentionHeads, -1).transposed(0, 2, 1, 3)\n        keys = keys.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)\n        values = values.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)\n\n        if let cache {\n            queries = rope(queries, offset: cache.offset)\n            keys = rope(keys, offset: cache.offset)\n        } else {\n            queries = rope(queries)\n            keys = rope(keys)\n        }\n\n        let output = attentionWithCacheUpdate(\n            queries: queries,\n            keys: keys,\n            values: values,\n            cache: cache,\n            scale: scale,\n            mask: mask\n        )\n        .transposed(0, 2, 1, 3)\n        .reshaped(B, L, -1)\n\n        return wo(output)\n    }\n}\n\nprivate class MLP: Module, UnaryLayer {\n    @ModuleInfo(key: \"gate_proj\") var gate: Linear\n    @ModuleInfo(key: \"down_proj\") var down: Linear\n    @ModuleInfo(key: \"up_proj\") var up: Linear\n\n    public init(dimensions: Int, hiddenDimensions: Int) {\n        _gate.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)\n        _down.wrappedValue = Linear(hiddenDimensions, dimensions, bias: false)\n        _up.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)\n    }\n\n    public func callAsFunction(_ x: MLXArray) -> MLXArray {\n        down(silu(gate(x)) * up(x))\n    }\n}\n\nprivate class TransformerBlock: Module {\n    @ModuleInfo(key: \"self_attn\") var attention: Attention\n    let mlp: MLP\n\n    @ModuleInfo(key: \"input_layernorm\") var inputLayerNorm: RMSNorm\n    @ModuleInfo(key: \"post_attention_layernorm\") var postAttentionLayerNorm: RMSNorm\n\n    public init(_ args: MiMoConfiguration) {\n        _attention.wrappedValue = Attention(args)\n        self.mlp = MLP(dimensions: args.hiddenSize, hiddenDimensions: args.intermediateSize)\n        _inputLayerNorm.wrappedValue = RMSNorm(\n            dimensions: args.hiddenSize, eps: args.rmsNormEps)\n        _postAttentionLayerNorm.wrappedValue = RMSNorm(\n            dimensions: args.hiddenSize, eps: args.rmsNormEps)\n    }\n\n    public func callAsFunction(\n        _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?\n    ) -> MLXArray {\n        var r = attention(inputLayerNorm(x), mask: mask, cache: cache)\n        let h = x + r\n        r = mlp(postAttentionLayerNorm(h))\n        let out = h + r\n        return out\n    }\n}\n\nprivate class MiMoModelInner: Module {\n    @ModuleInfo(key: \"embed_tokens\") var embedTokens: Embedding\n\n    fileprivate let layers: [TransformerBlock]\n    let norm: RMSNorm\n\n    let numNextnPredictLayers: Int\n\n    public init(_ args: MiMoConfiguration) {\n        precondition(args.vocabularySize > 0)\n\n        _embedTokens.wrappedValue = Embedding(\n            embeddingCount: args.vocabularySize, dimensions: args.hiddenSize)\n\n        self.layers = (0 ..< args.hiddenLayers).map { _ in\n            TransformerBlock(args)\n        }\n        self.norm = RMSNorm(dimensions: args.hiddenSize, eps: args.rmsNormEps)\n        self.numNextnPredictLayers = args.numNextnPredictLayers\n    }\n\n    public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]? = nil) -> MLXArray {\n        var h = embedTokens(inputs)\n\n        let mask = createAttentionMask(h: h, cache: cache)\n\n        for (i, layer) in layers.enumerated() {\n            h = layer(h, mask: mask, cache: cache?[i])\n        }\n\n        return norm(h)\n    }\n}\n\npublic class MiMoModel: Module, LLMModel, KVCacheDimensionProvider {\n    public let vocabularySize: Int\n    public let kvHeads: [Int]\n\n    private let model: MiMoModelInner\n    let configuration: MiMoConfiguration\n\n    @ModuleInfo(key: \"lm_head\") var lmHead: Linear?\n\n    public init(_ args: MiMoConfiguration) {\n        self.configuration = args\n        self.vocabularySize = args.vocabularySize\n        self.kvHeads = (0 ..< args.hiddenLayers).map { _ in args.kvHeads }\n        self.model = MiMoModelInner(args)\n\n        if !args.tieWordEmbeddings {\n            _lmHead.wrappedValue = Linear(args.hiddenSize, args.vocabularySize, bias: false)\n        }\n    }\n\n    public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]? = nil) -> MLXArray {\n        let out = model(inputs, cache: cache)\n\n        if let lmHead = lmHead {\n            return lmHead(out)\n        } else {\n            return model.embedTokens.asLinear(out)\n        }\n    }\n\n    public func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {\n        var weights = weights\n\n        if configuration.tieWordEmbeddings {\n            weights.removeValue(forKey: \"lm_head.weight\")\n        }\n\n        // Remove unused precomputed rotary freqs and mtp_layers\n        return weights.filter { key, _ in\n            !key.contains(\"self_attn.rotary_emb.inv_freq\") && !key.hasPrefix(\"model.mtp_layers.\")\n        }\n    }\n}\n\npublic struct MiMoConfiguration: Codable, Sendable {\n    var hiddenSize: Int\n    var hiddenLayers: Int\n    var intermediateSize: Int\n    var attentionHeads: Int\n    var rmsNormEps: Float\n    var vocabularySize: Int\n    var kvHeads: Int\n    var maxPositionEmbeddings: Int\n    var ropeTheta: Float\n    var ropeTraditional: Bool\n    var ropeScaling: [String: StringOrNumber]?\n    var tieWordEmbeddings: Bool\n    var numNextnPredictLayers: Int\n\n    enum CodingKeys: String, CodingKey {\n        case hiddenSize = \"hidden_size\"\n        case hiddenLayers = \"num_hidden_layers\"\n        case intermediateSize = \"intermediate_size\"\n        case attentionHeads = \"num_attention_heads\"\n        case rmsNormEps = \"rms_norm_eps\"\n        case vocabularySize = \"vocab_size\"\n        case kvHeads = \"num_key_value_heads\"\n        case maxPositionEmbeddings = \"max_position_embeddings\"\n        case ropeTheta = \"rope_theta\"\n        case ropeTraditional = \"rope_traditional\"\n        case ropeScaling = \"rope_scaling\"\n        case tieWordEmbeddings = \"tie_word_embeddings\"\n        case numNextnPredictLayers = \"num_nextn_predict_layers\"\n    }\n\n    public init(from decoder: Decoder) throws {\n        let container = try decoder.container(keyedBy: CodingKeys.self)\n\n        self.hiddenSize = try container.decode(Int.self, forKey: .hiddenSize)\n        self.hiddenLayers = try container.decode(Int.self, forKey: .hiddenLayers)\n        self.intermediateSize = try container.decode(Int.self, forKey: .intermediateSize)\n        self.attentionHeads = try container.decode(Int.self, forKey: .attentionHeads)\n        self.rmsNormEps = try container.decode(Float.self, forKey: .rmsNormEps)\n        self.vocabularySize = try container.decode(Int.self, forKey: .vocabularySize)\n        self.kvHeads = try container.decode(Int.self, forKey: .kvHeads)\n        self.maxPositionEmbeddings =\n            try container.decodeIfPresent(Int.self, forKey: .maxPositionEmbeddings) ?? 32768\n        self.ropeTheta = try container.decodeIfPresent(Float.self, forKey: .ropeTheta) ?? 10000.0\n        self.ropeTraditional =\n            try container.decodeIfPresent(Bool.self, forKey: .ropeTraditional) ?? false\n        self.ropeScaling = try container.decodeIfPresent(\n            [String: StringOrNumber].self, forKey: .ropeScaling)\n        self.tieWordEmbeddings =\n            try container.decodeIfPresent(Bool.self, forKey: .tieWordEmbeddings) ?? false\n        self.numNextnPredictLayers =\n            try container.decodeIfPresent(Int.self, forKey: .numNextnPredictLayers) ?? 2\n    }\n}\n\n// MARK: - LoRA\n\nextension MiMoModel: LoRAModel {\n    public func loraLinearLayers() -> LoRALinearLayers {\n        model.layers.map { ($0.attention, [\"q_proj\", \"v_proj\"]) }\n    }\n}"}]}
{"messages": [{"role": "user", "content": "convert the following MLX Python code into MLX Swift: \n\n# Copyright \u00a9 2023-2024 Apple Inc.\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional, Union\n\nimport mlx.core as mx\nimport mlx.nn as nn\n\nfrom .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention\n\n\n@dataclass\nclass ModelArgs(BaseModelArgs):\n    model_type: str\n    head_dim: int\n    num_transformer_layers: int\n    model_dim: int\n    vocab_size: int\n    ffn_dim_divisor: int\n    num_query_heads: List\n    num_kv_heads: List\n    ffn_multipliers: List\n    ffn_with_glu: bool = True\n    normalize_qk_projections: bool = True\n    share_input_output_layers: bool = True\n    rms_norm_eps: float = 1e-6\n    rope_freq_constant: float = 10000\n\n\ndef make_divisible(\n    v: Union[float, int],\n    divisor: Optional[int] = 8,\n    min_value: Optional[Union[float, int]] = None,\n) -> Union[float, int]:\n    \"\"\"\n    This function is taken from the original tf repo.\n    It ensures that all layers have a channel number that is divisible by the divisor\n    It can be seen at:\n    https://github.com/tensorflow/models/blob/2cfc99eff5e5eb729c6793d2f3d03aa1c9be2b15/research/slim/nets/mobilenet/mobilenet.py#L62\n    Args:\n        v: input value\n        divisor: default to 8\n        min_value: minimum divisor value\n    Returns:\n        new_v: new divisible value\n    \"\"\"\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < 0.9 * v:\n        new_v += divisor\n    return new_v\n\n\nclass Attention(nn.Module):\n    def __init__(self, args: ModelArgs, layer_id: int):\n        super().__init__()\n        self.head_dim = head_dim = args.head_dim\n        self.layer_id = layer_id\n        self.model_dim = model_dim = args.model_dim\n\n        self.n_heads = n_heads = args.num_query_heads[layer_id]\n        self.n_kv_heads = n_kv_heads = args.num_kv_heads[layer_id]\n        self.scale = head_dim**-0.5\n\n        op_size = (n_heads + (n_kv_heads * 2)) * head_dim\n        self.qkv_proj = nn.Linear(model_dim, op_size, bias=False)\n        self.out_proj = nn.Linear(n_heads * head_dim, model_dim, bias=False)\n\n        self.normalize_qk_projections = args.normalize_qk_projections\n\n        if self.normalize_qk_projections:\n            self.q_norm = nn.RMSNorm(head_dim, eps=args.rms_norm_eps)\n            self.k_norm = nn.RMSNorm(head_dim, eps=args.rms_norm_eps)\n\n        self.rope = nn.RoPE(head_dim, traditional=False, base=args.rope_freq_constant)\n\n    def __call__(\n        self,\n        x: mx.array,\n        mask: Optional[mx.array] = None,\n        cache: Optional[Any] = None,\n    ) -> mx.array:\n        B, L, D = x.shape\n\n        qkv = self.qkv_proj(x)\n\n        qkv = qkv.reshape(\n            B, L, self.n_heads + (self.n_kv_heads * 2), self.head_dim\n        ).transpose(0, 2, 1, 3)\n\n        queries, keys, values = mx.split(\n            qkv, [self.n_heads, self.n_heads + self.n_kv_heads], axis=1\n        )\n\n        # Prepare the queries, keys and values for the attention computation\n        if self.normalize_qk_projections:\n            queries = self.q_norm(queries)\n            keys = self.k_norm(keys)\n\n        if cache is not None:\n            queries = self.rope(queries, offset=cache.offset)\n            keys = self.rope(keys, offset=cache.offset)\n            keys, values = cache.update_and_fetch(keys, values)\n        else:\n            queries = self.rope(queries)\n            keys = self.rope(keys)\n\n        output = scaled_dot_product_attention(\n            queries, keys, values, cache=cache, scale=self.scale, mask=mask\n        )\n\n        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)\n\n        return self.out_proj(output)\n\n\nclass MLP(nn.Module):\n    def __init__(self, args: ModelArgs, layer_id: int):\n        super().__init__()\n        self.args = args\n        dim = args.model_dim\n        ffn_multiplier = args.ffn_multipliers[layer_id]\n\n        intermediate_dim = int(\n            make_divisible(\n                ffn_multiplier * args.model_dim,\n                divisor=args.ffn_dim_divisor,\n            )\n        )\n\n        self.proj_1 = nn.Linear(dim, 2 * intermediate_dim, bias=False)\n        self.proj_2 = nn.Linear(intermediate_dim, dim, bias=False)\n\n    def __call__(self, x) -> mx.array:\n        x = self.proj_1(x)\n        gate, x = mx.split(x, 2, axis=-1)\n        return self.proj_2(nn.silu(gate) * x)\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, args: ModelArgs, layer_id: int):\n        super().__init__()\n        dim = args.model_dim\n        self.attn = Attention(args, layer_id=layer_id)\n        self.ffn = MLP(args, layer_id=layer_id)\n        self.ffn_norm = nn.RMSNorm(dim, eps=args.rms_norm_eps)\n        self.attn_norm = nn.RMSNorm(dim, eps=args.rms_norm_eps)\n\n    def __call__(\n        self,\n        x: mx.array,\n        mask: Optional[mx.array] = None,\n        cache: Optional[Any] = None,\n    ) -> mx.array:\n        r = self.attn(self.attn_norm(x), mask, cache)\n        h = x + r\n        r = self.ffn(self.ffn_norm(h))\n        out = h + r\n        return out\n\n\nclass OpenELMModel(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.args = args\n        self.vocab_size = args.vocab_size\n        self.num_transformer_layers = args.num_transformer_layers\n        assert self.vocab_size > 0\n        self.token_embeddings = nn.Embedding(args.vocab_size, args.model_dim)\n        self.layers = [\n            TransformerBlock(args, layer_id=layer_id)\n            for layer_id in range(self.num_transformer_layers)\n        ]\n        self.norm = nn.RMSNorm(args.model_dim, eps=args.rms_norm_eps)\n\n    def __call__(\n        self,\n        inputs: mx.array,\n        mask: mx.array = None,\n        cache=None,\n    ):\n        h = self.token_embeddings(inputs)\n\n        if mask is None:\n            mask = create_attention_mask(h, cache)\n\n        if cache is None:\n            cache = [None] * len(self.layers)\n\n        for layer, c in zip(self.layers, cache):\n            h = layer(h, mask, cache=c)\n\n        return self.norm(h)\n\n\nclass Model(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.args = args\n        self.model_type = args.model_type\n        self.transformer = OpenELMModel(args)\n        if not args.share_input_output_layers:\n            self.lm_head = nn.Linear(args.model_dim, args.vocab_size, bias=False)\n\n    def __call__(\n        self,\n        inputs: mx.array,\n        mask: mx.array = None,\n        cache=None,\n    ):\n        out = self.transformer(inputs, mask, cache)\n        if self.args.share_input_output_layers:\n            out = self.transformer.token_embeddings.as_linear(out)\n        else:\n            out = self.lm_head(out)\n\n        return out\n\n    @property\n    def layers(self):\n        return self.transformer.layers"}, {"role": "assistant", "content": "//\n//  OpenELM.swift\n//  LLM\n//\n//  Created by Sachin Desai on 2024/4/27.\n//\n\nimport Foundation\nimport MLX\nimport MLXLMCommon\nimport MLXNN\n\nfunc computeHeads(modelDim: Int, headDim: Int) -> Int {\n    assert(modelDim % headDim == 0, \"modelDim must be divisible by headDim\")\n    return modelDim / headDim\n}\n\nfunc makeDivisible(_ v: Float, divisor: Int = 8, minValue: Float? = nil) -> Int {\n    let minVal = minValue ?? Float(divisor)\n    var roundDown = max(minVal, Float(Int((v + Float(divisor) / 2) / Float(divisor)) * divisor))\n\n    if roundDown < 0.9 * v {\n        roundDown += Float(divisor)\n    }\n    return Int(roundDown)\n}\n\nprivate class MultiHeadCausalAttention: Module {\n    let scale: Float\n    let heads: Int\n    let headDim: Int\n    let kvHeads: Int\n\n    @ModuleInfo(key: \"qkv_proj\") var qkvProj: Linear\n    @ModuleInfo(key: \"out_proj\") var outProj: Linear\n\n    @ModuleInfo(key: \"q_norm\") var qNorm: RMSNorm?\n    @ModuleInfo(key: \"k_norm\") var kNorm: RMSNorm?\n\n    let rope: RoPE\n\n    public init(_ args: OpenElmConfiguration, layerId: Int) {\n        self.headDim = args.headDimensions\n        let modelDim = args.modelDim\n\n        self.heads = args.numQueryHeads[layerId]\n        self.kvHeads = args.kvHeads[layerId]\n        self.scale = pow(Float(headDim), -0.5)\n\n        let opSize = (heads + (kvHeads * 2)) * headDim\n        self._qkvProj.wrappedValue = Linear(modelDim, opSize, bias: false)\n        self._outProj.wrappedValue = Linear(heads * headDim, modelDim, bias: false)\n\n        if args.normalizeQkProjections {\n            self._qNorm.wrappedValue = RMSNorm(dimensions: headDim, eps: args.rmsNormEps)\n            self._kNorm.wrappedValue = RMSNorm(dimensions: headDim, eps: args.rmsNormEps)\n        }\n\n        self.rope = RoPE(\n            dimensions: headDim, traditional: args.ropeTraditional, base: args.ropeTheta)\n    }\n\n    public func callAsFunction(\n        _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?\n    ) -> MLXArray {\n        let (B, L) = (x.dim(0), x.dim(1))\n        let qkv = qkvProj(x).reshaped(B, L, heads + (kvHeads * 2), headDim).transposed(0, 2, 1, 3)\n\n        let qkvSplit = split(qkv, indices: [heads, heads + kvHeads], axis: 1)\n        var queries = qkvSplit[0]\n        var keys = qkvSplit[1]\n        var values = qkvSplit[2]\n\n        if let qNorm, let kNorm {\n            queries = qNorm(queries)\n            keys = kNorm(keys)\n        }\n\n        if let cache {\n            queries = rope(queries, offset: cache.offset)\n            keys = rope(keys, offset: cache.offset)\n        } else {\n            queries = rope(queries)\n            keys = rope(keys)\n        }\n\n        let output = attentionWithCacheUpdate(\n            queries: queries,\n            keys: keys,\n            values: values,\n            cache: cache,\n            scale: scale,\n            mask: mask\n        )\n        .transposed(0, 2, 1, 3)\n        .reshaped(B, L, heads * headDim)\n\n        return outProj(output)\n    }\n}\n\nprivate class FeedForwardNetwork: Module, UnaryLayer {\n    @ModuleInfo var proj_1: Linear\n    @ModuleInfo var proj_2: Linear\n\n    public init(_ args: OpenElmConfiguration, layedId: Int) {\n        let dim = args.modelDim\n        let ffnMultiplier = args.ffnMultipliers[layedId]\n        let intermediateDim = Int(\n            makeDivisible(Float(ffnMultiplier) * Float(dim), divisor: args.ffnDimDivisor))\n\n        self.proj_1 = Linear(dim, 2 * intermediateDim, bias: false)\n        self.proj_2 = Linear(intermediateDim, dim, bias: false)\n    }\n\n    public func callAsFunction(_ x: MLXArray) -> MLXArray {\n        let a = proj_1(x)\n        let b = split(a, parts: 2, axis: -1)\n        let gate = b[0]\n        let x = b[1]\n        return proj_2(silu(gate) * x)\n    }\n}\n\nprivate class TransformerDecoderLayer: Module {\n    @ModuleInfo(key: \"attn\") var attn: MultiHeadCausalAttention\n    let ffn: FeedForwardNetwork\n\n    @ModuleInfo(key: \"ffn_norm\") var ffnNorm: RMSNorm\n    @ModuleInfo(key: \"attn_norm\") var attnNorm: RMSNorm\n\n    public init(_ args: OpenElmConfiguration, layerId: Int) {\n        let dim = args.modelDim\n        self._attn.wrappedValue = MultiHeadCausalAttention(args, layerId: layerId)\n        self.ffn = FeedForwardNetwork(args, layedId: layerId)\n        self._ffnNorm.wrappedValue = RMSNorm(dimensions: dim, eps: args.rmsNormEps)\n        self._attnNorm.wrappedValue = RMSNorm(dimensions: dim, eps: args.rmsNormEps)\n    }\n\n    public func callAsFunction(\n        _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?\n    ) -> MLXArray {\n        var r = attn(attnNorm(x), mask: mask, cache: cache)\n        let h = x + r\n        r = ffn(ffnNorm(h))\n        let out = h + r\n        return out\n    }\n}\n\nclass OpenELMModelInner: Module {\n    @ModuleInfo(key: \"token_embeddings\") var embedTokens: Embedding\n\n    fileprivate let layers: [TransformerDecoderLayer]\n    fileprivate let norm: RMSNorm\n\n    public init(_ args: OpenElmConfiguration) {\n        precondition(args.vocabularySize > 0)\n\n        self._embedTokens.wrappedValue = Embedding(\n            embeddingCount: args.vocabularySize, dimensions: args.modelDim)\n\n        self.layers = (0 ..< args.numTransformerLayers)\n            .map { layerId in\n                TransformerDecoderLayer(args, layerId: layerId)\n            }\n\n        self.norm = RMSNorm(dimensions: args.modelDim, eps: args.rmsNormEps)\n    }\n\n    public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]? = nil) -> MLXArray {\n        var h = embedTokens(inputs)\n        let mask = createAttentionMask(h: h, cache: cache)\n\n        for (i, layer) in layers.enumerated() {\n            h = layer(h, mask: mask, cache: cache?[i])\n        }\n\n        return norm(h)\n    }\n}\n\npublic class OpenELMModel: Module, LLMModel, KVCacheDimensionProvider {\n    public let vocabularySize: Int\n    public let kvHeads: [Int]\n\n    let transformer: OpenELMModelInner\n\n    @ModuleInfo(key: \"lm_head\") var lmHead: Linear?\n\n    public init(_ args: OpenElmConfiguration) {\n        self.vocabularySize = args.vocabularySize\n        self.kvHeads = args.kvHeads\n\n        self.transformer = OpenELMModelInner(args)\n        if !args.shareInputOutputLayers {\n            self._lmHead.wrappedValue = Linear(\n                args.numTransformerLayers, args.vocabularySize, bias: false)\n        }\n    }\n\n    public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]?) -> MLXArray {\n        var out = transformer(inputs, cache: cache)\n        if let lmHead {\n            out = lmHead(out)\n        } else {\n            out = transformer.embedTokens.asLinear(out)\n        }\n\n        return out\n    }\n}\n\npublic struct OpenElmConfiguration: Codable, Sendable {\n    var modelType: String\n    var headDimensions: Int\n    var numTransformerLayers: Int\n    var modelDim: Int\n    var vocabularySize: Int\n    var ffnDimDivisor: Int\n    var numQueryHeads: [Int] = []\n    var kvHeads: [Int] = []\n    var ffnWithGlu: Bool = true\n    var normalizeQkProjections: Bool = true\n    var shareInputOutputLayers: Bool = true\n    var rmsNormEps: Float = 1e-6\n    var ropeTheta: Float = 10_000\n    var ropeTraditional: Bool = false\n    var numGqaGroups: Int = 4\n    var ffnMultipliers: [Float] = [0.5, 4.0]\n    var qkvMultiplier: [Float] = [0.5, 1.0]\n\n    enum CodingKeys: String, CodingKey {\n        case modelType = \"model_type\"\n        case headDimensions = \"head_dim\"\n        case numTransformerLayers = \"num_transformer_layers\"\n        case modelDim = \"model_dim\"\n        case vocabularySize = \"vocab_size\"\n        case ffnDimDivisor = \"ffn_dim_divisor\"\n        case ffnMultipliers = \"ffn_multipliers\"\n        case ffnWithGlu = \"ffn_with_glu\"\n        case normalizeQkProjections = \"normalize_qk_projections\"\n        case shareInputOutputLayers = \"share_input_output_layers\"\n    }\n\n    public init(from decoder: Decoder) throws {\n        // custom implementation to handle optional keys with required values\n        let container: KeyedDecodingContainer<OpenElmConfiguration.CodingKeys> =\n            try decoder.container(\n                keyedBy: OpenElmConfiguration.CodingKeys.self)\n\n        self.modelType = try container.decode(\n            String.self, forKey: OpenElmConfiguration.CodingKeys.modelType)\n        self.headDimensions = try container.decode(\n            Int.self, forKey: OpenElmConfiguration.CodingKeys.headDimensions)\n        self.numTransformerLayers = try container.decode(\n            Int.self, forKey: OpenElmConfiguration.CodingKeys.numTransformerLayers)\n\n        self.modelDim = try container.decode(\n            Int.self, forKey: OpenElmConfiguration.CodingKeys.modelDim)\n        self.vocabularySize = try container.decode(\n            Int.self, forKey: OpenElmConfiguration.CodingKeys.vocabularySize)\n        self.ffnDimDivisor = try container.decode(\n            Int.self, forKey: OpenElmConfiguration.CodingKeys.ffnDimDivisor)\n\n        let qkvMultipliers = stride(\n            from: qkvMultiplier[0], through: qkvMultiplier[1],\n            by: (qkvMultiplier[1] - qkvMultiplier[0]) / Float(numTransformerLayers - 1)\n        )\n        .map { round($0 * 100) / 100 }\n\n        let headMultipleOf = numGqaGroups\n        let queryDims = qkvMultipliers.map { a in\n            makeDivisible(Float(self.modelDim) * a, divisor: self.headDimensions * headMultipleOf)\n        }\n\n        self.numQueryHeads = queryDims.map { qDim in\n            Int(computeHeads(modelDim: qDim, headDim: self.headDimensions))\n        }\n\n        self.kvHeads = self.numQueryHeads.map { qHeads in\n            qHeads / numGqaGroups\n        }\n\n        self.ffnMultipliers = stride(\n            from: ffnMultipliers[0], through: ffnMultipliers[1],\n            by: (ffnMultipliers[1] - ffnMultipliers[0]) / Float(numTransformerLayers - 1)\n        )\n        .map { round($0 * 100) / 100 }\n\n        self.ffnWithGlu =\n            try container.decodeIfPresent(\n                Bool.self, forKey: OpenElmConfiguration.CodingKeys.ffnWithGlu) ?? true\n        self.normalizeQkProjections =\n            try container.decodeIfPresent(\n                Bool.self, forKey: OpenElmConfiguration.CodingKeys.normalizeQkProjections) ?? true\n        self.shareInputOutputLayers =\n            try container.decodeIfPresent(\n                Bool.self, forKey: OpenElmConfiguration.CodingKeys.shareInputOutputLayers) ?? true\n    }\n}\n\n// MARK: - LoRA\n\nextension OpenELMModel: LoRAModel {\n    public func loraLinearLayers() -> LoRALinearLayers {\n        transformer.layers.map { ($0.attn, [\"qkv_proj\"]) }\n    }\n}"}]}
{"messages": [{"role": "user", "content": "convert the following MLX Python code into MLX Swift: \n\n# Copyright \u00a9 2023-2024 Apple Inc.\n\nimport math\nfrom dataclasses import dataclass\n\nimport mlx.core as mx\nimport mlx.nn as nn\n\nfrom .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention\n\n\n@dataclass\nclass ModelArgs(BaseModelArgs):\n    model_type: str = \"phi\"\n    max_position_embeddings: int = 2048\n    vocab_size: int = 51200\n    hidden_size: int = 2560\n    num_attention_heads: int = 32\n    num_hidden_layers: int = 32\n    num_key_value_heads: int = 32\n    partial_rotary_factor: float = 0.4\n    intermediate_size: int = 10240\n    layer_norm_eps: float = 1e-5\n    rope_theta: float = 10000.0\n\n    def __post_init__(self):\n        if self.num_key_value_heads is None:\n            self.num_key_value_heads = self.num_attention_heads\n\n\nclass PhiAttention(nn.Module):\n    def __init__(self, config: ModelArgs):\n        super().__init__()\n\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.repeats = self.num_heads // self.num_key_value_heads\n        self.rope_theta = config.rope_theta\n        self.partial_rotary_factor = config.partial_rotary_factor\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n\n        self.q_proj = nn.Linear(\n            self.hidden_size, self.num_heads * self.head_dim, bias=True\n        )\n        self.k_proj = nn.Linear(\n            self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True\n        )\n        self.v_proj = nn.Linear(\n            self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True\n        )\n        self.dense = nn.Linear(\n            self.num_heads * self.head_dim, self.hidden_size, bias=True\n        )\n\n        self.rope = nn.RoPE(\n            int(self.partial_rotary_factor * self.head_dim),\n            traditional=False,\n            base=self.rope_theta,\n        )\n\n    def __call__(self, x, mask=None, cache=None):\n        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)\n\n        # Extract some shapes\n        B, L, D = queries.shape\n        n_heads, n_kv_heads = self.num_heads, self.num_key_value_heads\n\n        # Prepare the queries, keys and values for the attention computation\n        queries = queries.reshape(\n            B,\n            L,\n            n_heads,\n            -1,\n        ).moveaxis(1, 2)\n        keys = keys.reshape(B, L, n_kv_heads, -1).moveaxis(1, 2)\n        values = values.reshape(B, L, n_kv_heads, -1).moveaxis(1, 2)\n\n        # Add RoPE to the queries and keys and combine them with the cache\n        if cache is not None:\n            queries = self.rope(queries, offset=cache.offset)\n            keys = self.rope(keys, offset=cache.offset)\n            keys, values = cache.update_and_fetch(keys, values)\n        else:\n            queries = self.rope(queries)\n            keys = self.rope(keys)\n\n        scale = math.sqrt(1 / queries.shape[-1])\n        output = scaled_dot_product_attention(\n            queries.astype(mx.float32),\n            keys,\n            values,\n            cache=cache,\n            scale=scale,\n            mask=mask,\n        ).astype(values.dtype)\n\n        output = output.moveaxis(2, 1).reshape(B, L, -1)\n\n        return self.dense(output)\n\n\nclass PhiMLP(nn.Module):\n    def __init__(self, config: ModelArgs):\n        super().__init__()\n        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n\n    def __call__(self, x) -> mx.array:\n        return self.fc2(nn.gelu_approx(self.fc1(x)))\n\n\nclass PhiDecoderLayer(nn.Module):\n    def __init__(self, config: ModelArgs):\n        super().__init__()\n        self.self_attn = PhiAttention(config=config)\n        self.input_layernorm = nn.LayerNorm(\n            config.hidden_size, eps=config.layer_norm_eps\n        )\n        self.mlp = PhiMLP(config)\n\n    def __call__(self, x, mask, cache):\n        h = self.input_layernorm(x)\n        attn_h = self.self_attn(h, mask, cache)\n        ff_h = self.mlp(h)\n        return attn_h + ff_h + x\n\n\nclass PhiModel(nn.Module):\n    def __init__(self, config: ModelArgs):\n        super().__init__()\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n        self.layers = [PhiDecoderLayer(config) for i in range(config.num_hidden_layers)]\n        self.final_layernorm = nn.LayerNorm(\n            config.hidden_size, eps=config.layer_norm_eps\n        )\n\n    def __call__(self, x, mask, cache):\n        x = self.embed_tokens(x)\n\n        if mask is None:\n            mask = create_attention_mask(x, cache)\n\n        if cache is None:\n            cache = [None] * len(self.layers)\n\n        for layer, c in zip(self.layers, cache):\n            x = layer(x, mask, c)\n        return self.final_layernorm(x)\n\n\nclass Model(nn.Module):\n    def __init__(self, config: ModelArgs):\n        super().__init__()\n        self.model_type = config.model_type\n        self.model = PhiModel(config)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=True)\n        self.args = config\n\n    def __call__(\n        self,\n        x: mx.array,\n        mask: mx.array = None,\n        cache=None,\n    ) -> mx.array:\n        y = self.model(x, mask, cache)\n        return self.lm_head(y)\n\n    @property\n    def layers(self):\n        return self.model.layers"}, {"role": "assistant", "content": "// Copyright \u00a9 2024 Apple Inc.\n\nimport Foundation\nimport MLX\nimport MLXLMCommon\nimport MLXNN\n\n// https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/models/phi.py\n\nprivate class PhiAttention: Module {\n\n    let args: PhiConfiguration\n    let heads: Int\n    let headDim: Int\n\n    @ModuleInfo(key: \"q_proj\") var wq: Linear\n    @ModuleInfo(key: \"k_proj\") var wk: Linear\n    @ModuleInfo(key: \"v_proj\") var wv: Linear\n    @ModuleInfo(key: \"dense\") var dense: Linear\n\n    let rope: RoPE\n\n    public init(_ args: PhiConfiguration) {\n        self.args = args\n\n        let hiddenSize = args.hiddenSize\n        self.heads = args.attentionHeads\n        self.headDim = args.hiddenSize / heads\n        let kvHeads = args.kvHeads\n\n        if headDim * heads != hiddenSize {\n            fatalError(\"hidden_size must be divisible by num_heads\")\n        }\n\n        self._wq.wrappedValue = Linear(hiddenSize, heads * headDim, bias: true)\n        self._wk.wrappedValue = Linear(hiddenSize, kvHeads * headDim, bias: true)\n        self._wv.wrappedValue = Linear(hiddenSize, kvHeads * headDim, bias: true)\n        self._dense.wrappedValue = Linear(heads * headDim, hiddenSize, bias: true)\n\n        self.rope = RoPE(\n            dimensions: Int(args.partialRotaryFactor * Float(headDim)), traditional: false,\n            base: args.ropeTheta)\n    }\n\n    public func callAsFunction(\n        _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?\n    ) -> MLXArray {\n        let (B, L) = (x.dim(0), x.dim(1))\n\n        var queries = wq(x)\n        var keys = wk(x)\n        var values = wv(x)\n\n        // prepare the queries, keys and values for the attention computation\n        queries = queries.reshaped(B, L, heads, headDim).transposed(0, 2, 1, 3)\n        keys = keys.reshaped(B, L, args.kvHeads, headDim).transposed(0, 2, 1, 3)\n        values = values.reshaped(B, L, args.kvHeads, headDim).transposed(0, 2, 1, 3)\n\n        // Add RoPE to the queries and keys and combine them with the cache\n        if let cache {\n            queries = rope(queries, offset: cache.offset)\n            keys = rope(keys, offset: cache.offset)\n        } else {\n            queries = rope(queries)\n            keys = rope(keys)\n        }\n\n        // Finally perform the attention computation\n        let scale = sqrt(1 / Float(queries.dim(-1)))\n        let output = attentionWithCacheUpdate(\n            queries: queries.asType(.float32),\n            keys: keys,\n            values: values,\n            cache: cache,\n            scale: scale,\n            mask: mask\n        )\n        .asType(values.dtype)\n        .transposed(0, 2, 1, 3)\n        .reshaped(B, L, -1)\n\n        return dense(output)\n    }\n}\n\nprivate class PhiMLP: Module, UnaryLayer {\n\n    @ModuleInfo var fc1: Linear\n    @ModuleInfo var fc2: Linear\n    @ModuleInfo var act: GELU\n\n    public init(_ config: PhiConfiguration) {\n        self.fc1 = Linear(config.hiddenSize, config.intermediateSize)\n        self.fc2 = Linear(config.intermediateSize, config.hiddenSize)\n        self.act = GELU(approximation: .precise)\n    }\n\n    public func callAsFunction(_ x: MLXArray) -> MLXArray {\n        fc2(act(fc1(x)))\n    }\n}\n\nprivate class PhiDecoderLayer: Module {\n\n    @ModuleInfo(key: \"self_attn\") var selfAttention: PhiAttention\n    @ModuleInfo(key: \"input_layernorm\") var inputLayerNorm: LayerNorm\n    var mlp: PhiMLP\n\n    public init(_ config: PhiConfiguration) {\n        self._selfAttention.wrappedValue = PhiAttention(config)\n        self._inputLayerNorm.wrappedValue = LayerNorm(\n            dimensions: config.hiddenSize, eps: config.layerNormEps)\n        self.mlp = PhiMLP(config)\n    }\n\n    public func callAsFunction(\n        _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?\n    ) -> MLXArray {\n        let h = inputLayerNorm(x)\n        let attentionH = selfAttention(h, mask: mask, cache: cache)\n        let ffH = mlp(h)\n        return attentionH + ffH + x\n    }\n}\n\nprivate class PhiModelInner: Module {\n\n    @ModuleInfo(key: \"embed_tokens\") var embedTokens: Embedding\n\n    @ModuleInfo var layers: [PhiDecoderLayer]\n    @ModuleInfo(key: \"final_layernorm\") var finalLayerNorm: LayerNorm\n\n    public init(_ args: PhiConfiguration) {\n        self._embedTokens.wrappedValue = Embedding(\n            embeddingCount: args.vocabularySize, dimensions: args.hiddenSize)\n\n        self.layers = (0 ..< args.hiddenLayers)\n            .map { _ in\n                PhiDecoderLayer(args)\n            }\n        self._finalLayerNorm.wrappedValue = LayerNorm(\n            dimensions: args.hiddenSize, eps: args.layerNormEps)\n    }\n\n    public func callAsFunction(\n        _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: [KVCache]? = nil\n    ) -> MLXArray {\n        var x = embedTokens(x)\n\n        for (i, layer) in layers.enumerated() {\n            x = layer(x, mask: mask, cache: cache?[i])\n        }\n\n        return finalLayerNorm(x)\n    }\n}\n\npublic class PhiModel: Module, LLMModel, KVCacheDimensionProvider {\n\n    public let vocabularySize: Int\n    public let kvHeads: [Int]\n\n    fileprivate let model: PhiModelInner\n\n    @ModuleInfo(key: \"lm_head\") var lmHead: Linear\n\n    public init(_ args: PhiConfiguration) {\n        self.vocabularySize = args.vocabularySize\n        self.kvHeads = (0 ..< args.hiddenLayers).map { _ in args.kvHeads }\n        self.model = PhiModelInner(args)\n        self._lmHead.wrappedValue = Linear(args.hiddenSize, args.vocabularySize, bias: true)\n    }\n\n    public func callAsFunction(_ x: MLXArray, cache: [KVCache]?) -> MLXArray {\n        let mask = createAttentionMask(h: x, cache: cache)\n\n        let y = model(x, mask: mask, cache: cache)\n        return lmHead(y)\n    }\n}\n\npublic struct PhiConfiguration: Codable, Sendable {\n    var maxPositionalEmbeddings = 2048\n    var vocabularySize = 51200\n    var hiddenSize = 2560\n    var attentionHeads = 32\n    var hiddenLayers = 32\n    var kvHeads = 32\n    var partialRotaryFactor: Float = 0.4\n    var intermediateSize = 10240\n    var layerNormEps: Float = 1e-5\n    var ropeTheta: Float = 10_000\n\n    enum CodingKeys: String, CodingKey {\n        case maxPositionalEmbeddings = \"max_position_embeddings\"\n        case vocabularySize = \"vocab_size\"\n        case hiddenSize = \"hidden_size\"\n        case attentionHeads = \"num_attention_heads\"\n        case hiddenLayers = \"num_hidden_layers\"\n        case kvHeads = \"num_key_value_heads\"\n        case partialRotaryFactor = \"partial_rotary_factor\"\n        case intermediateSize = \"intermediate_size\"\n        case layerNormEps = \"layer_norm_eps\"\n        case ropeTheta = \"rope_theta\"\n    }\n\n    public init(from decoder: Decoder) throws {\n        let container: KeyedDecodingContainer<PhiConfiguration.CodingKeys> = try decoder.container(\n            keyedBy: PhiConfiguration.CodingKeys.self)\n\n        self.maxPositionalEmbeddings = try container.decode(\n            Int.self, forKey: PhiConfiguration.CodingKeys.maxPositionalEmbeddings)\n        self.vocabularySize = try container.decode(\n            Int.self, forKey: PhiConfiguration.CodingKeys.vocabularySize)\n        self.hiddenSize = try container.decode(\n            Int.self, forKey: PhiConfiguration.CodingKeys.hiddenSize)\n        self.attentionHeads = try container.decode(\n            Int.self, forKey: PhiConfiguration.CodingKeys.attentionHeads)\n        self.hiddenLayers = try container.decode(\n            Int.self, forKey: PhiConfiguration.CodingKeys.hiddenLayers)\n        self.kvHeads =\n            try container.decodeIfPresent(Int.self, forKey: PhiConfiguration.CodingKeys.kvHeads)\n            ?? attentionHeads\n        self.partialRotaryFactor = try container.decode(\n            Float.self, forKey: PhiConfiguration.CodingKeys.partialRotaryFactor)\n        self.intermediateSize = try container.decode(\n            Int.self, forKey: PhiConfiguration.CodingKeys.intermediateSize)\n        self.layerNormEps = try container.decode(\n            Float.self, forKey: PhiConfiguration.CodingKeys.layerNormEps)\n        self.ropeTheta =\n            try container.decodeIfPresent(Float.self, forKey: PhiConfiguration.CodingKeys.ropeTheta)\n            ?? 10_000\n\n    }\n}\n\n// MARK: - LoRA\n\nextension PhiModel: LoRAModel {\n    public func loraLinearLayers() -> LoRALinearLayers {\n        model.layers.map { ($0.selfAttention, [\"q_proj\", \"v_proj\"]) }\n    }\n}"}]}
{"messages": [{"role": "user", "content": "convert the following MLX Python code into MLX Swift: \n\n# Copyright \u00a9 2023-2024 Apple Inc.\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional, Tuple, Union\n\nimport mlx.core as mx\nimport mlx.nn as nn\n\nfrom .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention\nfrom .rope_utils import SuScaledRoPE\n\n\n@dataclass\nclass ModelArgs(BaseModelArgs):\n    model_type: str\n    hidden_size: int\n    num_hidden_layers: int\n    intermediate_size: int\n    num_attention_heads: int\n    rms_norm_eps: float\n    vocab_size: int\n    num_key_value_heads: Optional[int] = None\n    rope_theta: float = 10000\n    rope_traditional: bool = False\n    rope_scaling: Optional[Dict[str, Union[float, List[float]]]] = None\n    partial_rotary_factor: float = 1.0\n    max_position_embeddings: int = 131072\n    original_max_position_embeddings: int = 4096\n    tie_word_embeddings: bool = False\n\n    def __post_init__(self):\n        if self.num_key_value_heads is None:\n            self.num_key_value_heads = self.num_attention_heads\n\n        if self.rope_scaling:\n            required_keys = {\"long_factor\", \"type\"}\n            if not all(key in self.rope_scaling for key in required_keys):\n                raise ValueError(f\"rope_scaling must contain keys {required_keys}\")\n\n            if self.rope_scaling[\"type\"] not in [\"longrope\", \"su\", \"linear\"]:\n                print(\n                    \"[WARNING] rope_scaling 'type' currently only supports 'linear', 'su', and 'longrope'; setting rope scaling to false.\"\n                )\n                self.rope_scaling = None\n\n\nclass Attention(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n\n        dim = args.hidden_size\n        self.n_heads = n_heads = args.num_attention_heads\n        assert args.num_key_value_heads is not None\n        self.n_kv_heads = n_kv_heads = args.num_key_value_heads\n        self.num_hidden_layers = args.num_hidden_layers\n\n        self.head_dim = head_dim = args.hidden_size // n_heads\n        self.scale = head_dim**-0.5\n\n        op_size = n_heads * head_dim + 2 * (n_kv_heads * head_dim)\n        self.qkv_proj = nn.Linear(dim, op_size, bias=False)\n        self.o_proj = nn.Linear(n_heads * head_dim, dim, bias=False)\n\n        rope_dim = int(head_dim * args.partial_rotary_factor)\n        if args.rope_scaling and args.rope_scaling[\"type\"] in [\"longrope\", \"su\"]:\n            self.rope = SuScaledRoPE(\n                rope_dim,\n                base=args.rope_theta,\n                max_position_embeddings=args.max_position_embeddings,\n                original_max_position_embeddings=args.original_max_position_embeddings,\n                short_factor=args.rope_scaling[\"short_factor\"],\n                long_factor=args.rope_scaling[\"long_factor\"],\n            )\n        else:\n            rope_scale = 1.0\n            if args.rope_scaling and args.rope_scaling[\"type\"] == \"linear\":\n                assert isinstance(args.rope_scaling[\"factor\"], float)\n                rope_scale = 1 / args.rope_scaling[\"factor\"]\n            self.rope = nn.RoPE(\n                rope_dim,\n                traditional=args.rope_traditional,\n                base=args.rope_theta,\n                scale=rope_scale,\n            )\n\n    def __call__(\n        self,\n        x: mx.array,\n        mask: Optional[mx.array] = None,\n        cache: Optional[Any] = None,\n    ) -> mx.array:\n        B, L, D = x.shape\n\n        qkv = self.qkv_proj(x)\n        query_pos = self.n_heads * self.head_dim\n        queries, keys, values = mx.split(\n            qkv, [query_pos, query_pos + self.n_kv_heads * self.head_dim], axis=-1\n        )\n\n        # Prepare the queries, keys and values for the attention computation\n        queries = queries.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)\n        keys = keys.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)\n        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)\n\n        if cache is not None:\n            queries = self.rope(queries, offset=cache.offset)\n            keys = self.rope(keys, offset=cache.offset)\n            keys, values = cache.update_and_fetch(keys, values)\n        else:\n            queries = self.rope(queries)\n            keys = self.rope(keys)\n\n        output = scaled_dot_product_attention(\n            queries, keys, values, cache=cache, scale=self.scale, mask=mask\n        )\n        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)\n        return self.o_proj(output)\n\n\nclass MLP(nn.Module):\n    def __init__(self, dim, hidden_dim):\n        super().__init__()\n        self.gate_up_proj = nn.Linear(dim, 2 * hidden_dim, bias=False)\n        self.down_proj = nn.Linear(hidden_dim, dim, bias=False)\n\n    def __call__(self, x) -> mx.array:\n        x = self.gate_up_proj(x)\n        gate, x = mx.split(x, 2, axis=-1)\n        return self.down_proj(nn.silu(gate) * x)\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.num_attention_heads = args.num_attention_heads\n        self.hidden_size = args.hidden_size\n        self.self_attn = Attention(args)\n        self.mlp = MLP(args.hidden_size, args.intermediate_size)\n        self.input_layernorm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)\n        self.post_attention_layernorm = nn.RMSNorm(\n            args.hidden_size, eps=args.rms_norm_eps\n        )\n        self.args = args\n\n    def __call__(\n        self,\n        x: mx.array,\n        mask: Optional[mx.array] = None,\n        cache: Optional[Any] = None,\n    ) -> mx.array:\n        r = self.self_attn(self.input_layernorm(x), mask, cache)\n        h = x + r\n        r = self.mlp(self.post_attention_layernorm(h))\n        out = h + r\n        return out\n\n\nclass Phi3Model(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.args = args\n        self.vocab_size = args.vocab_size\n        self.num_hidden_layers = args.num_hidden_layers\n        assert self.vocab_size > 0\n        self.embed_tokens = nn.Embedding(args.vocab_size, args.hidden_size)\n        self.layers = [\n            TransformerBlock(args=args) for _ in range(args.num_hidden_layers)\n        ]\n        self.norm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)\n\n    def __call__(\n        self,\n        inputs: mx.array,\n        mask: mx.array = None,\n        cache=None,\n    ):\n        h = self.embed_tokens(inputs)\n\n        if mask is None:\n            mask = create_attention_mask(h, cache)\n\n        if cache is None:\n            cache = [None] * len(self.layers)\n\n        for layer, c in zip(self.layers, cache):\n            h = layer(h, mask, c)\n\n        return self.norm(h)\n\n\nclass Model(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.model_type = args.model_type\n        self.model = Phi3Model(args)\n        if not args.tie_word_embeddings:\n            self.lm_head = nn.Linear(args.hidden_size, args.vocab_size, bias=False)\n        self.args = args\n\n    def __call__(\n        self,\n        inputs: mx.array,\n        mask: mx.array = None,\n        cache=None,\n    ):\n        out = self.model(inputs, mask, cache)\n        if self.args.tie_word_embeddings:\n            out = self.model.embed_tokens.as_linear(out)\n        else:\n            out = self.lm_head(out)\n        return out\n\n    @property\n    def layers(self):\n        return self.model.layers"}, {"role": "assistant", "content": "// Copyright \u00a9 2024 Apple Inc.\n\nimport Foundation\nimport MLX\nimport MLXLMCommon\nimport MLXNN\n\nprivate class Attention: Module {\n\n    let args: Phi3Configuration\n    let scale: Float\n\n    let heads: Int\n    let kvHeads: Int\n    let headDim: Int\n    let ropeDim: Int\n\n    @ModuleInfo(key: \"qkv_proj\") var wqkv: Linear\n    @ModuleInfo(key: \"o_proj\") var wo: Linear\n\n    enum PositionalEncoding {\n        case rope(RoPE)\n        case suScaledRotaryEmbedding(SuScaledRotaryEmbedding)\n\n        func applyEncoding(_ x: MLXArray, offset: Int = 0) -> MLXArray {\n            switch self {\n            case .rope(let rope):\n                return rope.callAsFunction(x, offset: offset)\n            case .suScaledRotaryEmbedding(let suScaledRotaryEmbedding):\n                return suScaledRotaryEmbedding.callAsFunction(x, offset: offset)\n            }\n        }\n    }\n\n    let rope: PositionalEncoding\n\n    public init(_ args: Phi3Configuration) {\n        self.args = args\n\n        let dim = args.hiddenSize\n        self.heads = args.attentionHeads\n        self.kvHeads = args.kvHeads\n\n        self.headDim = args.hiddenSize / heads\n        self.ropeDim = Int(Float(headDim) * args.partialRotaryFactor)\n        self.scale = pow(Float(headDim), -0.5)\n\n        self._wqkv.wrappedValue = Linear(dim, (heads + 2 * kvHeads) * headDim, bias: false)\n        self._wo.wrappedValue = Linear(heads * headDim, dim, bias: false)\n\n        let ropeScale: Float\n\n        if let ropeScaling = args.ropeScaling, ropeScaling.type == \"linear\",\n            let factor = ropeScaling.factor\n        {\n            ropeScale = 1 / factor\n        } else {\n            ropeScale = 1\n        }\n\n        if let ropeScaling = args.ropeScaling,\n            ropeScaling.type == \"su\" || ropeScaling.type == \"longrope\",\n            let shortFactor = ropeScaling.shortFactor, let longFactor = ropeScaling.longFactor\n        {\n            self.rope = .suScaledRotaryEmbedding(\n                SuScaledRotaryEmbedding(\n                    dimensions: ropeDim, base: args.ropeTheta,\n                    maxPositionEmbeddings: args.maxPositionEmbeddings,\n                    originalMaxPositionEmbeddings: args.originalMaxPositionEmbeddings,\n                    longFactor: longFactor))\n\n        } else {\n            self.rope = .rope(\n                RoPE(\n                    dimensions: ropeDim, traditional: args.ropeTraditional, base: args.ropeTheta,\n                    scale: ropeScale))\n        }\n    }\n\n    public func callAsFunction(\n        _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?\n    ) -> MLXArray {\n        let (B, L) = (x.dim(0), x.dim(1))\n\n        let queryPos = heads * headDim\n        let qkv = split(wqkv(x), indices: [queryPos, queryPos + kvHeads * headDim], axis: -1)\n        var queries = qkv[0]\n        var keys = qkv[1]\n        var values = qkv[2]\n\n        // prepare the queries, keys and values for the attention computation\n        queries = queries.reshaped(B, L, args.attentionHeads, -1).transposed(0, 2, 1, 3)\n        keys = keys.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)\n        values = values.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)\n\n        if let cache {\n            queries = rope.applyEncoding(queries, offset: cache.offset)\n            keys = rope.applyEncoding(keys, offset: cache.offset)\n        } else {\n            queries = rope.applyEncoding(queries)\n            keys = rope.applyEncoding(keys)\n        }\n\n        let output = attentionWithCacheUpdate(\n            queries: queries,\n            keys: keys,\n            values: values,\n            cache: cache,\n            scale: scale,\n            mask: mask\n        )\n        .transposed(0, 2, 1, 3)\n        .reshaped(B, L, -1)\n\n        return wo(output)\n    }\n}\n\nprivate class MLP: Module, UnaryLayer {\n\n    @ModuleInfo(key: \"gate_up_proj\") var gate_up: Linear\n    @ModuleInfo(key: \"down_proj\") var down: Linear\n\n    public init(dimensions: Int, hiddenDimensions: Int) {\n        self._gate_up.wrappedValue = Linear(dimensions, 2 * hiddenDimensions, bias: false)\n        self._down.wrappedValue = Linear(hiddenDimensions, dimensions, bias: false)\n    }\n\n    public func callAsFunction(_ x: MLXArray) -> MLXArray {\n        let gu = split(gate_up(x), parts: 2, axis: -1)\n        return down(silu(gu[0]) * gu[1])\n    }\n}\n\nprivate class TransformerBlock: Module {\n\n    @ModuleInfo(key: \"self_attn\") var attention: Attention\n    let mlp: MLP\n\n    @ModuleInfo(key: \"input_layernorm\") var inputLayerNorm: RMSNorm\n    @ModuleInfo(key: \"post_attention_layernorm\") var postAttentionLayerNorm: RMSNorm\n\n    public init(_ args: Phi3Configuration) {\n        self._attention.wrappedValue = Attention(args)\n        self.mlp = MLP(dimensions: args.hiddenSize, hiddenDimensions: args.intermediateSize)\n        self._inputLayerNorm.wrappedValue = RMSNorm(\n            dimensions: args.hiddenSize, eps: args.rmsNormEps)\n        self._postAttentionLayerNorm.wrappedValue = RMSNorm(\n            dimensions: args.hiddenSize, eps: args.rmsNormEps)\n    }\n\n    public func callAsFunction(\n        _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?\n    ) -> MLXArray {\n        var r = attention(inputLayerNorm(x), mask: mask, cache: cache)\n        let h = x + r\n        r = mlp(postAttentionLayerNorm(h))\n        let out = h + r\n        return out\n    }\n}\n\nprivate class Phi3ModelInner: Module {\n\n    @ModuleInfo(key: \"embed_tokens\") var embedTokens: Embedding\n\n    fileprivate let layers: [TransformerBlock]\n    let norm: RMSNorm\n    let args: Phi3Configuration\n\n    public init(_ args: Phi3Configuration) {\n        precondition(args.vocabularySize > 0)\n        self.args = args\n\n        self._embedTokens.wrappedValue = Embedding(\n            embeddingCount: args.vocabularySize, dimensions: args.hiddenSize)\n\n        self.layers = (0 ..< args.hiddenLayers)\n            .map { _ in\n                TransformerBlock(args)\n            }\n        self.norm = RMSNorm(dimensions: args.hiddenSize, eps: args.rmsNormEps)\n    }\n\n    public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]?) -> MLXArray {\n        var h = embedTokens(inputs)\n\n        let mask = createAttentionMask(h: h, cache: cache)\n\n        for (i, layer) in layers.enumerated() {\n            h = layer(h, mask: mask, cache: cache?[i])\n        }\n\n        return norm(h)\n    }\n}\n\npublic class Phi3Model: Module, LLMModel, KVCacheDimensionProvider {\n\n    public let vocabularySize: Int\n    public let kvHeads: [Int]\n\n    private let model: Phi3ModelInner\n    private let args: Phi3Configuration\n\n    @ModuleInfo(key: \"lm_head\") var lmHead: Linear?\n\n    public init(_ args: Phi3Configuration) {\n        self.vocabularySize = args.vocabularySize\n        self.kvHeads = (0 ..< args.hiddenLayers).map { _ in args.kvHeads }\n        self.model = Phi3ModelInner(args)\n        self.args = args\n\n        if !args.tieWordEmbeddings {\n            self._lmHead.wrappedValue = Linear(args.hiddenSize, args.vocabularySize, bias: false)\n        }\n    }\n\n    public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]?) -> MLXArray {\n        let out = model(inputs, cache: cache)\n        if args.tieWordEmbeddings {\n            return model.embedTokens.asLinear(out)\n        } else if let lmHead {\n            return lmHead(out)\n        } else {\n            fatalError(\n                \"Model configuration error: Neither tied embeddings nor lm_head is available\")\n        }\n    }\n}\n\nstruct RopeScalingWithFactorArrays: Codable {\n    let longFactor: [Float]?\n    let shortFactor: [Float]?\n    let factor: Float?\n    let type: String?\n    let longMScale: Float?\n    let shortMScale: Float?\n\n    enum CodingKeys: String, CodingKey {\n        case type\n        case factor\n        case longFactor = \"long_factor\"\n        case shortFactor = \"short_factor\"\n        case longMScale = \"long_mscale\"\n        case shortMScale = \"short_mscale\"\n    }\n}\n\npublic struct Phi3Configuration: Codable, Sendable {\n    var hiddenSize: Int\n    var hiddenLayers: Int\n    var intermediateSize: Int\n    var attentionHeads: Int\n    var rmsNormEps: Float\n    var vocabularySize: Int\n    var kvHeads: Int\n    var ropeTheta: Float = 10_000\n    var ropeTraditional: Bool = false\n    var ropeScaling: RopeScalingWithFactorArrays?\n    var partialRotaryFactor: Float = 1.0\n    var maxPositionEmbeddings: Int\n    var originalMaxPositionEmbeddings: Int\n    var tieWordEmbeddings: Bool = false\n\n    enum CodingKeys: String, CodingKey {\n        case hiddenSize = \"hidden_size\"\n        case hiddenLayers = \"num_hidden_layers\"\n        case intermediateSize = \"intermediate_size\"\n        case attentionHeads = \"num_attention_heads\"\n        case rmsNormEps = \"rms_norm_eps\"\n        case vocabularySize = \"vocab_size\"\n        case kvHeads = \"num_key_value_heads\"\n        case ropeTheta = \"rope_theta\"\n        case ropeTraditional = \"rope_traditional\"\n        case ropeScaling = \"rope_scaling\"\n        case partialRotaryFactor = \"partial_rotary_factor\"\n        case maxPositionEmbeddings = \"max_position_embeddings\"\n        case originalMaxPositionEmbeddings = \"original_max_position_embeddings\"\n        case tieWordEmbeddings = \"tie_word_embeddings\"\n    }\n\n    public init(from decoder: Decoder) throws {\n        // custom implementation to handle optional keys with required values\n        let container: KeyedDecodingContainer<Phi3Configuration.CodingKeys> = try decoder.container(\n            keyedBy: Phi3Configuration.CodingKeys.self)\n\n        hiddenSize = try container.decode(Int.self, forKey: Phi3Configuration.CodingKeys.hiddenSize)\n        hiddenLayers = try container.decode(\n            Int.self, forKey: Phi3Configuration.CodingKeys.hiddenLayers)\n        intermediateSize = try container.decode(\n            Int.self, forKey: Phi3Configuration.CodingKeys.intermediateSize)\n        attentionHeads = try container.decode(\n            Int.self, forKey: Phi3Configuration.CodingKeys.attentionHeads)\n        rmsNormEps = try container.decode(\n            Float.self, forKey: Phi3Configuration.CodingKeys.rmsNormEps)\n        vocabularySize = try container.decode(\n            Int.self, forKey: Phi3Configuration.CodingKeys.vocabularySize)\n        kvHeads = try container.decode(Int.self, forKey: Phi3Configuration.CodingKeys.kvHeads)\n        ropeTheta =\n            try container.decodeIfPresent(\n                Float.self, forKey: Phi3Configuration.CodingKeys.ropeTheta) ?? 10_000\n        ropeTraditional =\n            try container.decodeIfPresent(\n                Bool.self, forKey: Phi3Configuration.CodingKeys.ropeTraditional) ?? false\n        ropeScaling = try container.decodeIfPresent(\n            RopeScalingWithFactorArrays.self, forKey: .ropeScaling)\n        partialRotaryFactor =\n            try container.decodeIfPresent(\n                Float.self, forKey: .partialRotaryFactor) ?? 1.0\n        maxPositionEmbeddings = try container.decode(Int.self, forKey: .maxPositionEmbeddings)\n        originalMaxPositionEmbeddings = try container.decode(\n            Int.self, forKey: .originalMaxPositionEmbeddings)\n        tieWordEmbeddings =\n            try container.decodeIfPresent(\n                Bool.self, forKey: .tieWordEmbeddings) ?? false\n    }\n}\n\n// MARK: - LoRA\n\nextension Phi3Model: LoRAModel {\n    public func loraLinearLayers() -> LoRALinearLayers {\n        model.layers.map { ($0.attention, [\"qkv_proj\"]) }\n    }\n}"}]}
{"messages": [{"role": "user", "content": "convert the following MLX Python code into MLX Swift: \n\n# Copyright \u00a9 2024 Apple Inc.\nimport math\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional, Union\n\nimport mlx.core as mx\nimport mlx.nn as nn\n\nfrom .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention\nfrom .rope_utils import SuScaledRoPE\nfrom .switch_layers import SwitchGLU\n\n\n@dataclass\nclass ModelArgs(BaseModelArgs):\n    model_type: str = \"phimoe\"\n    vocab_size: int = 32064\n    hidden_size: int = 4096\n    intermediate_size: int = 6400\n    num_hidden_layers: int = 32\n    num_attention_heads: int = 32\n    num_key_value_heads: int = 8\n    max_position_embeddings: int = 131072\n    original_max_position_embeddings: int = 4096\n    rms_norm_eps: float = 1e-6\n    rope_scaling: Dict[str, Union[float, List[float]]] = None\n    num_local_experts: int = 16\n    num_experts_per_tok: int = 2\n    rope_theta: float = 10000.0\n\n\nclass Attention(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n\n        dim = args.hidden_size\n        self.n_heads = n_heads = args.num_attention_heads\n        self.n_kv_heads = n_kv_heads = args.num_key_value_heads\n\n        head_dim = args.hidden_size // n_heads\n        self.scale = head_dim**-0.5\n\n        self.q_proj = nn.Linear(dim, n_heads * head_dim, bias=True)\n        self.k_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=True)\n        self.v_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=True)\n        self.o_proj = nn.Linear(n_heads * head_dim, dim, bias=True)\n\n        self.rope = SuScaledRoPE(\n            head_dim,\n            base=args.rope_theta,\n            max_position_embeddings=args.max_position_embeddings,\n            original_max_position_embeddings=args.original_max_position_embeddings,\n            short_factor=args.rope_scaling[\"short_factor\"],\n            long_factor=args.rope_scaling[\"long_factor\"],\n            short_mscale=args.rope_scaling[\"short_mscale\"],\n            long_mscale=args.rope_scaling[\"long_mscale\"],\n        )\n\n    def __call__(\n        self,\n        x: mx.array,\n        mask: Optional[mx.array] = None,\n        cache=None,\n    ) -> mx.array:\n        B, L, D = x.shape\n\n        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)\n\n        # Prepare the queries, keys and values for the attention computation\n        queries = queries.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)\n        keys = keys.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)\n        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)\n\n        if cache is not None:\n            queries = self.rope(queries, offset=cache.offset)\n            keys = self.rope(keys, offset=cache.offset)\n            keys, values = cache.update_and_fetch(keys, values)\n        else:\n            queries = self.rope(queries)\n            keys = self.rope(keys)\n\n        output = scaled_dot_product_attention(\n            queries, keys, values, cache=cache, scale=self.scale, mask=mask\n        )\n        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)\n        return self.o_proj(output)\n\n\nclass PhiMoESparseMoeBlock(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.hidden_dim = args.hidden_size\n        self.ffn_dim = args.intermediate_size\n        self.num_experts = args.num_local_experts\n        self.top_k = args.num_experts_per_tok\n\n        self.gate = nn.Linear(self.hidden_dim, self.num_experts, bias=False)\n        self.switch_mlp = SwitchGLU(self.hidden_dim, self.ffn_dim, self.num_experts)\n\n    def __call__(self, x: mx.array) -> mx.array:\n        gates = self.gate(x)\n\n        k = self.top_k\n        inds = mx.stop_gradient(mx.argpartition(-gates, kth=k - 1, axis=-1)[..., :k])\n        scores = mx.take_along_axis(gates, inds, axis=-1)\n        scores = mx.softmax(scores, axis=-1, precise=True)\n\n        y = self.switch_mlp(x, inds)\n        y = (y * scores[..., None]).sum(axis=-2)\n\n        return y\n\n\nclass PhiMoEDecoderLayer(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.hidden_size = args.hidden_size\n\n        self.self_attn = Attention(args)\n        self.block_sparse_moe = PhiMoESparseMoeBlock(args)\n        self.input_layernorm = nn.LayerNorm(args.hidden_size, eps=args.rms_norm_eps)\n        self.post_attention_layernorm = nn.LayerNorm(\n            args.hidden_size, eps=args.rms_norm_eps\n        )\n\n    def __call__(\n        self,\n        x: mx.array,\n        mask: Optional[mx.array] = None,\n        cache=None,\n    ) -> mx.array:\n        residual = x\n        hidden_states = self.input_layernorm(x)\n        hidden_states = self.self_attn(hidden_states, mask=mask, cache=cache)\n        hidden_states = residual + hidden_states\n\n        residual = hidden_states\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states = self.block_sparse_moe(hidden_states)\n        hidden_states = residual + hidden_states\n\n        return hidden_states\n\n\nclass PhiMoEModel(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.args = args\n        self.vocab_size = args.vocab_size\n\n        self.embed_tokens = nn.Embedding(args.vocab_size, args.hidden_size)\n        self.layers = [PhiMoEDecoderLayer(args) for _ in range(args.num_hidden_layers)]\n        self.norm = nn.LayerNorm(args.hidden_size, eps=args.rms_norm_eps)\n\n    def __call__(\n        self,\n        inputs: mx.array,\n        mask: mx.array = None,\n        cache=None,\n    ) -> mx.array:\n        h = self.embed_tokens(inputs)\n\n        if mask is None:\n            mask = create_attention_mask(h, cache)\n\n        if cache is None:\n            cache = [None] * len(self.layers)\n\n        for layer, c in zip(self.layers, cache):\n            h = layer(h, mask, c)\n\n        return self.norm(h)\n\n\nclass Model(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.model_type = args.model_type\n        self.args = args\n        self.model = PhiMoEModel(args)\n        self.lm_head = nn.Linear(args.hidden_size, args.vocab_size, bias=True)\n\n    def __call__(\n        self,\n        inputs: mx.array,\n        mask: mx.array = None,\n        cache=None,\n    ):\n        out = self.model(inputs, mask, cache)\n        return self.lm_head(out)\n\n    def sanitize(self, weights):\n        if \"model.layers.0.block_sparse_moe.experts.0.w1.weight\" not in weights:\n            return weights\n        for l in range(self.args.num_hidden_layers):\n            prefix = f\"model.layers.{l}\"\n            for n, m in [(\"w1\", \"gate_proj\"), (\"w2\", \"down_proj\"), (\"w3\", \"up_proj\")]:\n                for k in [\"weight\", \"scales\", \"biases\"]:\n                    if f\"{prefix}.block_sparse_moe.experts.0.{n}.{k}\" in weights:\n                        to_join = [\n                            weights.pop(\n                                f\"{prefix}.block_sparse_moe.experts.{e}.{n}.{k}\"\n                            )\n                            for e in range(self.args.num_local_experts)\n                        ]\n                        weights[f\"{prefix}.block_sparse_moe.switch_mlp.{m}.{k}\"] = (\n                            mx.stack(to_join)\n                        )\n\n        return weights\n\n    @property\n    def layers(self):\n        return self.model.layers"}, {"role": "assistant", "content": "import Foundation\nimport MLX\nimport MLXLMCommon\nimport MLXNN\n\n// Port of https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/models/phimoe.py\n\npublic struct PhiMoEConfiguration: Codable, Sendable {\n    var modelType: String = \"phimoe\"\n    var vocabularySize: Int = 32064\n    var hiddenSize: Int = 4096\n    var intermediateSize: Int = 6400\n    var hiddenLayers: Int = 32\n    var attentionHeads: Int = 32\n    var kvHeads: Int = 8\n    var maxPositionEmbeddings: Int = 131072\n    var originalMaxPositionEmbeddings: Int = 4096\n    var rmsNormEps: Float = 1e-6\n    var ropeScaling: RopeScalingWithFactorArrays?\n    var numLocalExperts: Int = 16\n    var numExpertsPerToken: Int = 2\n    var ropeTheta: Float = 10000.0\n\n    enum CodingKeys: String, CodingKey {\n        case modelType = \"model_type\"\n        case vocabularySize = \"vocab_size\"\n        case hiddenSize = \"hidden_size\"\n        case intermediateSize = \"intermediate_size\"\n        case hiddenLayers = \"num_hidden_layers\"\n        case attentionHeads = \"num_attention_heads\"\n        case kvHeads = \"num_key_value_heads\"\n        case maxPositionEmbeddings = \"max_position_embeddings\"\n        case originalMaxPositionEmbeddings = \"original_max_position_embeddings\"\n        case rmsNormEps = \"rms_norm_eps\"\n        case ropeScaling = \"rope_scaling\"\n        case numLocalExperts = \"num_local_experts\"\n        case numExpertsPerToken = \"num_experts_per_tok\"\n        case ropeTheta = \"rope_theta\"\n    }\n}\n\nprivate class Attention: Module {\n    let args: PhiMoEConfiguration\n    let scale: Float\n\n    @ModuleInfo(key: \"q_proj\") var wq: Linear\n    @ModuleInfo(key: \"k_proj\") var wk: Linear\n    @ModuleInfo(key: \"v_proj\") var wv: Linear\n    @ModuleInfo(key: \"o_proj\") var wo: Linear\n\n    let rope: SuScaledRotaryEmbedding\n\n    init(_ args: PhiMoEConfiguration) {\n        self.args = args\n\n        let dim = args.hiddenSize\n        let heads = args.attentionHeads\n        let kvHeads = args.kvHeads\n\n        let headDim = args.hiddenSize / heads\n        self.scale = pow(Float(headDim), -0.5)\n\n        self._wq.wrappedValue = Linear(dim, heads * headDim, bias: true)\n        self._wk.wrappedValue = Linear(dim, kvHeads * headDim, bias: true)\n        self._wv.wrappedValue = Linear(dim, kvHeads * headDim, bias: true)\n        self._wo.wrappedValue = Linear(heads * headDim, dim, bias: true)\n\n        self.rope = SuScaledRotaryEmbedding(\n            dimensions: headDim,\n            base: args.ropeTheta,\n            maxPositionEmbeddings: args.maxPositionEmbeddings,\n            originalMaxPositionEmbeddings: args.originalMaxPositionEmbeddings,\n            longFactor: args.ropeScaling?.longFactor as? [Float] ?? [1.0],\n            longMScale: args.ropeScaling?.longMScale as? Float\n        )\n    }\n\n    func callAsFunction(\n        _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?\n    ) -> MLXArray {\n        let (B, L, _) = (x.dim(0), x.dim(1), x.dim(2))\n\n        let queries = wq(x)\n        let keys = wk(x)\n        let values = wv(x)\n\n        // Prepare the queries, keys and values for the attention computation\n        var q = queries.reshaped(B, L, args.attentionHeads, -1).transposed(0, 2, 1, 3)\n        var k = keys.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)\n        var v = values.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)\n\n        if let cache {\n            q = rope(q, offset: cache.offset)\n            k = rope(k, offset: cache.offset)\n        } else {\n            q = rope(q)\n            k = rope(k)\n        }\n\n        let output = attentionWithCacheUpdate(\n            queries: q,\n            keys: k,\n            values: v,\n            cache: cache,\n            scale: scale,\n            mask: mask\n        )\n        .transposed(0, 2, 1, 3)\n        .reshaped(B, L, -1)\n\n        return wo(output)\n    }\n}\n\nprivate class PhiMoESparseMoeBlock: Module {\n    let hiddenDim: Int\n    let ffnDim: Int\n    let numExperts: Int\n    let topK: Int\n\n    @ModuleInfo(key: \"gate\") var gate: Linear\n    @ModuleInfo(key: \"switch_mlp\") var switchMLP: SwitchGLU\n\n    init(_ args: PhiMoEConfiguration) {\n        self.hiddenDim = args.hiddenSize\n        self.ffnDim = args.intermediateSize\n        self.numExperts = args.numLocalExperts\n        self.topK = args.numExpertsPerToken\n\n        self._gate.wrappedValue = Linear(hiddenDim, numExperts, bias: false)\n        self._switchMLP.wrappedValue = SwitchGLU(\n            inputDims: hiddenDim, hiddenDims: ffnDim, numExperts: numExperts)\n    }\n\n    func callAsFunction(_ x: MLXArray) -> MLXArray {\n        let gates = gate(x)\n\n        let k = self.topK\n        let inds = MLX.stopGradient(\n            MLX.argPartition(\n                -gates,\n                kth: k - 1,\n                axis: -1\n            )[.ellipsis, ..<k])\n        let scores = MLX.softmax(MLX.takeAlong(gates, inds, axis: -1), axis: -1, precise: true)\n\n        let y = switchMLP(x, inds)\n        return (y * scores[.ellipsis, .newAxis]).sum(axis: -2)\n    }\n}\n\nprivate class PhiMoEDecoderLayer: Module {\n    let hiddenSize: Int\n\n    @ModuleInfo(key: \"self_attn\") var selfAttn: Attention\n    @ModuleInfo(key: \"block_sparse_moe\") var blockSparseMoe: PhiMoESparseMoeBlock\n    @ModuleInfo(key: \"input_layernorm\") var inputLayerNorm: LayerNorm\n    @ModuleInfo(key: \"post_attention_layernorm\") var postAttentionLayerNorm: LayerNorm\n\n    init(_ args: PhiMoEConfiguration) {\n        self.hiddenSize = args.hiddenSize\n\n        self._selfAttn.wrappedValue = Attention(args)\n        self._blockSparseMoe.wrappedValue = PhiMoESparseMoeBlock(args)\n        self._inputLayerNorm.wrappedValue = LayerNorm(\n            dimensions: args.hiddenSize, eps: args.rmsNormEps)\n        self._postAttentionLayerNorm.wrappedValue = LayerNorm(\n            dimensions: args.hiddenSize, eps: args.rmsNormEps)\n    }\n\n    func callAsFunction(\n        _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?\n    ) -> MLXArray {\n        var residual = x\n        var hiddenStates = inputLayerNorm(x)\n        hiddenStates = selfAttn(hiddenStates, mask: mask, cache: cache)\n        hiddenStates = residual + hiddenStates\n\n        residual = hiddenStates\n        hiddenStates = postAttentionLayerNorm(hiddenStates)\n        hiddenStates = blockSparseMoe(hiddenStates)\n        hiddenStates = residual + hiddenStates\n\n        return hiddenStates\n    }\n}\n\nprivate class PhiMoEModelInner: Module {\n    let args: PhiMoEConfiguration\n\n    @ModuleInfo(key: \"embed_tokens\") var embedTokens: Embedding\n    let layers: [PhiMoEDecoderLayer]\n    @ModuleInfo(key: \"norm\") var norm: LayerNorm\n\n    init(_ args: PhiMoEConfiguration) {\n        self.args = args\n\n        self._embedTokens.wrappedValue = Embedding(\n            embeddingCount: args.vocabularySize, dimensions: args.hiddenSize)\n        self.layers = (0 ..< args.hiddenLayers).map { _ in PhiMoEDecoderLayer(args) }\n        self._norm.wrappedValue = LayerNorm(dimensions: args.hiddenSize, eps: args.rmsNormEps)\n    }\n\n    func callAsFunction(_ inputs: MLXArray, cache: [KVCache]?) -> MLXArray {\n        var h = embedTokens(inputs)\n\n        let mask = createAttentionMask(h: h, cache: cache)\n\n        for (i, layer) in layers.enumerated() {\n            h = layer(h, mask: mask, cache: cache?[i])\n        }\n\n        return norm(h)\n    }\n}\n\npublic class PhiMoEModel: Module, LLMModel, KVCacheDimensionProvider {\n    public let vocabularySize: Int\n    public let kvHeads: [Int]\n\n    fileprivate let model: PhiMoEModelInner\n    @ModuleInfo(key: \"lm_head\") var lmHead: Linear\n\n    public init(_ args: PhiMoEConfiguration) {\n        self.vocabularySize = args.vocabularySize\n        self.kvHeads = Array(repeating: args.kvHeads, count: args.hiddenLayers)\n        self.model = PhiMoEModelInner(args)\n        self._lmHead.wrappedValue = Linear(args.hiddenSize, args.vocabularySize, bias: true)\n    }\n\n    public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]?) -> MLXArray {\n        let out = model(inputs, cache: cache)\n        return lmHead(out)\n    }\n\n    public func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {\n        var sanitizedWeights = weights\n        if sanitizedWeights[\"model.layers.0.block_sparse_moe.experts.0.w1.weight\"] == nil {\n            return sanitizedWeights\n        }\n\n        for l in 0 ..< model.args.hiddenLayers {\n            let prefix = \"model.layers.\\(l)\"\n            for (n, m) in [(\"w1\", \"gate_proj\"), (\"w2\", \"down_proj\"), (\"w3\", \"up_proj\")] {\n                for k in [\"weight\", \"scales\", \"biases\"] {\n                    if sanitizedWeights[\"\\(prefix).block_sparse_moe.experts.0.\\(n).\\(k)\"] != nil {\n                        let toJoin = (0 ..< model.args.numLocalExperts).map { e in\n                            sanitizedWeights.removeValue(\n                                forKey: \"\\(prefix).block_sparse_moe.experts.\\(e).\\(n).\\(k)\")!\n                        }\n                        sanitizedWeights[\"\\(prefix).block_sparse_moe.switch_mlp.\\(m).\\(k)\"] =\n                            MLX.stacked(toJoin)\n                    }\n                }\n            }\n        }\n\n        return sanitizedWeights\n    }\n}\n\n// MARK: - LoRA\n\nextension PhiMoEModel: LoRAModel {\n    public func loraLinearLayers() -> LoRALinearLayers {\n        model.layers.map { ($0.selfAttn, [\"q_proj\", \"v_proj\"]) }\n    }\n}"}]}
{"messages": [{"role": "user", "content": "convert the following MLX Python code into MLX Swift: \n\n# Copyright \u00a9 2023-2024 Apple Inc.\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, Optional, Union\n\nimport mlx.core as mx\nimport mlx.nn as nn\n\nfrom .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention\nfrom .rope_utils import initialize_rope\n\n\n@dataclass\nclass ModelArgs(BaseModelArgs):\n    model_type: str\n    hidden_size: int\n    num_hidden_layers: int\n    intermediate_size: int\n    num_attention_heads: int\n    rms_norm_eps: float\n    vocab_size: int\n    num_key_value_heads: int\n    max_position_embeddings: int = 32768\n    rope_theta: float = 1000000\n    rope_traditional: bool = False\n    rope_scaling: Optional[Dict[str, Union[float, str]]] = None\n    tie_word_embeddings: bool = True\n\n\nclass Attention(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n\n        dim = args.hidden_size\n        self.n_heads = n_heads = args.num_attention_heads\n        assert args.num_key_value_heads is not None\n        self.n_kv_heads = n_kv_heads = args.num_key_value_heads\n\n        head_dim = args.hidden_size // n_heads\n        self.scale = head_dim**-0.5\n\n        self.q_proj = nn.Linear(dim, n_heads * head_dim, bias=True)\n        self.k_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=True)\n        self.v_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=True)\n        self.o_proj = nn.Linear(n_heads * head_dim, dim, bias=False)\n\n        self.rope = initialize_rope(\n            head_dim,\n            base=args.rope_theta,\n            traditional=args.rope_traditional,\n            scaling_config=args.rope_scaling,\n            max_position_embeddings=args.max_position_embeddings,\n        )\n\n    def __call__(\n        self,\n        x: mx.array,\n        mask: Optional[mx.array] = None,\n        cache: Optional[Any] = None,\n    ) -> mx.array:\n        B, L, D = x.shape\n\n        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)\n\n        # Prepare the queries, keys and values for the attention computation\n        queries = queries.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)\n        keys = keys.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)\n        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)\n\n        if cache is not None:\n            queries = self.rope(queries, offset=cache.offset)\n            keys = self.rope(keys, offset=cache.offset)\n            keys, values = cache.update_and_fetch(keys, values)\n        else:\n            queries = self.rope(queries)\n            keys = self.rope(keys)\n\n        output = scaled_dot_product_attention(\n            queries, keys, values, cache=cache, scale=self.scale, mask=mask\n        )\n        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)\n        return self.o_proj(output)\n\n\nclass MLP(nn.Module):\n    def __init__(self, dim, hidden_dim):\n        super().__init__()\n        self.gate_proj = nn.Linear(dim, hidden_dim, bias=False)\n        self.down_proj = nn.Linear(hidden_dim, dim, bias=False)\n        self.up_proj = nn.Linear(dim, hidden_dim, bias=False)\n\n    def __call__(self, x) -> mx.array:\n        return self.down_proj(nn.silu(self.gate_proj(x)) * self.up_proj(x))\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.num_attention_heads = args.num_attention_heads\n        self.hidden_size = args.hidden_size\n        self.self_attn = Attention(args)\n        self.mlp = MLP(args.hidden_size, args.intermediate_size)\n        self.input_layernorm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)\n        self.post_attention_layernorm = nn.RMSNorm(\n            args.hidden_size, eps=args.rms_norm_eps\n        )\n        self.args = args\n\n    def __call__(\n        self,\n        x: mx.array,\n        mask: Optional[mx.array] = None,\n        cache: Optional[Any] = None,\n    ) -> mx.array:\n        r = self.self_attn(self.input_layernorm(x), mask, cache)\n        h = x + r\n        r = self.mlp(self.post_attention_layernorm(h))\n        out = h + r\n        return out\n\n\nclass Qwen2Model(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.args = args\n        self.vocab_size = args.vocab_size\n        self.num_hidden_layers = args.num_hidden_layers\n        assert self.vocab_size > 0\n        self.embed_tokens = nn.Embedding(args.vocab_size, args.hidden_size)\n        self.layers = [\n            TransformerBlock(args=args) for _ in range(args.num_hidden_layers)\n        ]\n        self.norm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)\n\n    def __call__(\n        self,\n        inputs: mx.array,\n        mask: mx.array = None,\n        cache=None,\n        input_embeddings: Optional[mx.array] = None,\n    ):\n        if input_embeddings is not None:\n            h = input_embeddings\n        else:\n            h = self.embed_tokens(inputs)\n\n        if mask is None:\n            mask = create_attention_mask(h, cache)\n\n        if cache is None:\n            cache = [None] * len(self.layers)\n\n        for layer, c in zip(self.layers, cache):\n            h = layer(h, mask, c)\n\n        return self.norm(h)\n\n\nclass Model(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.args = args\n        self.model_type = args.model_type\n        self.model = Qwen2Model(args)\n        if not args.tie_word_embeddings:\n            self.lm_head = nn.Linear(args.hidden_size, args.vocab_size, bias=False)\n\n    def __call__(\n        self,\n        inputs: mx.array,\n        mask: mx.array = None,\n        cache=None,\n        input_embeddings: Optional[mx.array] = None,\n    ):\n        out = self.model(inputs, mask, cache, input_embeddings)\n        if self.args.tie_word_embeddings:\n            out = self.model.embed_tokens.as_linear(out)\n        else:\n            out = self.lm_head(out)\n        return out\n\n    def sanitize(self, weights):\n        if self.args.tie_word_embeddings:\n            weights.pop(\"lm_head.weight\", None)\n        # Remove unused precomputed rotary freqs\n        return {\n            k: v for k, v in weights.items() if \"self_attn.rotary_emb.inv_freq\" not in k\n        }\n\n    @property\n    def layers(self):\n        return self.model.layers"}, {"role": "assistant", "content": "//\n//  Qwen2.swift\n//  LLM\n//\n//  Created by John Mai on 2024/3/3.\n//\n\nimport Foundation\nimport MLX\nimport MLXLMCommon\nimport MLXNN\n\n// port of https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/models/qwen2.py\n\nprivate class Attention: Module {\n    let args: Qwen2Configuration\n    let scale: Float\n\n    @ModuleInfo(key: \"q_proj\") var wq: Linear\n    @ModuleInfo(key: \"k_proj\") var wk: Linear\n    @ModuleInfo(key: \"v_proj\") var wv: Linear\n    @ModuleInfo(key: \"o_proj\") var wo: Linear\n\n    let rope: RoPE\n\n    public init(_ args: Qwen2Configuration) {\n        self.args = args\n\n        let dim = args.hiddenSize\n        let heads = args.attentionHeads\n        let kvHeads = args.kvHeads\n\n        let headDim = args.hiddenSize / heads\n        self.scale = pow(Float(headDim), -0.5)\n\n        _wq.wrappedValue = Linear(dim, heads * headDim, bias: true)\n        _wk.wrappedValue = Linear(dim, kvHeads * headDim, bias: true)\n        _wv.wrappedValue = Linear(dim, kvHeads * headDim, bias: true)\n        _wo.wrappedValue = Linear(heads * headDim, dim, bias: false)\n\n        let ropeScale: Float\n        if let ropeScaling = args.ropeScaling, ropeScaling[\"type\"] == .string(\"linear\"),\n            let factor = ropeScaling[\"factor\"]\n        {\n            if let v = factor.asFloat() {\n                ropeScale = 1 / v\n            } else {\n                fatalError(\"ropeScaling.factor must be a float\")\n            }\n        } else {\n            ropeScale = 1\n        }\n\n        self.rope = RoPE(\n            dimensions: headDim, traditional: args.ropeTraditional, base: args.ropeTheta,\n            scale: ropeScale)\n    }\n\n    public func callAsFunction(\n        _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?\n    ) -> MLXArray {\n        let (B, L) = (x.dim(0), x.dim(1))\n\n        var queries = wq(x)\n        var keys = wk(x)\n        var values = wv(x)\n\n        // prepare the queries, keys and values for the attention computation\n        queries = queries.reshaped(B, L, args.attentionHeads, -1).transposed(0, 2, 1, 3)\n        keys = keys.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)\n        values = values.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)\n\n        if let cache {\n            queries = rope(queries, offset: cache.offset)\n            keys = rope(keys, offset: cache.offset)\n        } else {\n            queries = rope(queries)\n            keys = rope(keys)\n        }\n\n        let output = attentionWithCacheUpdate(\n            queries: queries,\n            keys: keys,\n            values: values,\n            cache: cache,\n            scale: scale,\n            mask: mask\n        )\n        .transposed(0, 2, 1, 3)\n        .reshaped(B, L, -1)\n\n        return wo(output)\n    }\n}\n\nprivate class MLP: Module, UnaryLayer {\n    @ModuleInfo(key: \"gate_proj\") var gate: Linear\n    @ModuleInfo(key: \"down_proj\") var down: Linear\n    @ModuleInfo(key: \"up_proj\") var up: Linear\n\n    public init(dimensions: Int, hiddenDimensions: Int) {\n        _gate.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)\n        _down.wrappedValue = Linear(hiddenDimensions, dimensions, bias: false)\n        _up.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)\n    }\n\n    public func callAsFunction(_ x: MLXArray) -> MLXArray {\n        down(silu(gate(x)) * up(x))\n    }\n}\n\nprivate class TransformerBlock: Module {\n    @ModuleInfo(key: \"self_attn\") var attention: Attention\n    let mlp: MLP\n\n    @ModuleInfo(key: \"input_layernorm\") var inputLayerNorm: RMSNorm\n    @ModuleInfo(key: \"post_attention_layernorm\") var postAttentionLayerNorm: RMSNorm\n\n    public init(_ args: Qwen2Configuration) {\n        _attention.wrappedValue = Attention(args)\n        self.mlp = MLP(dimensions: args.hiddenSize, hiddenDimensions: args.intermediateSize)\n        _inputLayerNorm.wrappedValue = RMSNorm(\n            dimensions: args.hiddenSize, eps: args.rmsNormEps)\n        _postAttentionLayerNorm.wrappedValue = RMSNorm(\n            dimensions: args.hiddenSize, eps: args.rmsNormEps)\n    }\n\n    public func callAsFunction(\n        _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?\n    ) -> MLXArray {\n        var r = attention(inputLayerNorm(x), mask: mask, cache: cache)\n        let h = x + r\n        r = mlp(postAttentionLayerNorm(h))\n        let out = h + r\n        return out\n    }\n}\n\nprivate class Qwen2ModelInner: Module {\n    @ModuleInfo(key: \"embed_tokens\") var embedTokens: Embedding\n\n    fileprivate let layers: [TransformerBlock]\n    let norm: RMSNorm\n\n    public init(_ args: Qwen2Configuration) {\n        precondition(args.vocabularySize > 0)\n\n        _embedTokens.wrappedValue = Embedding(\n            embeddingCount: args.vocabularySize, dimensions: args.hiddenSize)\n\n        self.layers = (0 ..< args.hiddenLayers)\n            .map { _ in\n                TransformerBlock(args)\n            }\n        self.norm = RMSNorm(dimensions: args.hiddenSize, eps: args.rmsNormEps)\n    }\n\n    public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]? = nil) -> MLXArray {\n        var h = embedTokens(inputs)\n\n        let mask = createAttentionMask(h: h, cache: cache)\n\n        for (i, layer) in layers.enumerated() {\n            h = layer(h, mask: mask, cache: cache?[i])\n        }\n\n        return norm(h)\n    }\n}\n\npublic class Qwen2Model: Module, LLMModel, KVCacheDimensionProvider {\n    public let vocabularySize: Int\n    public let kvHeads: [Int]\n\n    private let model: Qwen2ModelInner\n    let configuration: Qwen2Configuration\n\n    @ModuleInfo(key: \"lm_head\") var lmHead: Linear?\n\n    public init(_ args: Qwen2Configuration) {\n        self.configuration = args\n        self.vocabularySize = args.vocabularySize\n        self.kvHeads = (0 ..< args.hiddenLayers).map { _ in args.kvHeads }\n        self.model = Qwen2ModelInner(args)\n\n        if !args.tieWordEmbeddings {\n            _lmHead.wrappedValue = Linear(args.hiddenSize, args.vocabularySize, bias: false)\n        }\n    }\n\n    public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]?) -> MLXArray {\n        var out = model(inputs, cache: cache)\n        if let lmHead {\n            out = lmHead(out)\n        } else {\n            out = model.embedTokens.asLinear(out)\n        }\n        return out\n    }\n\n    public func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {\n        var weights = weights\n\n        if configuration.tieWordEmbeddings {\n            weights[\"lm_head.weight\"] = nil\n        }\n\n        // Remove unused precomputed rotary freqs\n        return weights.filter {\n            !$0.key.contains(\"self_attn.rotary_emb.inv_freq\")\n        }\n    }\n}\n\npublic struct Qwen2Configuration: Codable, Sendable {\n    var hiddenSize: Int\n    var hiddenLayers: Int\n    var intermediateSize: Int\n    var attentionHeads: Int\n    var rmsNormEps: Float\n    var vocabularySize: Int\n    var kvHeads: Int\n    var ropeTheta: Float = 1_000_000\n    var ropeTraditional: Bool = false\n    var ropeScaling: [String: StringOrNumber]? = nil\n    var tieWordEmbeddings = false\n\n    enum CodingKeys: String, CodingKey {\n        case hiddenSize = \"hidden_size\"\n        case hiddenLayers = \"num_hidden_layers\"\n        case intermediateSize = \"intermediate_size\"\n        case attentionHeads = \"num_attention_heads\"\n        case rmsNormEps = \"rms_norm_eps\"\n        case vocabularySize = \"vocab_size\"\n        case kvHeads = \"num_key_value_heads\"\n        case ropeTheta = \"rope_theta\"\n        case ropeTraditional = \"rope_traditional\"\n        case ropeScaling = \"rope_scaling\"\n        case tieWordEmbeddings = \"tie_word_embeddings\"\n    }\n\n    public init(from decoder: Decoder) throws {\n        // custom implementation to handle optional keys with required values\n        let container: KeyedDecodingContainer<Qwen2Configuration.CodingKeys> =\n            try decoder.container(\n                keyedBy: Qwen2Configuration.CodingKeys.self)\n\n        self.hiddenSize = try container.decode(\n            Int.self, forKey: Qwen2Configuration.CodingKeys.hiddenSize)\n        self.hiddenLayers = try container.decode(\n            Int.self, forKey: Qwen2Configuration.CodingKeys.hiddenLayers)\n        self.intermediateSize = try container.decode(\n            Int.self, forKey: Qwen2Configuration.CodingKeys.intermediateSize)\n        self.attentionHeads = try container.decode(\n            Int.self, forKey: Qwen2Configuration.CodingKeys.attentionHeads)\n        self.rmsNormEps = try container.decode(\n            Float.self, forKey: Qwen2Configuration.CodingKeys.rmsNormEps)\n        self.vocabularySize = try container.decode(\n            Int.self, forKey: Qwen2Configuration.CodingKeys.vocabularySize)\n        self.kvHeads = try container.decode(Int.self, forKey: Qwen2Configuration.CodingKeys.kvHeads)\n        self.ropeTheta =\n            try container.decodeIfPresent(\n                Float.self, forKey: Qwen2Configuration.CodingKeys.ropeTheta)\n            ?? 1_000_000\n        self.ropeTraditional =\n            try container.decodeIfPresent(\n                Bool.self, forKey: Qwen2Configuration.CodingKeys.ropeTraditional) ?? false\n        self.ropeScaling = try container.decodeIfPresent(\n            [String: StringOrNumber].self, forKey: Qwen2Configuration.CodingKeys.ropeScaling)\n        self.tieWordEmbeddings =\n            try container.decodeIfPresent(Bool.self, forKey: .tieWordEmbeddings) ?? false\n    }\n}\n\n// MARK: - LoRA\n\nextension Qwen2Model: LoRAModel {\n    public func loraLinearLayers() -> LoRALinearLayers {\n        model.layers.map { ($0.attention, [\"q_proj\", \"v_proj\"]) }\n    }\n}"}]}
{"messages": [{"role": "user", "content": "convert the following MLX Python code into MLX Swift: \n\n# Copyright \u00a9 2023-2024 Apple Inc.\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, Optional, Union\n\nimport mlx.core as mx\nimport mlx.nn as nn\n\nfrom .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention\nfrom .rope_utils import initialize_rope\n\n\n@dataclass\nclass ModelArgs(BaseModelArgs):\n    model_type: str\n    hidden_size: int\n    num_hidden_layers: int\n    intermediate_size: int\n    num_attention_heads: int\n    rms_norm_eps: float\n    vocab_size: int\n    num_key_value_heads: int\n    max_position_embeddings: int\n    rope_theta: float\n    head_dim: int\n    tie_word_embeddings: bool\n    rope_scaling: Optional[Dict[str, Union[float, str]]] = None\n\n\nclass Attention(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n\n        dim = args.hidden_size\n        self.n_heads = n_heads = args.num_attention_heads\n        assert args.num_key_value_heads is not None\n        self.n_kv_heads = n_kv_heads = args.num_key_value_heads\n\n        head_dim = args.head_dim\n        self.scale = head_dim**-0.5\n\n        self.q_proj = nn.Linear(dim, n_heads * head_dim, bias=False)\n        self.k_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=False)\n        self.v_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=False)\n        self.o_proj = nn.Linear(n_heads * head_dim, dim, bias=False)\n\n        self.q_norm = nn.RMSNorm(head_dim, eps=args.rms_norm_eps)\n        self.k_norm = nn.RMSNorm(head_dim, eps=args.rms_norm_eps)\n        self.rope = initialize_rope(\n            head_dim,\n            base=args.rope_theta,\n            traditional=False,\n            scaling_config=args.rope_scaling,\n            max_position_embeddings=args.max_position_embeddings,\n        )\n\n    def __call__(\n        self,\n        x: mx.array,\n        mask: Optional[mx.array] = None,\n        cache: Optional[Any] = None,\n    ) -> mx.array:\n        B, L, D = x.shape\n\n        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)\n\n        queries = self.q_norm(queries.reshape(B, L, self.n_heads, -1)).transpose(\n            0, 2, 1, 3\n        )\n        keys = self.k_norm(keys.reshape(B, L, self.n_kv_heads, -1)).transpose(\n            0, 2, 1, 3\n        )\n        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)\n\n        if cache is not None:\n            queries = self.rope(queries, offset=cache.offset)\n            keys = self.rope(keys, offset=cache.offset)\n            keys, values = cache.update_and_fetch(keys, values)\n        else:\n            queries = self.rope(queries)\n            keys = self.rope(keys)\n\n        output = scaled_dot_product_attention(\n            queries, keys, values, cache=cache, scale=self.scale, mask=mask\n        )\n        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)\n        return self.o_proj(output)\n\n\nclass MLP(nn.Module):\n    def __init__(self, dim, hidden_dim):\n        super().__init__()\n        self.gate_proj = nn.Linear(dim, hidden_dim, bias=False)\n        self.down_proj = nn.Linear(hidden_dim, dim, bias=False)\n        self.up_proj = nn.Linear(dim, hidden_dim, bias=False)\n\n    def __call__(self, x) -> mx.array:\n        return self.down_proj(nn.silu(self.gate_proj(x)) * self.up_proj(x))\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.num_attention_heads = args.num_attention_heads\n        self.hidden_size = args.hidden_size\n        self.self_attn = Attention(args)\n        self.mlp = MLP(args.hidden_size, args.intermediate_size)\n        self.input_layernorm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)\n        self.post_attention_layernorm = nn.RMSNorm(\n            args.hidden_size, eps=args.rms_norm_eps\n        )\n        self.args = args\n\n    def __call__(\n        self,\n        x: mx.array,\n        mask: Optional[mx.array] = None,\n        cache: Optional[Any] = None,\n    ) -> mx.array:\n        r = self.self_attn(self.input_layernorm(x), mask, cache)\n        h = x + r\n        r = self.mlp(self.post_attention_layernorm(h))\n        out = h + r\n        return out\n\n\nclass Qwen3Model(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.args = args\n        self.vocab_size = args.vocab_size\n        self.num_hidden_layers = args.num_hidden_layers\n        assert self.vocab_size > 0\n        self.embed_tokens = nn.Embedding(args.vocab_size, args.hidden_size)\n        self.layers = [\n            TransformerBlock(args=args) for _ in range(args.num_hidden_layers)\n        ]\n        self.norm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)\n\n    def __call__(\n        self,\n        inputs: mx.array,\n        mask: mx.array = None,\n        cache=None,\n    ):\n        h = self.embed_tokens(inputs)\n\n        if mask is None:\n            mask = create_attention_mask(h, cache)\n\n        if cache is None:\n            cache = [None] * len(self.layers)\n\n        for layer, c in zip(self.layers, cache):\n            h = layer(h, mask, c)\n\n        return self.norm(h)\n\n\nclass Model(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.args = args\n        self.model_type = args.model_type\n        self.model = Qwen3Model(args)\n        if not args.tie_word_embeddings:\n            self.lm_head = nn.Linear(args.hidden_size, args.vocab_size, bias=False)\n\n    def __call__(\n        self,\n        inputs: mx.array,\n        mask: mx.array = None,\n        cache=None,\n    ):\n        out = self.model(inputs, mask, cache)\n        if self.args.tie_word_embeddings:\n            out = self.model.embed_tokens.as_linear(out)\n        else:\n            out = self.lm_head(out)\n        return out\n\n    def sanitize(self, weights):\n        if self.args.tie_word_embeddings:\n            weights.pop(\"lm_head.weight\", None)\n        return weights\n\n    @property\n    def layers(self):\n        return self.model.layers"}, {"role": "assistant", "content": "//\n//  Qwen3.swift\n//  LLM\n//\n//  Created by John Mai on 2025/4/28.\n//\n\nimport Foundation\nimport MLX\nimport MLXLMCommon\nimport MLXNN\n\n// port of https://github.com/ml-explore/mlx-lm/blob/main/mlx_lm/models/qwen3.py\n\nprivate class Attention: Module {\n    let args: Qwen3Configuration\n    let scale: Float\n\n    @ModuleInfo(key: \"q_proj\") var wq: Linear\n    @ModuleInfo(key: \"k_proj\") var wk: Linear\n    @ModuleInfo(key: \"v_proj\") var wv: Linear\n    @ModuleInfo(key: \"o_proj\") var wo: Linear\n\n    @ModuleInfo(key: \"q_norm\") var qNorm: RMSNorm\n    @ModuleInfo(key: \"k_norm\") var kNorm: RMSNorm\n\n    let rope: RoPE\n\n    public init(_ args: Qwen3Configuration) {\n        self.args = args\n\n        let dim = args.hiddenSize\n        let heads = args.attentionHeads\n        let kvHeads = args.kvHeads\n\n        let headDim = args.headDim\n        self.scale = pow(Float(headDim), -0.5)\n\n        _wq.wrappedValue = Linear(dim, heads * headDim, bias: false)\n        _wk.wrappedValue = Linear(dim, kvHeads * headDim, bias: false)\n        _wv.wrappedValue = Linear(dim, kvHeads * headDim, bias: false)\n        _wo.wrappedValue = Linear(heads * headDim, dim, bias: false)\n\n        _qNorm.wrappedValue = RMSNorm(dimensions: headDim, eps: args.rmsNormEps)\n        _kNorm.wrappedValue = RMSNorm(dimensions: headDim, eps: args.rmsNormEps)\n\n        let ropeScale: Float\n        if let ropeScaling = args.ropeScaling, ropeScaling[\"type\"] == .string(\"linear\"),\n            let factor = ropeScaling[\"factor\"]\n        {\n            if let v = factor.asFloat() {\n                ropeScale = 1 / v\n            } else {\n                fatalError(\"ropeScaling.factor must be a float\")\n            }\n        } else {\n            ropeScale = 1\n        }\n\n        self.rope = RoPE(\n            dimensions: headDim, traditional: false, base: args.ropeTheta,\n            scale: ropeScale)\n    }\n\n    public func callAsFunction(\n        _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?\n    ) -> MLXArray {\n        let (B, L) = (x.dim(0), x.dim(1))\n\n        var queries = wq(x)\n        var keys = wk(x)\n        var values = wv(x)\n\n        // prepare the queries, keys and values for the attention computation\n        queries = qNorm(queries.reshaped(B, L, args.attentionHeads, -1)).transposed(0, 2, 1, 3)\n        keys = kNorm(keys.reshaped(B, L, args.kvHeads, -1)).transposed(0, 2, 1, 3)\n        values = values.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)\n\n        // Apply RoPE positioning\n        if let cache {\n            queries = rope(queries, offset: cache.offset)\n            keys = rope(keys, offset: cache.offset)\n        } else {\n            queries = rope(queries)\n            keys = rope(keys)\n        }\n\n        // Use the automatic attention router that handles both quantized and regular caches\n        let output = attentionWithCacheUpdate(\n            queries: queries,\n            keys: keys,\n            values: values,\n            cache: cache,\n            scale: scale,\n            mask: mask\n        )\n        .transposed(0, 2, 1, 3)\n        .reshaped(B, L, -1)\n\n        return wo(output)\n    }\n}\n\nprivate class MLP: Module, UnaryLayer {\n    @ModuleInfo(key: \"gate_proj\") var gate: Linear\n    @ModuleInfo(key: \"down_proj\") var down: Linear\n    @ModuleInfo(key: \"up_proj\") var up: Linear\n\n    public init(dimensions: Int, hiddenDimensions: Int) {\n        _gate.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)\n        _down.wrappedValue = Linear(hiddenDimensions, dimensions, bias: false)\n        _up.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)\n    }\n\n    public func callAsFunction(_ x: MLXArray) -> MLXArray {\n        down(silu(gate(x)) * up(x))\n    }\n}\n\nprivate class TransformerBlock: Module {\n    @ModuleInfo(key: \"self_attn\") var attention: Attention\n    let mlp: MLP\n\n    @ModuleInfo(key: \"input_layernorm\") var inputLayerNorm: RMSNorm\n    @ModuleInfo(key: \"post_attention_layernorm\") var postAttentionLayerNorm: RMSNorm\n\n    public init(_ args: Qwen3Configuration) {\n        _attention.wrappedValue = Attention(args)\n        self.mlp = MLP(dimensions: args.hiddenSize, hiddenDimensions: args.intermediateSize)\n        _inputLayerNorm.wrappedValue = RMSNorm(\n            dimensions: args.hiddenSize, eps: args.rmsNormEps)\n        _postAttentionLayerNorm.wrappedValue = RMSNorm(\n            dimensions: args.hiddenSize, eps: args.rmsNormEps)\n    }\n\n    public func callAsFunction(\n        _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?\n    ) -> MLXArray {\n        var r = attention(inputLayerNorm(x), mask: mask, cache: cache)\n        let h = x + r\n        r = mlp(postAttentionLayerNorm(h))\n        let out = h + r\n        return out\n    }\n}\n\nprivate class Qwen3ModelInner: Module {\n    @ModuleInfo(key: \"embed_tokens\") var embedTokens: Embedding\n\n    fileprivate let layers: [TransformerBlock]\n    let norm: RMSNorm\n\n    public init(_ args: Qwen3Configuration) {\n        precondition(args.vocabularySize > 0)\n\n        _embedTokens.wrappedValue = Embedding(\n            embeddingCount: args.vocabularySize, dimensions: args.hiddenSize)\n\n        self.layers = (0 ..< args.hiddenLayers)\n            .map { _ in\n                TransformerBlock(args)\n            }\n        self.norm = RMSNorm(dimensions: args.hiddenSize, eps: args.rmsNormEps)\n    }\n\n    public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]? = nil) -> MLXArray {\n        var h = embedTokens(inputs)\n\n        let mask = createAttentionMask(h: h, cache: cache)\n\n        for (i, layer) in layers.enumerated() {\n            h = layer(h, mask: mask, cache: cache?[i])\n        }\n\n        return norm(h)\n    }\n}\n\npublic class Qwen3Model: Module, LLMModel, KVCacheDimensionProvider {\n    public let vocabularySize: Int\n    public let kvHeads: [Int]\n\n    private let model: Qwen3ModelInner\n    let configuration: Qwen3Configuration\n\n    @ModuleInfo(key: \"lm_head\") var lmHead: Linear?\n\n    public init(_ args: Qwen3Configuration) {\n        self.configuration = args\n        self.vocabularySize = args.vocabularySize\n        self.kvHeads = (0 ..< args.hiddenLayers).map { _ in args.kvHeads }\n        self.model = Qwen3ModelInner(args)\n\n        if !args.tieWordEmbeddings {\n            _lmHead.wrappedValue = Linear(args.hiddenSize, args.vocabularySize, bias: false)\n        }\n    }\n\n    public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]?) -> MLXArray {\n        var out = model(inputs, cache: cache)\n        if let lmHead {\n            out = lmHead(out)\n        } else {\n            out = model.embedTokens.asLinear(out)\n        }\n        return out\n    }\n\n    public func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {\n        var weights = weights\n\n        if configuration.tieWordEmbeddings {\n            weights[\"lm_head.weight\"] = nil\n        }\n\n        return weights\n    }\n}\n\npublic struct Qwen3Configuration: Codable, Sendable {\n    var hiddenSize: Int\n    var hiddenLayers: Int\n    var intermediateSize: Int\n    var attentionHeads: Int\n    var rmsNormEps: Float\n    var vocabularySize: Int\n    var kvHeads: Int\n    var ropeTheta: Float = 1_000_000\n    var headDim: Int\n    var ropeScaling: [String: StringOrNumber]? = nil\n    var tieWordEmbeddings = false\n    var maxPositionEmbeddings: Int = 32768\n\n    enum CodingKeys: String, CodingKey {\n        case hiddenSize = \"hidden_size\"\n        case hiddenLayers = \"num_hidden_layers\"\n        case intermediateSize = \"intermediate_size\"\n        case attentionHeads = \"num_attention_heads\"\n        case rmsNormEps = \"rms_norm_eps\"\n        case vocabularySize = \"vocab_size\"\n        case kvHeads = \"num_key_value_heads\"\n        case ropeTheta = \"rope_theta\"\n        case headDim = \"head_dim\"\n        case ropeScaling = \"rope_scaling\"\n        case tieWordEmbeddings = \"tie_word_embeddings\"\n        case maxPositionEmbeddings = \"max_position_embeddings\"\n    }\n\n    public init(from decoder: Decoder) throws {\n        // custom implementation to handle optional keys with required values\n        let container: KeyedDecodingContainer<Qwen3Configuration.CodingKeys> =\n            try decoder.container(\n                keyedBy: Qwen3Configuration.CodingKeys.self)\n\n        self.hiddenSize = try container.decode(\n            Int.self, forKey: Qwen3Configuration.CodingKeys.hiddenSize)\n        self.hiddenLayers = try container.decode(\n            Int.self, forKey: Qwen3Configuration.CodingKeys.hiddenLayers)\n        self.intermediateSize = try container.decode(\n            Int.self, forKey: Qwen3Configuration.CodingKeys.intermediateSize)\n        self.attentionHeads = try container.decode(\n            Int.self, forKey: Qwen3Configuration.CodingKeys.attentionHeads)\n        self.rmsNormEps = try container.decode(\n            Float.self, forKey: Qwen3Configuration.CodingKeys.rmsNormEps)\n        self.vocabularySize = try container.decode(\n            Int.self, forKey: Qwen3Configuration.CodingKeys.vocabularySize)\n        self.kvHeads = try container.decode(Int.self, forKey: Qwen3Configuration.CodingKeys.kvHeads)\n        self.ropeTheta =\n            try container.decodeIfPresent(\n                Float.self, forKey: Qwen3Configuration.CodingKeys.ropeTheta)\n            ?? 1_000_000\n        self.headDim = try container.decode(\n            Int.self, forKey: Qwen3Configuration.CodingKeys.headDim)\n        self.ropeScaling = try container.decodeIfPresent(\n            [String: StringOrNumber].self, forKey: Qwen3Configuration.CodingKeys.ropeScaling)\n        self.tieWordEmbeddings =\n            try container.decodeIfPresent(Bool.self, forKey: .tieWordEmbeddings) ?? false\n        self.maxPositionEmbeddings =\n            try container.decodeIfPresent(Int.self, forKey: .maxPositionEmbeddings) ?? 32768\n    }\n}\n\n// MARK: - LoRA\n\nextension Qwen3Model: LoRAModel {\n    public func loraLinearLayers() -> LoRALinearLayers {\n        model.layers.map { ($0.attention, [\"q_proj\", \"v_proj\"]) }\n    }\n}"}]}
{"messages": [{"role": "user", "content": "convert the following MLX Python code into MLX Swift: \n\n# Copyright \u00a9 2025 Apple Inc.\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional, Union\n\nimport mlx.core as mx\nimport mlx.nn as nn\n\nfrom .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention\nfrom .switch_layers import SwitchGLU\n\n\n@dataclass\nclass ModelArgs(BaseModelArgs):\n    model_type: str\n    hidden_size: int\n    num_hidden_layers: int\n    intermediate_size: int\n    num_attention_heads: int\n    num_experts: int\n    num_experts_per_tok: int\n    decoder_sparse_step: int\n    mlp_only_layers: List[int]\n    moe_intermediate_size: int\n    rms_norm_eps: float\n    vocab_size: int\n    num_key_value_heads: int\n    head_dim: int\n    rope_theta: float\n    tie_word_embeddings: bool\n    max_position_embeddings: int\n    norm_topk_prob: bool\n    rope_scaling: Optional[Dict[str, Union[float, str]]] = None\n\n\nclass Attention(nn.Module):\n    def __init__(self, args: ModelArgs, layer_idx: int):\n        super().__init__()\n\n        dim = args.hidden_size\n        self.n_heads = n_heads = args.num_attention_heads\n        assert args.num_key_value_heads is not None\n        self.n_kv_heads = n_kv_heads = args.num_key_value_heads\n\n        head_dim = getattr(\n            args, \"head_dim\", args.hidden_size // args.num_attention_heads\n        )\n        self.scale = head_dim**-0.5\n\n        self.q_proj = nn.Linear(dim, n_heads * head_dim, bias=False)\n        self.k_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=False)\n        self.v_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=False)\n        self.o_proj = nn.Linear(n_heads * head_dim, dim, bias=False)\n\n        self.q_norm = nn.RMSNorm(head_dim, eps=args.rms_norm_eps)\n        self.k_norm = nn.RMSNorm(head_dim, eps=args.rms_norm_eps)\n\n        self.rope = nn.RoPE(\n            head_dim,\n            traditional=False,\n            base=args.rope_theta,\n        )\n\n    def __call__(\n        self,\n        x: mx.array,\n        mask: Optional[mx.array] = None,\n        cache: Optional[Any] = None,\n    ) -> mx.array:\n        B, L, D = x.shape\n\n        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)\n\n        # Prepare the queries, keys and values for the attention computation\n        queries = self.q_norm(queries.reshape(B, L, self.n_heads, -1)).transpose(\n            0, 2, 1, 3\n        )\n        keys = self.k_norm(keys.reshape(B, L, self.n_kv_heads, -1)).transpose(\n            0, 2, 1, 3\n        )\n        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)\n\n        if cache is not None:\n            queries = self.rope(queries, offset=cache.offset)\n            keys = self.rope(keys, offset=cache.offset)\n            keys, values = cache.update_and_fetch(keys, values)\n        else:\n            queries = self.rope(queries)\n            keys = self.rope(keys)\n\n        output = scaled_dot_product_attention(\n            queries, keys, values, cache=cache, scale=self.scale, mask=mask\n        )\n        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)\n        return self.o_proj(output)\n\n\nclass MLP(nn.Module):\n    def __init__(self, dim, hidden_dim):\n        super().__init__()\n        self.gate_proj = nn.Linear(dim, hidden_dim, bias=False)\n        self.down_proj = nn.Linear(hidden_dim, dim, bias=False)\n        self.up_proj = nn.Linear(dim, hidden_dim, bias=False)\n\n    def __call__(self, x) -> mx.array:\n        return self.down_proj(nn.silu(self.gate_proj(x)) * self.up_proj(x))\n\n\nclass Qwen3MoeSparseMoeBlock(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        dim = args.hidden_size\n        intermediate_size = args.moe_intermediate_size\n\n        self.num_experts = num_experts = args.num_experts\n        self.top_k = args.num_experts_per_tok\n        self.norm_topk_prob = args.norm_topk_prob\n\n        self.gate = nn.Linear(dim, num_experts, bias=False)\n        self.switch_mlp = SwitchGLU(dim, intermediate_size, num_experts)\n\n    def __call__(\n        self,\n        x: mx.array,\n    ):\n        gates = self.gate(x)\n        gates = mx.softmax(gates, axis=-1, precise=True)\n\n        k = self.top_k\n        inds = mx.stop_gradient(mx.argpartition(-gates, kth=k - 1, axis=-1)[..., :k])\n        scores = mx.take_along_axis(gates, inds, axis=-1)\n        if self.norm_topk_prob:\n            scores /= mx.sum(scores, axis=-1, keepdims=True)\n\n        y = self.switch_mlp(x, inds)\n        y = (y * scores[..., None]).sum(axis=-2)\n\n        return y\n\n\nclass Qwen3MoeDecoderLayer(nn.Module):\n    def __init__(self, args: ModelArgs, layer_idx: int):\n        super().__init__()\n        self.hidden_size = args.hidden_size\n        self.self_attn = Attention(args, layer_idx)\n\n        self.input_layernorm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)\n        self.post_attention_layernorm = nn.RMSNorm(\n            args.hidden_size, eps=args.rms_norm_eps\n        )\n        self.args = args\n\n        if (layer_idx not in args.mlp_only_layers) and (\n            args.num_experts > 0 and (layer_idx + 1) % args.decoder_sparse_step == 0\n        ):\n            self.mlp = Qwen3MoeSparseMoeBlock(args)\n        else:\n            self.mlp = MLP(args.hidden_size, args.intermediate_size)\n\n    def __call__(\n        self,\n        x: mx.array,\n        mask: Optional[mx.array] = None,\n        cache: Optional[Any] = None,\n    ) -> mx.array:\n        r = self.self_attn(self.input_layernorm(x), mask, cache)\n        h = x + r\n        r = self.mlp(self.post_attention_layernorm(h))\n        out = h + r\n        return out\n\n\nclass Qwen3MoeModel(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.args = args\n        self.vocab_size = args.vocab_size\n        self.num_hidden_layers = args.num_hidden_layers\n        assert self.vocab_size > 0\n        self.embed_tokens = nn.Embedding(args.vocab_size, args.hidden_size)\n        self.layers = [\n            Qwen3MoeDecoderLayer(args=args, layer_idx=i)\n            for i in range(args.num_hidden_layers)\n        ]\n        self.norm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)\n\n    def __call__(\n        self,\n        inputs: mx.array,\n        mask: mx.array = None,\n        cache=None,\n    ):\n        h = self.embed_tokens(inputs)\n\n        if mask is None:\n            mask = create_attention_mask(h, cache)\n\n        if cache is None:\n            cache = [None] * len(self.layers)\n\n        for layer, c in zip(self.layers, cache):\n            h = layer(h, mask, c)\n\n        return self.norm(h)\n\n\nclass Model(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.args = args\n        self.model_type = args.model_type\n        self.model = Qwen3MoeModel(args)\n        self.lm_head = nn.Linear(args.hidden_size, args.vocab_size, bias=False)\n\n    def __call__(\n        self,\n        inputs: mx.array,\n        mask: mx.array = None,\n        cache=None,\n    ):\n        out = self.model(inputs, mask, cache)\n        return self.lm_head(out)\n\n    def sanitize(self, weights):\n        if \"model.layers.0.mlp.experts.0.up_proj.weight\" not in weights:\n            return weights\n        for l in range(self.args.num_hidden_layers):\n            prefix = f\"model.layers.{l}\"\n            for n in [\"up_proj\", \"down_proj\", \"gate_proj\"]:\n                if f\"{prefix}.mlp.experts.0.{n}.weight\" in weights:\n                    to_join = [\n                        weights.pop(f\"{prefix}.mlp.experts.{e}.{n}.weight\")\n                        for e in range(self.args.num_experts)\n                    ]\n                    weights[f\"{prefix}.mlp.switch_mlp.{n}.weight\"] = mx.stack(to_join)\n        return weights\n\n    @property\n    def layers(self):\n        return self.model.layers\n\nimport inspect\nfrom dataclasses import dataclass\nfrom functools import partial\nfrom typing import Any, Dict, List, Optional, Union\n\nimport mlx.core as mx\nimport mlx.nn as nn\n\nfrom ..base import (\n    LanguageModelOutput,\n    create_attention_mask,\n    scaled_dot_product_attention,\n)\nfrom ..cache import KVCache, RotatingKVCache\n\n\n@dataclass\nclass TextConfig:\n    model_type: str\n    hidden_size: int\n    num_hidden_layers: int\n    intermediate_size: int\n    num_attention_heads: int = 8\n    head_dim: int = 256\n    rms_norm_eps: float = 1.0e-6\n    vocab_size: int = 262208\n    num_key_value_heads: int = 4\n    rope_global_base_freq: float = 1_000_000.0\n    rope_local_base_freq: float = 10_000.0\n    rope_traditional: bool = False\n    query_pre_attn_scalar: float = 256\n    sliding_window: int = 1024\n    rope_scaling: Optional[Dict[str, Union[float, List[float]]]] = None\n    mm_tokens_per_image: int = 256\n    sliding_window_pattern: int = 6\n    max_position_embeddings: int = 4096\n\n    @classmethod\n    def from_dict(cls, params):\n        return cls(\n            **{\n                k: v\n                for k, v in params.items()\n                if k in inspect.signature(cls).parameters\n            }\n        )\n\n\nclass RMSNorm(nn.Module):\n    def __init__(self, dims: int, eps: float = 1e-5):\n        super().__init__()\n        self.weight = mx.ones((dims,))\n        self.eps = eps\n\n    def __call__(self, x):\n        return mx.fast.rms_norm(x, 1.0 + self.weight, self.eps)\n\n\nclass Attention(nn.Module):\n    def __init__(self, config: TextConfig, layer_idx: int):\n        super().__init__()\n\n        dim = config.hidden_size\n        self.n_heads = n_heads = config.num_attention_heads\n        self.n_kv_heads = n_kv_heads = config.num_key_value_heads\n        self.repeats = n_heads // n_kv_heads\n        self.head_dim = head_dim = config.head_dim\n        self.layer_idx = layer_idx\n\n        self.scale = config.query_pre_attn_scalar**-0.5\n\n        self.q_proj = nn.Linear(dim, n_heads * head_dim, bias=False)\n        self.k_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=False)\n        self.v_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=False)\n        self.o_proj = nn.Linear(n_heads * head_dim, dim, bias=False)\n\n        self.q_norm = RMSNorm(dims=head_dim, eps=config.rms_norm_eps)\n        self.k_norm = RMSNorm(dims=head_dim, eps=config.rms_norm_eps)\n        self.is_sliding = (layer_idx + 1) % config.sliding_window_pattern != 0\n\n        self.rope = nn.RoPE(\n            head_dim,\n            traditional=config.rope_traditional,\n            base=(\n                config.rope_local_base_freq\n                if self.is_sliding\n                else config.rope_global_base_freq\n            ),\n        )\n\n    def __call__(\n        self,\n        x: mx.array,\n        mask: Optional[mx.array] = None,\n        cache: Optional[Any] = None,\n    ) -> mx.array:\n        B, L, _ = x.shape\n        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)\n        queries = queries.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)\n\n        keys = keys.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)\n        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)\n\n        queries = self.q_norm(queries)\n        keys = self.k_norm(keys)\n\n        if cache is not None:\n            queries = self.rope(queries, offset=cache.offset)\n            keys = self.rope(keys, offset=cache.offset)\n            keys, values = cache.update_and_fetch(keys, values)\n        else:\n            queries = self.rope(queries)\n            keys = self.rope(keys)\n\n        # Sliding window\n        if mask is not None and isinstance(mask, mx.array):\n            if mask.shape[-1] != keys.shape[-2]:\n                mask = mask[..., -keys.shape[-2] :]\n\n        output = scaled_dot_product_attention(\n            queries, keys, values, cache, scale=self.scale, mask=mask\n        )\n        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)\n        return self.o_proj(output)\n\n\nclass MLP(nn.Module):\n    def __init__(self, dim, hidden_dim):\n        super().__init__()\n        self.gate_proj = nn.Linear(dim, hidden_dim, bias=False)\n        self.down_proj = nn.Linear(hidden_dim, dim, bias=False)\n        self.up_proj = nn.Linear(dim, hidden_dim, bias=False)\n\n    def __call__(self, x) -> mx.array:\n        # This should not be GELU approx, jax.nn.gelu\n        return self.down_proj(nn.gelu_approx(self.gate_proj(x)) * self.up_proj(x))\n\n\n@partial(mx.compile, shapeless=True)\ndef clip_residual(x, y=None):\n    bound = mx.finfo(mx.float16).max\n    if y is None:\n        if x.dtype == mx.float16:\n            return mx.clip(x.astype(mx.float32), -bound, bound).astype(mx.float16)\n        else:\n            return x\n\n    if x.dtype != mx.float16:\n        return x + y\n\n    return mx.clip(x.astype(mx.float32) + y.astype(mx.float32), -bound, bound).astype(\n        mx.float16\n    )\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, config: TextConfig, layer_idx: int):\n        super().__init__()\n        self.num_attention_heads = config.num_attention_heads\n        self.hidden_size = config.hidden_size\n        self.self_attn = Attention(config, layer_idx)\n        self.mlp = MLP(config.hidden_size, config.intermediate_size)\n        self.input_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.post_attention_layernorm = RMSNorm(\n            config.hidden_size, eps=config.rms_norm_eps\n        )\n        self.pre_feedforward_layernorm = RMSNorm(\n            config.hidden_size, eps=config.rms_norm_eps\n        )\n        self.post_feedforward_layernorm = RMSNorm(\n            config.hidden_size, eps=config.rms_norm_eps\n        )\n\n    def __call__(\n        self,\n        x: mx.array,\n        mask: Optional[mx.array] = None,\n        cache: Optional[Any] = None,\n    ) -> mx.array:\n\n        # Clip the input to avoid overflow in float16\n        # Float16 has a max value of 65504. When values exceed this limit, they become inf.\n        # Example: If x contains 70000.0 in float16, it becomes inf, causing gradient issues.\n        # We upcast to float32 for operations that might exceed the limit, then clip and\n        # convert back to float16 to maintain numerical stability.\n\n        # Clip input to avoid overflow in float16\n        x = clip_residual(x)\n\n        # Self-attention block\n        r = self.self_attn(self.input_layernorm(x), mask, cache)\n        h = self.post_attention_layernorm(r)\n\n        # Add residual connection with overflow protection for float16\n        h = clip_residual(x + h)\n\n        # MLP block\n        r = self.mlp(self.pre_feedforward_layernorm(h))\n        out = self.post_feedforward_layernorm(r)\n\n        # Add residual connection with overflow protection for float16\n        out = clip_residual(h + out)\n\n        return out\n\n\nclass Gemma3Model(nn.Module):\n    def __init__(self, config: TextConfig):\n        super().__init__()\n        self.config = config\n        self.vocab_size = config.vocab_size\n        self.num_hidden_layers = config.num_hidden_layers\n        assert self.vocab_size > 0\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n        self.layers = [\n            TransformerBlock(config=config, layer_idx=layer_idx)\n            for layer_idx in range(config.num_hidden_layers)\n        ]\n        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def __call__(\n        self,\n        inputs: mx.array,\n        inputs_embeds: mx.array = None,\n        mask: mx.array = None,\n        cache=None,\n    ):\n        if inputs_embeds is None:\n            h = self.embed_tokens(inputs)\n        else:\n            h = inputs_embeds\n\n        h *= mx.array(self.config.hidden_size**0.5, mx.bfloat16).astype(h.dtype)\n\n        if cache is None:\n            cache = [None] * len(self.layers)\n\n        if mask is None:\n            j = self.config.sliding_window_pattern\n            full_mask = create_attention_mask(h, cache[j - 1 : j])\n            sliding_window_mask = create_attention_mask(h, cache)\n\n        for i, (layer, c) in enumerate(zip(self.layers, cache)):\n            is_global = (\n                i % self.config.sliding_window_pattern\n                == self.config.sliding_window_pattern - 1\n            )\n\n            local_mask = mask\n            if mask is None and is_global:\n                local_mask = full_mask\n            elif mask is None:\n                local_mask = sliding_window_mask\n\n            h = layer(h, local_mask, c)\n\n        return self.norm(h)\n\n\nclass LanguageModel(nn.Module):\n    def __init__(self, config: TextConfig):\n        super().__init__()\n        self.config = config\n        self.model_type = config.model_type\n        self.model = Gemma3Model(config)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n    def __call__(\n        self,\n        inputs: mx.array,\n        inputs_embeds: Optional[mx.array] = None,\n        mask: Optional[mx.array] = None,\n        cache=None,\n    ):\n        out = self.model(inputs, inputs_embeds=inputs_embeds, mask=mask, cache=cache)\n        out = self.lm_head(out)\n        return LanguageModelOutput(logits=out)\n\n    def sanitize(self, weights):\n        if \"lm_head.weight\" not in weights:\n            weights[\"language_model.lm_head.weight\"] = weights[\n                \"language_model.model.embed_tokens.weight\"\n            ]\n        return {\n            k: v for k, v in weights.items() if \"self_attn.rotary_emb.inv_freq\" not in k\n        }\n\n    @property\n    def layers(self):\n        return self.model.layers\n\n    @property\n    def head_dim(self):\n        return self.config.head_dim\n\n    @property\n    def n_kv_heads(self):\n        return self.config.num_key_value_heads\n\n    def make_cache(self):\n        caches = []\n        for i in range(self.config.num_hidden_layers):\n            if (\n                i % self.config.sliding_window_pattern\n                == self.config.sliding_window_pattern - 1\n            ):\n                caches.append(KVCache())\n            else:\n                caches.append(\n                    RotatingKVCache(\n                        max_size=self.config.sliding_window,\n                        keep=0,\n                    )\n                )\n        return caches\n\nimport inspect\nfrom dataclasses import dataclass\nfrom typing import Optional\n\nimport mlx.core as mx\nimport mlx.nn as nn\nimport numpy as np\n\n\n@dataclass\nclass VisionConfig:\n    model_type: str\n    num_hidden_layers: int\n    hidden_size: int\n    intermediate_size: int\n    num_attention_heads: int\n    patch_size: int\n    image_size: int = 224\n    num_channels: int = 3\n    layer_norm_eps: float = 1e-6\n\n    @classmethod\n    def from_dict(cls, params):\n        return cls(\n            **{\n                k: v\n                for k, v in params.items()\n                if k in inspect.signature(cls).parameters\n            }\n        )\n\n\ndef check_array_shape(arr):\n    shape = arr.shape\n\n    # Check if the shape has 4 dimensions\n    if len(shape) != 4:\n        return False\n\n    out_channels, kH, KW, _ = shape\n\n    # Check if out_channels is the largest, and kH and KW are the same\n    if (out_channels >= kH) and (out_channels >= KW) and (kH == KW):\n        return True\n    else:\n        return False\n\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dims: int,\n        num_heads: int,\n        query_input_dims: Optional[int] = None,\n        key_input_dims: Optional[int] = None,\n        value_input_dims: Optional[int] = None,\n        value_dims: Optional[int] = None,\n        value_output_dims: Optional[int] = None,\n        bias: bool = True,\n    ):\n        super().__init__()\n\n        if (dims % num_heads) != 0:\n            raise ValueError(\n                \"The input feature dimensions should be divisible by the \"\n                f\"number of heads ({dims} % {num_heads}) != 0\"\n            )\n\n        query_input_dims = query_input_dims or dims\n        key_input_dims = key_input_dims or dims\n        value_input_dims = value_input_dims or key_input_dims\n        value_dims = value_dims or dims\n        value_output_dims = value_output_dims or dims\n\n        self.num_heads = num_heads\n        head_dim = dims // num_heads\n        self.scale = head_dim**-0.5\n\n        self.q_proj = nn.Linear(query_input_dims, dims, bias=bias)\n        self.k_proj = nn.Linear(key_input_dims, dims, bias=bias)\n        self.v_proj = nn.Linear(value_input_dims, value_dims, bias=bias)\n        self.out_proj = nn.Linear(value_dims, value_output_dims, bias=bias)\n\n    def __call__(self, x, mask=None):\n        queries = self.q_proj(x)\n        keys = self.k_proj(x)\n        values = self.v_proj(x)\n\n        num_heads = self.num_heads\n        B, L, D = queries.shape\n        _, S, _ = keys.shape\n        queries = queries.reshape(B, L, num_heads, -1).transpose(0, 2, 1, 3)\n        keys = keys.reshape(B, S, num_heads, -1).transpose(0, 2, 1, 3)\n        values = values.reshape(B, S, num_heads, -1).transpose(0, 2, 1, 3)\n\n        output = mx.fast.scaled_dot_product_attention(\n            queries, keys, values, scale=self.scale, mask=mask\n        )\n        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)\n        return self.out_proj(output)\n\n\nclass MLP(nn.Module):\n    def __init__(self, config: VisionConfig):\n        super().__init__()\n        self.activation_fn = nn.GELU(approx=\"precise\")\n        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size, bias=True)\n        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size, bias=True)\n\n    def __call__(self, x: mx.array) -> mx.array:\n        x = self.fc1(x)\n        x = self.activation_fn(x)\n        x = self.fc2(x)\n        return x\n\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, config: VisionConfig):\n        super().__init__()\n        self.embed_dim = config.hidden_size\n        self.self_attn = Attention(\n            config.hidden_size, config.num_attention_heads, bias=True\n        )\n        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n        self.mlp = MLP(config)\n        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n\n    def __call__(self, x: mx.array, mask: Optional[mx.array] = None) -> mx.array:\n        r = self.self_attn(self.layer_norm1(x), mask)\n        h = x + r\n        r = self.mlp(self.layer_norm2(h))\n        return h + r\n\n\nclass Encoder(nn.Module):\n    def __init__(self, config: VisionConfig):\n        super().__init__()\n        self.layers = [EncoderLayer(config) for _ in range(config.num_hidden_layers)]\n\n    def __call__(\n        self,\n        x: mx.array,\n        output_hidden_states: Optional[bool] = None,\n        mask: Optional[mx.array] = None,\n    ) -> mx.array:\n        encoder_states = (x,) if output_hidden_states else None\n        h = x\n        for l in self.layers:\n            x = l(x, mask=mask)\n            if output_hidden_states:\n                encoder_states = encoder_states + (x,)\n\n            h = x\n\n        return (h, encoder_states)\n\n\nclass VisionEmbeddings(nn.Module):\n    def __init__(self, config: VisionConfig):\n        super().__init__()\n        self.config = config\n        self.embed_dim = config.hidden_size\n        self.image_size = config.image_size\n        self.patch_size = config.patch_size\n\n        self.patch_embedding = nn.Conv2d(\n            in_channels=config.num_channels,\n            out_channels=self.embed_dim,\n            kernel_size=self.patch_size,\n            stride=self.patch_size,\n        )\n\n        self.num_patches = (self.image_size // self.patch_size) ** 2\n        self.num_positions = self.num_patches\n        self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)\n\n    def __call__(self, x: mx.array) -> mx.array:\n        patch_embeddings = self.patch_embedding(x)\n        patch_embeddings = mx.flatten(patch_embeddings, start_axis=1, end_axis=2)\n        position_ids = mx.array(np.arange(self.num_positions)[None, :])\n        embeddings = patch_embeddings\n        embeddings += self.position_embedding(position_ids)\n        return embeddings\n\n\nclass SigLipVisionModel(nn.Module):\n    def __init__(self, config: VisionConfig):\n        super().__init__()\n        self.embeddings = VisionEmbeddings(config)\n        self.encoder = Encoder(config)\n        self.post_layernorm = nn.LayerNorm(config.hidden_size)\n\n    def __call__(\n        self,\n        x: mx.array,\n        output_hidden_states: Optional[bool] = None,\n    ) -> mx.array:\n        x = self.embeddings(x)\n\n        encoder_outputs = self.encoder(\n            x=x, output_hidden_states=output_hidden_states, mask=None\n        )\n\n        pooler_output = self.post_layernorm(encoder_outputs[0])\n\n        return pooler_output, x, encoder_outputs[-1]\n\n\nclass VisionModel(nn.Module):\n    def __init__(self, config: VisionConfig):\n        super().__init__()\n        self.model_type = config.model_type\n        if self.model_type not in [\"siglip_vision_model\", \"gemma3\", \"gemma3_vision\"]:\n            raise ValueError(f\"Unsupported model type: {self.model_type}\")\n\n        self.vision_model = SigLipVisionModel(config)\n\n    def __call__(\n        self, x: mx.array, output_hidden_states: Optional[bool] = None\n    ) -> mx.array:\n        return self.vision_model(x, output_hidden_states)\n\n    def sanitize(self, weights):\n        sanitized_weights = {}\n        for k, v in weights.items():\n            if \"patch_embedding.weight\" in k:\n                # PyTorch conv2d weight tensors have shape:\n                #   [out_channels, in_channels, kH, KW]\n                # MLX conv2d expects the weight be of shape:\n                #   [out_channels, kH, KW, in_channels]\n                if check_array_shape(v):\n                    sanitized_weights[k] = v\n                else:\n                    sanitized_weights[k] = v.transpose(0, 2, 3, 1)\n            else:\n                sanitized_weights[k] = v\n\n        return sanitized_weights"}, {"role": "assistant", "content": "//\n//  Qwen3MoE.swift\n//  LLM\n//\n//  Created by John Mai on 2025/4/30.\n//\n\nimport Foundation\nimport MLX\nimport MLXLMCommon\nimport MLXNN\n\n// port of https://github.com/ml-explore/mlx-lm/blob/main/mlx_lm/models/qwen3_moe.py\n\nprivate class Attention: Module {\n    let args: Qwen3MoEConfiguration\n    let scale: Float\n\n    @ModuleInfo(key: \"q_proj\") var wq: Linear\n    @ModuleInfo(key: \"k_proj\") var wk: Linear\n    @ModuleInfo(key: \"v_proj\") var wv: Linear\n    @ModuleInfo(key: \"o_proj\") var wo: Linear\n\n    @ModuleInfo(key: \"q_norm\") var qNorm: RMSNorm\n    @ModuleInfo(key: \"k_norm\") var kNorm: RMSNorm\n\n    let rope: RoPE\n\n    public init(_ args: Qwen3MoEConfiguration, layerIdx: Int) {\n        self.args = args\n\n        let dim = args.hiddenSize\n        let heads = args.attentionHeads\n        let kvHeads = args.kvHeads\n\n        let headDim = args.headDim\n        self.scale = pow(Float(headDim), -0.5)\n\n        _wq.wrappedValue = Linear(dim, heads * headDim, bias: false)\n        _wk.wrappedValue = Linear(dim, kvHeads * headDim, bias: false)\n        _wv.wrappedValue = Linear(dim, kvHeads * headDim, bias: false)\n        _wo.wrappedValue = Linear(heads * headDim, dim, bias: false)\n\n        _qNorm.wrappedValue = RMSNorm(dimensions: headDim, eps: args.rmsNormEps)\n        _kNorm.wrappedValue = RMSNorm(dimensions: headDim, eps: args.rmsNormEps)\n\n        let ropeScale: Float\n        if let ropeScaling = args.ropeScaling, ropeScaling[\"type\"] == .string(\"linear\"),\n            let factor = ropeScaling[\"factor\"]\n        {\n            if let v = factor.asFloat() {\n                ropeScale = 1 / v\n            } else {\n                fatalError(\"ropeScaling.factor must be a float\")\n            }\n        } else {\n            ropeScale = 1\n        }\n\n        self.rope = RoPE(\n            dimensions: headDim, traditional: false, base: args.ropeTheta,\n            scale: ropeScale)\n    }\n\n    public func callAsFunction(\n        _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?\n    ) -> MLXArray {\n        let (B, L) = (x.dim(0), x.dim(1))\n\n        var queries = wq(x)\n        var keys = wk(x)\n        var values = wv(x)\n\n        // prepare the queries, keys and values for the attention computation\n        queries = qNorm(queries.reshaped(B, L, args.attentionHeads, -1)).transposed(0, 2, 1, 3)\n        keys = kNorm(keys.reshaped(B, L, args.kvHeads, -1)).transposed(0, 2, 1, 3)\n        values = values.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)\n\n        if let cache {\n            queries = rope(queries, offset: cache.offset)\n            keys = rope(keys, offset: cache.offset)\n        } else {\n            queries = rope(queries)\n            keys = rope(keys)\n        }\n\n        let output = attentionWithCacheUpdate(\n            queries: queries,\n            keys: keys,\n            values: values,\n            cache: cache,\n            scale: scale,\n            mask: mask\n        )\n        .transposed(0, 2, 1, 3)\n        .reshaped(B, L, -1)\n\n        return wo(output)\n    }\n}\n\nprivate class MLP: Module, UnaryLayer {\n    @ModuleInfo(key: \"gate_proj\") var gate: Linear\n    @ModuleInfo(key: \"down_proj\") var down: Linear\n    @ModuleInfo(key: \"up_proj\") var up: Linear\n\n    public init(dimensions: Int, hiddenDimensions: Int) {\n        _gate.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)\n        _down.wrappedValue = Linear(hiddenDimensions, dimensions, bias: false)\n        _up.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)\n    }\n\n    public func callAsFunction(_ x: MLXArray) -> MLXArray {\n        down(silu(gate(x)) * up(x))\n    }\n}\n\nprivate class Qwen3MoESparseMoeBlock: Module, UnaryLayer {\n    let numExperts: Int\n    let topK: Int\n    let normTopkProb: Bool\n\n    @ModuleInfo(key: \"gate\") var gate: Linear\n    @ModuleInfo(key: \"switch_mlp\") var switchMLP: SwitchGLU\n\n    init(_ args: Qwen3MoEConfiguration) {\n        self.numExperts = args.numExperts\n        self.topK = args.numExpertsPerToken\n        self.normTopkProb = args.normTopkProb\n\n        _gate.wrappedValue = Linear(args.hiddenSize, numExperts, bias: false)\n        _switchMLP.wrappedValue = SwitchGLU(\n            inputDims: args.hiddenSize, hiddenDims: args.moeIntermediateSize, numExperts: numExperts\n        )\n    }\n\n    func callAsFunction(_ x: MLXArray) -> MLXArray {\n        let gates = gate(x)\n        let softGates = MLX.softmax(gates, axis: -1, precise: true)\n\n        let k = topK\n        let inds = MLX.argPartition(-gates, kth: k - 1, axis: -1)[.ellipsis, ..<k]\n        var scores = MLX.takeAlong(softGates, inds, axis: -1)\n\n        if normTopkProb {\n            scores = scores / MLX.sum(scores, axis: -1, keepDims: true)\n        }\n\n        let y = switchMLP(x, inds)\n        return (y * scores[.ellipsis, .newAxis]).sum(axis: -2)\n    }\n}\n\nprivate class Qwen3MoeDecoderLayer: Module {\n    let args: Qwen3MoEConfiguration\n    let layerIdx: Int\n\n    @ModuleInfo(key: \"self_attn\") var selfAttn: Attention\n    @ModuleInfo(key: \"input_layernorm\") var inputLayerNorm: RMSNorm\n    @ModuleInfo(key: \"post_attention_layernorm\") var postAttentionLayerNorm: RMSNorm\n\n    fileprivate let mlp: UnaryLayer\n\n    init(_ args: Qwen3MoEConfiguration, layerIdx: Int) {\n        self.args = args\n        self.layerIdx = layerIdx\n\n        _selfAttn.wrappedValue = Attention(args, layerIdx: layerIdx)\n        _inputLayerNorm.wrappedValue = RMSNorm(dimensions: args.hiddenSize, eps: args.rmsNormEps)\n        _postAttentionLayerNorm.wrappedValue = RMSNorm(\n            dimensions: args.hiddenSize, eps: args.rmsNormEps)\n\n        if !args.mlpOnlyLayers.contains(layerIdx),\n            args.numExperts > 0, (layerIdx + 1) % args.decoderSparseStep == 0\n        {\n            self.mlp = Qwen3MoESparseMoeBlock(args)\n        } else {\n            self.mlp = MLP(dimensions: args.hiddenSize, hiddenDimensions: args.intermediateSize)\n        }\n    }\n\n    func callAsFunction(\n        _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?\n    ) -> MLXArray {\n        var r = selfAttn(inputLayerNorm(x), mask: mask, cache: cache)\n        let h = x + r\n        r = mlp(postAttentionLayerNorm(h))\n        let out = h + r\n        return out\n    }\n}\n\nprivate class Qwen3MoEModelInner: Module {\n    @ModuleInfo(key: \"embed_tokens\") var embedTokens: Embedding\n\n    fileprivate let layers: [Qwen3MoeDecoderLayer]\n    let norm: RMSNorm\n    let args: Qwen3MoEConfiguration\n\n    init(_ args: Qwen3MoEConfiguration) {\n        self.args = args\n        precondition(args.vocabularySize > 0)\n\n        _embedTokens.wrappedValue = Embedding(\n            embeddingCount: args.vocabularySize, dimensions: args.hiddenSize)\n\n        self.layers = (0 ..< args.hiddenLayers)\n            .map { i in\n                Qwen3MoeDecoderLayer(args, layerIdx: i)\n            }\n        self.norm = RMSNorm(dimensions: args.hiddenSize, eps: args.rmsNormEps)\n    }\n\n    func callAsFunction(_ inputs: MLXArray, cache: [KVCache]? = nil) -> MLXArray {\n        var h = embedTokens(inputs)\n\n        let mask = createAttentionMask(h: h, cache: cache)\n\n        for (i, layer) in layers.enumerated() {\n            h = layer(h, mask: mask, cache: cache?[i])\n        }\n\n        return norm(h)\n    }\n}\n\npublic class Qwen3MoEModel: Module, LLMModel, KVCacheDimensionProvider {\n    public let vocabularySize: Int\n    public let kvHeads: [Int]\n\n    fileprivate let model: Qwen3MoEModelInner\n    let configuration: Qwen3MoEConfiguration\n\n    @ModuleInfo(key: \"lm_head\") var lmHead: Linear?\n\n    public init(_ args: Qwen3MoEConfiguration) {\n        self.configuration = args\n        self.vocabularySize = args.vocabularySize\n        self.kvHeads = (0 ..< args.hiddenLayers).map { _ in args.kvHeads }\n        self.model = Qwen3MoEModelInner(args)\n\n        if !args.tieWordEmbeddings {\n            _lmHead.wrappedValue = Linear(args.hiddenSize, args.vocabularySize, bias: false)\n        }\n    }\n\n    public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]?) -> MLXArray {\n        var out = model(inputs, cache: cache)\n        if let lmHead {\n            out = lmHead(out)\n        } else {\n            out = model.embedTokens.asLinear(out)\n        }\n        return out\n    }\n\n    public func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {\n        var sanitizedWeights = weights\n\n        if configuration.tieWordEmbeddings {\n            sanitizedWeights[\"lm_head.weight\"] = nil\n        }\n\n        if sanitizedWeights[\"model.layers.0.mlp.experts.0.up_proj.weight\"] == nil {\n            return sanitizedWeights\n        }\n\n        for l in 0 ..< configuration.hiddenLayers {\n            let prefix = \"model.layers.\\(l)\"\n            for n in [\"up_proj\", \"down_proj\", \"gate_proj\"] {\n                if sanitizedWeights[\"\\(prefix).mlp.experts.0.\\(n).weight\"] != nil {\n                    let toJoin = (0 ..< configuration.numExperts).map { e in\n                        sanitizedWeights.removeValue(\n                            forKey: \"\\(prefix).mlp.experts.\\(e).\\(n).weight\")!\n                    }\n                    sanitizedWeights[\"\\(prefix).mlp.switch_mlp.\\(n).weight\"] = MLX.stacked(toJoin)\n                }\n            }\n        }\n\n        return sanitizedWeights\n    }\n}\n\npublic struct Qwen3MoEConfiguration: Codable, Sendable {\n    var modelType: String = \"qwen3_moe\"\n    var hiddenSize: Int\n    var hiddenLayers: Int\n    var intermediateSize: Int\n    var attentionHeads: Int\n    var numExperts: Int\n    var numExpertsPerToken: Int\n    var decoderSparseStep: Int\n    var mlpOnlyLayers: [Int]\n    var moeIntermediateSize: Int\n    var rmsNormEps: Float\n    var vocabularySize: Int\n    var kvHeads: Int\n    var headDim: Int\n    var ropeTheta: Float = 1_000_000\n    var tieWordEmbeddings: Bool = false\n    var maxPositionEmbeddings: Int = 32768\n    var normTopkProb: Bool = false\n    var ropeScaling: [String: StringOrNumber]? = nil\n\n    enum CodingKeys: String, CodingKey {\n        case modelType = \"model_type\"\n        case hiddenSize = \"hidden_size\"\n        case hiddenLayers = \"num_hidden_layers\"\n        case intermediateSize = \"intermediate_size\"\n        case attentionHeads = \"num_attention_heads\"\n        case numExperts = \"num_experts\"\n        case numExpertsPerToken = \"num_experts_per_tok\"\n        case decoderSparseStep = \"decoder_sparse_step\"\n        case mlpOnlyLayers = \"mlp_only_layers\"\n        case moeIntermediateSize = \"moe_intermediate_size\"\n        case rmsNormEps = \"rms_norm_eps\"\n        case vocabularySize = \"vocab_size\"\n        case kvHeads = \"num_key_value_heads\"\n        case headDim = \"head_dim\"\n        case ropeTheta = \"rope_theta\"\n        case tieWordEmbeddings = \"tie_word_embeddings\"\n        case maxPositionEmbeddings = \"max_position_embeddings\"\n        case normTopkProb = \"norm_topk_prob\"\n        case ropeScaling = \"rope_scaling\"\n    }\n\n    public init(from decoder: Decoder) throws {\n        let container = try decoder.container(keyedBy: CodingKeys.self)\n\n        self.modelType =\n            try container.decodeIfPresent(String.self, forKey: .modelType) ?? \"qwen3_moe\"\n        self.hiddenSize = try container.decode(Int.self, forKey: .hiddenSize)\n        self.hiddenLayers = try container.decode(Int.self, forKey: .hiddenLayers)\n        self.intermediateSize = try container.decode(Int.self, forKey: .intermediateSize)\n        self.attentionHeads = try container.decode(Int.self, forKey: .attentionHeads)\n        self.numExperts = try container.decode(Int.self, forKey: .numExperts)\n        self.numExpertsPerToken = try container.decode(Int.self, forKey: .numExpertsPerToken)\n        self.decoderSparseStep = try container.decode(Int.self, forKey: .decoderSparseStep)\n        self.mlpOnlyLayers = try container.decode([Int].self, forKey: .mlpOnlyLayers)\n        self.moeIntermediateSize = try container.decode(Int.self, forKey: .moeIntermediateSize)\n        self.rmsNormEps = try container.decode(Float.self, forKey: .rmsNormEps)\n        self.vocabularySize = try container.decode(Int.self, forKey: .vocabularySize)\n        self.kvHeads = try container.decode(Int.self, forKey: .kvHeads)\n        self.headDim = try container.decode(Int.self, forKey: .headDim)\n        self.ropeTheta = try container.decodeIfPresent(Float.self, forKey: .ropeTheta) ?? 1_000_000\n        self.tieWordEmbeddings =\n            try container.decodeIfPresent(Bool.self, forKey: .tieWordEmbeddings) ?? false\n        self.maxPositionEmbeddings =\n            try container.decodeIfPresent(Int.self, forKey: .maxPositionEmbeddings) ?? 32768\n        self.normTopkProb = try container.decodeIfPresent(Bool.self, forKey: .normTopkProb) ?? false\n        self.ropeScaling = try container.decodeIfPresent(\n            [String: StringOrNumber].self, forKey: .ropeScaling)\n    }\n}\n\n// MARK: - LoRA\n\nextension Qwen3MoEModel: LoRAModel {\n    public func loraLinearLayers() -> LoRALinearLayers {\n        model.layers.map { ($0.selfAttn, [\"q_proj\", \"v_proj\"]) }\n    }\n}"}]}
{"messages": [{"role": "user", "content": "convert the following MLX Python code into MLX Swift: \n\n# Copyright \u00a9 2023-2024 Apple Inc.\n\nfrom dataclasses import dataclass\nfrom typing import Any, Optional\n\nimport mlx.core as mx\nimport mlx.nn as nn\n\nfrom .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention\n\n\n@dataclass\nclass ModelArgs(BaseModelArgs):\n    model_type: str\n    hidden_size: int\n    num_hidden_layers: int\n    intermediate_size: int\n    num_attention_heads: int\n    num_key_value_heads: int\n    norm_epsilon: float = 1e-5\n    vocab_size: int = 49152\n    rope_theta: float = 100000\n    tie_word_embeddings: bool = True\n\n\nclass Attention(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.args = args\n\n        dim = args.hidden_size\n        self.n_heads = n_heads = args.num_attention_heads\n        self.n_kv_heads = n_kv_heads = args.num_key_value_heads\n\n        head_dim = args.hidden_size // args.num_attention_heads\n        self.scale = head_dim**-0.5\n\n        self.q_proj = nn.Linear(dim, n_heads * head_dim, bias=True)\n        self.k_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=True)\n        self.v_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=True)\n        self.o_proj = nn.Linear(n_heads * head_dim, dim, bias=True)\n        self.rope = nn.RoPE(head_dim, traditional=False, base=args.rope_theta)\n\n    def __call__(\n        self,\n        x: mx.array,\n        mask: Optional[mx.array] = None,\n        cache: Optional[Any] = None,\n    ) -> mx.array:\n        B, L, D = x.shape\n\n        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)\n\n        # Prepare the queries, keys and values for the attention computation\n        queries = queries.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)\n        keys = keys.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)\n        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)\n\n        if cache is not None:\n            queries = self.rope(queries, offset=cache.offset)\n            keys = self.rope(keys, offset=cache.offset)\n            keys, values = cache.update_and_fetch(keys, values)\n        else:\n            queries = self.rope(queries)\n            keys = self.rope(keys)\n\n        output = scaled_dot_product_attention(\n            queries, keys, values, cache=cache, scale=self.scale, mask=mask\n        )\n\n        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)\n        return self.o_proj(output)\n\n\nclass MLP(nn.Module):\n    def __init__(self, dim, hidden_dim):\n        super().__init__()\n        self.c_fc = nn.Linear(dim, hidden_dim, bias=True)\n        self.c_proj = nn.Linear(hidden_dim, dim, bias=True)\n\n    def __call__(self, x):\n        return self.c_proj(nn.gelu(self.c_fc(x)))\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.hidden_size = args.hidden_size\n        self.n_heads = args.num_attention_heads\n\n        self.self_attn = Attention(args)\n        self.mlp = MLP(args.hidden_size, args.intermediate_size)\n        self.input_layernorm = nn.LayerNorm(args.hidden_size, eps=args.norm_epsilon)\n        self.post_attention_layernorm = nn.LayerNorm(\n            args.hidden_size, eps=args.norm_epsilon\n        )\n        self.args = args\n\n    def __call__(\n        self,\n        x: mx.array,\n        mask: Optional[mx.array] = None,\n        cache: Optional[Any] = None,\n    ) -> mx.array:\n        r = self.self_attn(self.input_layernorm(x), mask, cache)\n        h = x + r\n        r = self.mlp(self.post_attention_layernorm(h))\n        out = h + r\n        return out\n\n\nclass Starcoder2Model(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.args = args\n        self.vocab_size = args.vocab_size\n        self.num_hidden_layers = args.num_hidden_layers\n        assert self.vocab_size > 0\n        self.embed_tokens = nn.Embedding(args.vocab_size, args.hidden_size)\n        self.layers = [\n            TransformerBlock(args=args) for _ in range(args.num_hidden_layers)\n        ]\n        self.norm = nn.LayerNorm(args.hidden_size, eps=args.norm_epsilon)\n\n    def __call__(\n        self,\n        inputs: mx.array,\n        mask: mx.array = None,\n        cache=None,\n    ):\n        h = self.embed_tokens(inputs)\n\n        if mask is None:\n            mask = create_attention_mask(h, cache)\n\n        if cache is None:\n            cache = [None] * len(self.layers)\n\n        for layer, c in zip(self.layers, cache):\n            h = layer(h, mask, c)\n\n        return self.norm(h)\n\n\nclass Model(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.args = args\n        self.model_type = args.model_type\n        self.model = Starcoder2Model(args)\n        if not args.tie_word_embeddings:\n            self.lm_head = nn.Linear(args.hidden_size, args.vocab_size, bias=False)\n\n    def __call__(\n        self,\n        inputs: mx.array,\n        mask: mx.array = None,\n        cache=None,\n    ):\n        out = self.model(inputs, mask, cache)\n        if self.args.tie_word_embeddings:\n            out = self.model.embed_tokens.as_linear(out)\n        else:\n            out = self.lm_head(out)\n        return out\n\n    @property\n    def layers(self):\n        return self.model.layers"}, {"role": "assistant", "content": "//\n//  Starcoder2.swift\n//  LLM\n//\n//  Created by John Mai on 2024/3/7.\n//\n\nimport Foundation\nimport MLX\nimport MLXLMCommon\nimport MLXNN\n\n// port of https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/models/starcoder2.py\n\nprivate class Attention: Module {\n    let args: Starcoder2Configuration\n    let scale: Float\n\n    @ModuleInfo(key: \"q_proj\") var wq: Linear\n    @ModuleInfo(key: \"k_proj\") var wk: Linear\n    @ModuleInfo(key: \"v_proj\") var wv: Linear\n    @ModuleInfo(key: \"o_proj\") var wo: Linear\n\n    let rope: RoPE\n\n    public init(_ args: Starcoder2Configuration) {\n        self.args = args\n\n        let dim = args.hiddenSize\n        let heads = args.attentionHeads\n        let kvHeads = args.kvHeads\n\n        let headDim = args.hiddenSize / heads\n        self.scale = pow(Float(headDim), -0.5)\n\n        _wq.wrappedValue = Linear(dim, heads * headDim, bias: true)\n        _wk.wrappedValue = Linear(dim, kvHeads * headDim, bias: true)\n        _wv.wrappedValue = Linear(dim, kvHeads * headDim, bias: true)\n        _wo.wrappedValue = Linear(heads * headDim, dim, bias: true)\n\n        self.rope = RoPE(dimensions: headDim, traditional: false, base: args.ropeTheta)\n    }\n\n    public func callAsFunction(\n        _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?\n    ) -> MLXArray {\n        let (B, L) = (x.dim(0), x.dim(1))\n\n        var queries = wq(x)\n        var keys = wk(x)\n        var values = wv(x)\n\n        // prepare the queries, keys and values for the attention computation\n        queries = queries.reshaped(B, L, args.attentionHeads, -1).transposed(0, 2, 1, 3)\n        keys = keys.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)\n        values = values.reshaped(B, L, args.kvHeads, -1).transposed(0, 2, 1, 3)\n\n        if let cache {\n            queries = rope(queries, offset: cache.offset)\n            keys = rope(keys, offset: cache.offset)\n        } else {\n            queries = rope(queries)\n            keys = rope(keys)\n        }\n\n        let output = attentionWithCacheUpdate(\n            queries: queries,\n            keys: keys,\n            values: values,\n            cache: cache,\n            scale: scale,\n            mask: mask\n        )\n        .transposed(0, 2, 1, 3)\n        .reshaped(B, L, -1)\n\n        return wo(output)\n    }\n}\n\nprivate class MLP: Module, UnaryLayer {\n    @ModuleInfo(key: \"c_fc\") var cFc: Linear\n    @ModuleInfo(key: \"c_proj\") var cProj: Linear\n\n    public init(dimensions: Int, hiddenDimensions: Int) {\n        _cFc.wrappedValue = Linear(dimensions, hiddenDimensions, bias: true)\n        _cProj.wrappedValue = Linear(hiddenDimensions, dimensions, bias: true)\n    }\n\n    public func callAsFunction(_ x: MLXArray) -> MLXArray {\n        cProj(gelu(cFc(x)))\n    }\n}\n\nprivate class TransformerBlock: Module {\n    @ModuleInfo(key: \"self_attn\") var attention: Attention\n    let mlp: MLP\n\n    @ModuleInfo(key: \"input_layernorm\") var inputLayerNorm: LayerNorm\n    @ModuleInfo(key: \"post_attention_layernorm\") var postAttentionLayerNorm: LayerNorm\n\n    public init(_ args: Starcoder2Configuration) {\n        _attention.wrappedValue = Attention(args)\n        self.mlp = MLP(dimensions: args.hiddenSize, hiddenDimensions: args.intermediateSize)\n        _inputLayerNorm.wrappedValue = LayerNorm(\n            dimensions: args.hiddenSize, eps: args.normEpsilon)\n        _postAttentionLayerNorm.wrappedValue = LayerNorm(\n            dimensions: args.hiddenSize, eps: args.normEpsilon)\n    }\n\n    public func callAsFunction(\n        _ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode, cache: KVCache?\n    ) -> MLXArray {\n        var r = attention(inputLayerNorm(x), mask: mask, cache: cache)\n        let h = x + r\n        r = mlp(postAttentionLayerNorm(h))\n        let out = h + r\n        return out\n    }\n}\n\nprivate class Starcoder2ModelInner: Module {\n    @ModuleInfo(key: \"embed_tokens\") var embedTokens: Embedding\n\n    fileprivate let layers: [TransformerBlock]\n    let norm: LayerNorm\n\n    public init(_ args: Starcoder2Configuration) {\n        precondition(args.vocabularySize > 0)\n\n        _embedTokens.wrappedValue = Embedding(\n            embeddingCount: args.vocabularySize, dimensions: args.hiddenSize)\n\n        self.layers = (0 ..< args.hiddenLayers)\n            .map { _ in\n                TransformerBlock(args)\n            }\n        self.norm = LayerNorm(dimensions: args.hiddenSize, eps: args.normEpsilon)\n    }\n\n    public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]? = nil) -> MLXArray {\n        var h = embedTokens(inputs)\n\n        let mask = createAttentionMask(h: h, cache: cache)\n\n        for (i, layer) in layers.enumerated() {\n            h = layer(h, mask: mask, cache: cache?[i])\n        }\n\n        return norm(h)\n    }\n}\n\npublic class Starcoder2Model: Module, LLMModel, KVCacheDimensionProvider {\n    public let vocabularySize: Int\n    public let kvHeads: [Int]\n\n    public let tieWordEmbeddings: Bool\n    private let model: Starcoder2ModelInner\n\n    @ModuleInfo(key: \"lm_head\") var lmHead: Linear\n\n    public init(_ args: Starcoder2Configuration) {\n        self.vocabularySize = args.vocabularySize\n        self.kvHeads = (0 ..< args.hiddenLayers).map { _ in args.kvHeads }\n        self.model = Starcoder2ModelInner(args)\n        self.tieWordEmbeddings = args.tieWordEmbeddings\n        if !self.tieWordEmbeddings {\n            _lmHead.wrappedValue = Linear(args.hiddenSize, args.vocabularySize, bias: false)\n        }\n    }\n\n    public func callAsFunction(_ inputs: MLXArray, cache: [KVCache]?) -> MLXArray {\n        var out = model(inputs, cache: cache)\n\n        if !tieWordEmbeddings {\n            return lmHead(out)\n        } else {\n            out = model.embedTokens.asLinear(out)\n            return out\n        }\n    }\n}\n\npublic struct Starcoder2Configuration: Codable, Sendable {\n    var hiddenSize: Int\n    var hiddenLayers: Int\n    var intermediateSize: Int\n    var attentionHeads: Int\n    var kvHeads: Int\n    var maxPositionEmbeddings: Int = 16384\n    var normEpsilon: Float = 1e-5\n    var normType: String = \"layer_norm\"\n    var vocabularySize: Int = 49152\n    var ropeTheta: Float = 100000\n    var tieWordEmbeddings: Bool = true\n\n    enum CodingKeys: String, CodingKey {\n        case hiddenSize = \"hidden_size\"\n        case hiddenLayers = \"num_hidden_layers\"\n        case intermediateSize = \"intermediate_size\"\n        case attentionHeads = \"num_attention_heads\"\n        case kvHeads = \"num_key_value_heads\"\n        case maxPositionEmbeddings = \"max_position_embeddings\"\n        case normEpsilon = \"norm_epsilon\"\n        case normType = \"norm_type\"\n        case vocabularySize = \"vocab_size\"\n        case ropeTheta = \"rope_theta\"\n        case tieWordEmbeddings = \"tie_word_embeddings\"\n    }\n\n    public init(from decoder: Decoder) throws {\n        // custom implementation to handle optional keys with required values\n        let container: KeyedDecodingContainer<Starcoder2Configuration.CodingKeys> =\n            try decoder.container(\n                keyedBy: Starcoder2Configuration.CodingKeys.self)\n\n        self.hiddenSize = try container.decode(\n            Int.self, forKey: Starcoder2Configuration.CodingKeys.hiddenSize)\n        self.hiddenLayers = try container.decode(\n            Int.self, forKey: Starcoder2Configuration.CodingKeys.hiddenLayers)\n        self.intermediateSize = try container.decode(\n            Int.self, forKey: Starcoder2Configuration.CodingKeys.intermediateSize)\n        self.attentionHeads = try container.decode(\n            Int.self, forKey: Starcoder2Configuration.CodingKeys.attentionHeads)\n        self.kvHeads = try container.decode(\n            Int.self, forKey: Starcoder2Configuration.CodingKeys.kvHeads)\n        self.maxPositionEmbeddings =\n            try container.decodeIfPresent(\n                Int.self, forKey: Starcoder2Configuration.CodingKeys.maxPositionEmbeddings) ?? 16384\n        self.normEpsilon =\n            try container.decodeIfPresent(\n                Float.self, forKey: Starcoder2Configuration.CodingKeys.normEpsilon) ?? 1e-5\n        self.normType =\n            try container.decodeIfPresent(\n                String.self, forKey: Starcoder2Configuration.CodingKeys.normType) ?? \"layer_norm\"\n        self.vocabularySize =\n            try container.decodeIfPresent(\n                Int.self, forKey: Starcoder2Configuration.CodingKeys.vocabularySize) ?? 49152\n        self.ropeTheta =\n            try container.decodeIfPresent(\n                Float.self, forKey: Starcoder2Configuration.CodingKeys.ropeTheta)\n            ?? 100000\n        self.tieWordEmbeddings =\n            try container.decodeIfPresent(\n                Bool.self, forKey: Starcoder2Configuration.CodingKeys.tieWordEmbeddings)\n            ?? true\n    }\n}\n\n// MARK: - LoRA\n\nextension Starcoder2Model: LoRAModel {\n    public func loraLinearLayers() -> LoRALinearLayers {\n        model.layers.map { ($0.attention, [\"q_proj\", \"v_proj\"]) }\n    }\n}"}]}
{"messages": [{"role": "user", "content": "convert the following MLX Python code into MLX Swift: \n\nimport glob\nimport inspect\nimport json\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import List, Optional\n\nimport mlx.core as mx\nimport mlx.nn as nn\nimport numpy as np\nfrom huggingface_hub import snapshot_download\n\nfrom .language import LanguageModel, RMSNorm, TextConfig\nfrom .vision import VisionConfig, VisionModel\n\n\n@dataclass\nclass ModelConfig:\n    text_config: TextConfig\n    vision_config: VisionConfig\n    model_type: str\n    vocab_size: int = 257152\n    ignore_index: int = -100\n    image_token_index: int = 262144\n    hidden_size: int = 2048\n    pad_token_id: int = 0\n    eos_token_id: Optional[List[int]] = None\n\n    @classmethod\n    def from_dict(cls, params):\n        return cls(\n            **{\n                k: v\n                for k, v in params.items()\n                if k in inspect.signature(cls).parameters\n            }\n        )\n\n\nclass Gemma3MultiModalProjector(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n        self.mm_input_projection_weight = mx.ones(\n            (config.vision_config.hidden_size, config.text_config.hidden_size)\n        )\n\n        self.mm_soft_emb_norm = RMSNorm(\n            config.vision_config.hidden_size, eps=config.vision_config.layer_norm_eps\n        )\n        self.patches_per_image = int(\n            config.vision_config.image_size // config.vision_config.patch_size\n        )\n        self.tokens_per_side = int(config.text_config.mm_tokens_per_image**0.5)\n        self.kernel_size = self.patches_per_image // self.tokens_per_side\n        self.avg_pool = nn.AvgPool2d(\n            kernel_size=self.kernel_size, stride=self.kernel_size\n        )\n\n    def __call__(self, x: mx.array) -> mx.array:\n        b, _, l = x.shape\n\n        reshaped_vision_outputs = x.transpose(0, 2, 1)\n        reshaped_vision_outputs = reshaped_vision_outputs.reshape(\n            b, l, self.patches_per_image, self.patches_per_image\n        )\n\n        # Transpose to place h, w in indices 1, 2\n        reshaped_vision_outputs = reshaped_vision_outputs.transpose(0, 2, 3, 1)\n        pooled_vision_outputs = self.avg_pool(reshaped_vision_outputs)\n        pooled_vision_outputs = pooled_vision_outputs.transpose(0, 3, 1, 2).flatten(2)\n        pooled_vision_outputs = pooled_vision_outputs.transpose(0, 2, 1)\n\n        normed_vision_outputs = self.mm_soft_emb_norm(pooled_vision_outputs)\n\n        projected_vision_outputs = mx.einsum(\n            \"btm,md->btd\", normed_vision_outputs, self.mm_input_projection_weight\n        )\n        return projected_vision_outputs.astype(x.dtype)\n\n\ndef masked_scatter(\n    final_embedding: mx.array,\n    image_mask_expanded: mx.array,\n    scaled_image_features: mx.array,\n):\n    # Reshape the tensors to 1D\n    final_embedding_shape = final_embedding.shape\n    scaled_image_features_flattened = mx.flatten(scaled_image_features)\n    final_embedding_flattened = mx.flatten(final_embedding)\n    image_mask_expanded_flattened = mx.flatten(image_mask_expanded)\n\n    # Scatter the scaled image features into the special image token positions\n    image_positions = mx.array(np.where(image_mask_expanded_flattened)[0], mx.uint32)\n    final_embedding_flattened[image_positions] = scaled_image_features_flattened\n\n    # Reshape back to the original shape\n    final_embedding = mx.reshape(final_embedding_flattened, final_embedding_shape)\n\n    return final_embedding\n\n\nclass Model(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n        self.model_type = config.model_type\n        self.config = config\n\n        self.vision_tower = VisionModel(config.vision_config)\n        self.language_model = LanguageModel(config.text_config)\n        self.multi_modal_projector = Gemma3MultiModalProjector(config)\n\n    def get_input_embeddings(\n        self,\n        input_ids: Optional[mx.array] = None,\n        pixel_values: Optional[mx.array] = None,\n        mask: Optional[mx.array] = None,\n    ):\n        if pixel_values is None:\n            return self.language_model.model.embed_tokens(input_ids), None\n\n        inputs_embeds = self.language_model.model.embed_tokens(input_ids)\n\n        hidden_state, _, _ = self.vision_tower(\n            pixel_values.transpose(0, 2, 3, 1).astype(inputs_embeds.dtype),\n            output_hidden_states=True,\n        )\n\n        image_features = self.multi_modal_projector(hidden_state)\n\n        final_inputs_embeds, final_attention_mask_4d = (\n            self.prepare_inputs_for_multimodal(\n                self.config.hidden_size,\n                self.config.pad_token_id,\n                self.config.image_token_index,\n                image_features,\n                inputs_embeds,\n                input_ids,\n                mask,\n            )\n        )\n        return final_inputs_embeds, final_attention_mask_4d\n\n    @staticmethod\n    def prepare_inputs_for_multimodal(\n        hidden_size,\n        pad_token_id,\n        image_token_index,\n        image_features,\n        inputs_embeds,\n        input_ids,\n        attention_mask,\n    ):\n        _, _, embed_dim = image_features.shape\n\n        batch_size, sequence_length = input_ids.shape\n        scaled_image_features = image_features / (hidden_size**0.5)\n        final_embedding = mx.zeros((batch_size, sequence_length, embed_dim))\n\n        pad_token_id = pad_token_id\n        pad_token_id = pad_token_id if pad_token_id is not None else 0\n        text_mask = (input_ids != image_token_index) & (input_ids != pad_token_id)\n        image_mask = input_ids == image_token_index\n        pad_mask = input_ids == pad_token_id\n\n        # expand masks to match embedding dimension\n        text_mask_expanded = mx.expand_dims(text_mask, -1)\n        text_mask_expanded = mx.repeat(text_mask_expanded, embed_dim, axis=-1)\n        pad_mask_expanded = mx.expand_dims(pad_mask, -1)\n        pad_mask_expanded = mx.repeat(pad_mask_expanded, embed_dim, axis=-1)\n        image_mask_expanded = mx.expand_dims(image_mask, -1)\n        image_mask_expanded = mx.repeat(image_mask_expanded, embed_dim, axis=-1)\n\n        # insert padding and text token embeddings\n        final_embedding = mx.where(text_mask_expanded, inputs_embeds, final_embedding)\n        final_embedding = mx.where(\n            pad_mask_expanded, mx.zeros_like(final_embedding), final_embedding\n        )\n\n        # insert image token embeddings\n        final_embedding = masked_scatter(\n            final_embedding, image_mask_expanded, scaled_image_features\n        )\n\n        attention_mask_expanded_1 = mx.expand_dims(attention_mask, 1)\n        attention_mask_expanded_2 = mx.expand_dims(attention_mask, 2)\n        final_attention_mask_4d = attention_mask_expanded_1 * attention_mask_expanded_2\n        final_attention_mask_4d = final_attention_mask_4d\n        final_attention_mask_4d = mx.expand_dims(final_attention_mask_4d, 1)\n        final_embedding = mx.array(final_embedding)\n        return final_embedding.astype(inputs_embeds.dtype), final_attention_mask_4d\n\n    def __call__(\n        self,\n        input_ids: mx.array,\n        pixel_values: mx.array,\n        mask: Optional[mx.array] = None,\n        cache: Optional[mx.array] = None,\n        **kwargs,\n    ):\n        input_embeddings, final_attention_mask_4d = self.get_input_embeddings(\n            input_ids, pixel_values, mask\n        )\n\n        logits = self.language_model(\n            inputs=input_ids,\n            cache=cache,\n            inputs_embeds=input_embeddings,\n        )\n        return logits"}, {"role": "assistant", "content": "import CoreImage\nimport MLX\nimport MLXFast\nimport MLXLMCommon\nimport MLXNN\nimport Tokenizers\n\n// Based on https://github.com/Blaizzy/mlx-vlm/tree/main/mlx_vlm/models/gemma3\n\n// MARK: - Text Configuration\n\npublic struct Gemma3TextConfiguration: Codable, Sendable {\n    public let modelType: String\n    public let hiddenSize: Int\n    public let hiddenLayers: Int\n    public let intermediateSize: Int\n    public let slidingWindow: Int\n    public let ropeScaling: [String: StringOrNumber]?\n    public let finalLogitSoftcapping: Float?\n\n    public let vocabularySize: Int = 262208\n    public let rmsNormEps: Float = 1.0e-6\n\n    // Decoded from JSON when present, with fallback if not\n\n    private let _attentionHeads: Int?\n    private let _kvHeads: Int?\n    private let _headDim: Int?\n    private let _queryPreAttnScalar: Float?\n\n    // Not included in 4B model config.json, included for 12B and 27B models\n    public var attentionHeads: Int {\n        _attentionHeads ?? 8\n    }\n\n    // Not included in 4B model config.json, included for 12B and 27B models\n    public var kvHeads: Int {\n        _kvHeads ?? 4\n    }\n\n    // Not included in 4B and 12B model config.json, included for 27B model\n    public var headDim: Int {\n        _headDim ?? 256\n    }\n\n    // Not included in 4B and 12B model config.json, included for 27B model\n    public var queryPreAttnScalar: Float {\n        _queryPreAttnScalar ?? 256\n    }\n\n    public let ropeGlobalBaseFreq: Float = 1_000_000.0\n    public let ropeLocalBaseFreq: Float = 10_000.0\n    public let ropeTraditional: Bool = false\n    public let mmTokensPerImage: Int = 256\n    public let slidingWindowPattern: Int = 6\n    public let maxPositionEmbeddings: Int = 4096\n\n    enum CodingKeys: String, CodingKey {\n        case modelType = \"model_type\"\n        case hiddenSize = \"hidden_size\"\n        case hiddenLayers = \"num_hidden_layers\"\n        case intermediateSize = \"intermediate_size\"\n        case slidingWindow = \"sliding_window\"\n        case ropeScaling = \"rope_scaling\"\n        case finalLogitSoftcapping = \"final_logit_softcapping\"\n        case _attentionHeads = \"num_attention_heads\"\n        case _kvHeads = \"num_key_value_heads\"\n        case _headDim = \"head_dim\"\n        case _queryPreAttnScalar = \"query_pre_attn_scalar\"\n    }\n}\n\n// MARK: - Vision Configuration\n\npublic struct Gemma3VisionConfiguration: Codable, Sendable {\n    public let modelType: String\n    public let hiddenLayers: Int\n    public let hiddenSize: Int\n    public let intermediateSize: Int\n    public let attentionHeads: Int\n    public let patchSize: Int\n    public let imageSize: Int\n\n    public let numChannels: Int = 3\n    public let layerNormEps: Float = 1e-6\n\n    enum CodingKeys: String, CodingKey {\n        case modelType = \"model_type\"\n        case hiddenLayers = \"num_hidden_layers\"\n        case hiddenSize = \"hidden_size\"\n        case intermediateSize = \"intermediate_size\"\n        case attentionHeads = \"num_attention_heads\"\n        case patchSize = \"patch_size\"\n        case imageSize = \"image_size\"\n    }\n}\n\n// MARK: - Quantization Configuration\n\npublic struct QuantizationConfig: Codable, Sendable {\n    public let groupSize: Int\n    public let bits: Int\n\n    enum CodingKeys: String, CodingKey {\n        case groupSize = \"group_size\"\n        case bits\n    }\n}\n\n// MARK: - Model Configuration\n\npublic struct Gemma3Configuration: Codable, Sendable {\n    public let textConfiguration: Gemma3TextConfiguration\n    public let visionConfiguration: Gemma3VisionConfiguration\n    public let modelType: String\n    public let mmTokensPerImage: Int\n    public let quantization: QuantizationConfig?\n\n    private let _vocabularySize: Int?\n    private let _padTokenId: Int?\n\n    // Computed properties that use the text configuration or provide defaults\n\n    public var vocabularySize: Int {\n        _vocabularySize ?? textConfiguration.vocabularySize\n    }\n\n    public var hiddenSize: Int {\n        textConfiguration.hiddenSize\n    }\n\n    public var padTokenId: Int {\n        _padTokenId ?? 0\n    }\n\n    enum CodingKeys: String, CodingKey {\n        case textConfiguration = \"text_config\"\n        case visionConfiguration = \"vision_config\"\n        case modelType = \"model_type\"\n        case mmTokensPerImage = \"mm_tokens_per_image\"\n        case quantization\n\n        case _vocabularySize = \"vocab_size\"\n        case _padTokenId = \"pad_token_id\"\n    }\n}\n\n// MARK: - Attention\n\nprivate class Attention: Module {\n    let numHeads: Int\n    let numKVHeads: Int\n    let repeats: Int\n    let headDim: Int\n    let layerIdx: Int\n    let scale: Float\n    let isSliding: Bool\n\n    @ModuleInfo(key: \"q_proj\") var queryProj: Linear\n    @ModuleInfo(key: \"k_proj\") var keyProj: Linear\n    @ModuleInfo(key: \"v_proj\") var valueProj: Linear\n    @ModuleInfo(key: \"o_proj\") var outputProj: Linear\n\n    @ModuleInfo(key: \"q_norm\") var queryNorm: Gemma.RMSNorm\n    @ModuleInfo(key: \"k_norm\") var keyNorm: Gemma.RMSNorm\n\n    @ModuleInfo var rope: RoPE\n\n    init(config: Gemma3TextConfiguration, layerIdx: Int) {\n        let dim = config.hiddenSize\n        self.numHeads = config.attentionHeads\n        self.numKVHeads = config.kvHeads\n        self.repeats = numHeads / numKVHeads\n        self.headDim = config.headDim\n        self.layerIdx = layerIdx\n\n        self.scale = pow(config.queryPreAttnScalar, -0.5)\n\n        self._queryProj.wrappedValue = Linear(dim, numHeads * headDim, bias: false)\n        self._keyProj.wrappedValue = Linear(dim, numKVHeads * headDim, bias: false)\n        self._valueProj.wrappedValue = Linear(dim, numKVHeads * headDim, bias: false)\n        self._outputProj.wrappedValue = Linear(numHeads * headDim, dim, bias: false)\n\n        self._queryNorm.wrappedValue = Gemma.RMSNorm(\n            dimensions: headDim, eps: config.rmsNormEps)\n        self._keyNorm.wrappedValue = Gemma.RMSNorm(dimensions: headDim, eps: config.rmsNormEps)\n\n        // Gemma3 uses sliding window attention pattern\n        self.isSliding = (layerIdx + 1) % config.slidingWindowPattern != 0\n\n        let baseFreq = isSliding ? config.ropeLocalBaseFreq : config.ropeGlobalBaseFreq\n        self._rope.wrappedValue = RoPE(\n            dimensions: headDim,\n            traditional: config.ropeTraditional,\n            base: baseFreq\n        )\n    }\n\n    func callAsFunction(\n        _ x: MLXArray,\n        mask: MLXFast.ScaledDotProductAttentionMaskMode,\n        cache: KVCache? = nil\n    ) -> MLXArray {\n        let (B, L, _) = (x.dim(0), x.dim(1), x.dim(2))\n\n        var queries = queryProj(x)\n        var keys = keyProj(x)\n        var values = valueProj(x)\n\n        // Reshape for multi-head attention\n        queries = queries.reshaped(B, L, numHeads, -1).transposed(0, 2, 1, 3)\n        keys = keys.reshaped(B, L, numKVHeads, -1).transposed(0, 2, 1, 3)\n        values = values.reshaped(B, L, numKVHeads, -1).transposed(0, 2, 1, 3)\n\n        // Apply normalization\n        queries = queryNorm(queries)\n        keys = keyNorm(keys)\n\n        // Apply rotary position embedding\n        if let cache {\n            queries = rope(queries, offset: cache.offset)\n            keys = rope(keys, offset: cache.offset)\n        } else {\n            queries = rope(queries)\n            keys = rope(keys)\n        }\n\n        // Handle sliding window masking\n        var finalMask = mask\n        if case .array(let maskArray) = mask, maskArray.shape.last! != keys.shape[2] {\n            let keyLen = keys.shape[2]\n            let slicedMask = maskArray[.ellipsis, (-keyLen)...]\n            finalMask = .array(slicedMask)\n        }\n\n        // Scaled dot-product attention with native GQA support\n        let output = attentionWithCacheUpdate(\n            queries: queries,\n            keys: keys,\n            values: values,\n            cache: cache,\n            scale: scale,\n            mask: finalMask\n        )\n        .transposed(0, 2, 1, 3)\n        .reshaped(B, L, -1)\n\n        return outputProj(output)\n    }\n}\n\n// MARK: - MLP\n\nprivate class MLP: Module, UnaryLayer {\n    @ModuleInfo(key: \"gate_proj\") var gateProj: Linear\n    @ModuleInfo(key: \"down_proj\") var downProj: Linear\n    @ModuleInfo(key: \"up_proj\") var upProj: Linear\n\n    init(dimensions: Int, hiddenDimensions: Int) {\n        self._gateProj.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)\n        self._downProj.wrappedValue = Linear(hiddenDimensions, dimensions, bias: false)\n        self._upProj.wrappedValue = Linear(dimensions, hiddenDimensions, bias: false)\n    }\n\n    func callAsFunction(_ x: MLXArray) -> MLXArray {\n        downProj(geluApproximate(gateProj(x)) * upProj(x))\n    }\n}\n\n// MARK: - TransformerBlock\n\nprivate class TransformerBlock: Module {\n    @ModuleInfo(key: \"self_attn\") var selfAttention: Attention\n    @ModuleInfo var mlp: MLP\n    @ModuleInfo(key: \"input_layernorm\") var inputLayerNorm: Gemma.RMSNorm\n    @ModuleInfo(key: \"post_attention_layernorm\") var postAttentionLayerNorm: Gemma.RMSNorm\n    @ModuleInfo(key: \"pre_feedforward_layernorm\") var preFeedforwardLayerNorm: Gemma.RMSNorm\n    @ModuleInfo(key: \"post_feedforward_layernorm\") var postFeedforwardLayerNorm: Gemma.RMSNorm\n\n    let numAttentionHeads: Int\n    let hiddenSize: Int\n\n    init(config: Gemma3TextConfiguration, layerIdx: Int) {\n        self.numAttentionHeads = config.attentionHeads\n        self.hiddenSize = config.hiddenSize\n\n        self._selfAttention.wrappedValue = Attention(config: config, layerIdx: layerIdx)\n        self.mlp = MLP(dimensions: config.hiddenSize, hiddenDimensions: config.intermediateSize)\n\n        self._inputLayerNorm.wrappedValue = Gemma.RMSNorm(\n            dimensions: config.hiddenSize, eps: config.rmsNormEps)\n        self._postAttentionLayerNorm.wrappedValue = Gemma.RMSNorm(\n            dimensions: config.hiddenSize, eps: config.rmsNormEps)\n        self._preFeedforwardLayerNorm.wrappedValue = Gemma.RMSNorm(\n            dimensions: config.hiddenSize, eps: config.rmsNormEps)\n        self._postFeedforwardLayerNorm.wrappedValue = Gemma.RMSNorm(\n            dimensions: config.hiddenSize, eps: config.rmsNormEps)\n    }\n\n    func callAsFunction(\n        _ x: MLXArray,\n        mask: MLXFast.ScaledDotProductAttentionMaskMode,\n        cache: KVCache? = nil\n    ) -> MLXArray {\n        let r = selfAttention(inputLayerNorm(x), mask: mask, cache: cache)\n        let h = Gemma.clipResidual(x, postAttentionLayerNorm(r))\n        let r2 = mlp(preFeedforwardLayerNorm(h))\n        let out = Gemma.clipResidual(h, postFeedforwardLayerNorm(r2))\n        return out\n    }\n}\n\n// MARK: - GemmaModel\n\nprivate class GemmaModel: Module {\n    @ModuleInfo(key: \"embed_tokens\") var embedTokens: Embedding\n    @ModuleInfo var layers: [TransformerBlock]\n    @ModuleInfo var norm: Gemma.RMSNorm\n\n    let config: Gemma3TextConfiguration\n\n    init(_ config: Gemma3TextConfiguration) {\n        self.config = config\n\n        self._embedTokens.wrappedValue = Embedding(\n            embeddingCount: config.vocabularySize,\n            dimensions: config.hiddenSize\n        )\n\n        self._layers.wrappedValue = (0 ..< config.hiddenLayers).map { layerIdx in\n            TransformerBlock(config: config, layerIdx: layerIdx)\n        }\n\n        self.norm = Gemma.RMSNorm(dimensions: config.hiddenSize, eps: config.rmsNormEps)\n    }\n\n    func callAsFunction(\n        _ inputs: MLXArray? = nil,\n        inputEmbedding: MLXArray? = nil,\n        mask: MLXFast.ScaledDotProductAttentionMaskMode? = nil,\n        cache: [KVCache?]? = nil\n    ) -> MLXArray {\n        var h: MLXArray\n        if let inputEmbedding = inputEmbedding {\n            h = inputEmbedding\n        } else if let inputs = inputs {\n            h = embedTokens(inputs)\n        } else {\n            fatalError(\"Either inputs or inputEmbedding must be provided\")\n        }\n\n        // Apply embedding scaling\n        let scale = MLXArray(sqrtf(Float(config.hiddenSize)), dtype: .bfloat16).asType(\n            inputs?.dtype ?? h.dtype)\n        h = h * scale\n\n        var layerCache = cache\n        if layerCache == nil {\n            layerCache = Array(repeating: nil as KVCache?, count: layers.count)\n        }\n\n        // Create attention masks for global and sliding window layers\n        var fullMask: MLXFast.ScaledDotProductAttentionMaskMode = .none\n        var slidingWindowMask: MLXFast.ScaledDotProductAttentionMaskMode = .none\n\n        if mask == nil {\n            let j = config.slidingWindowPattern\n            if j > 0 && j <= layerCache!.count {\n                let globalCacheSlice = layerCache![(j - 1) ..< j].compactMap { $0 }\n                fullMask = createAttentionMask(h: h, cache: globalCacheSlice)\n            }\n            slidingWindowMask = createAttentionMask(h: h, cache: layerCache?.compactMap { $0 })\n        }\n\n        for (i, layer) in layers.enumerated() {\n            let isGlobal = (i % config.slidingWindowPattern == config.slidingWindowPattern - 1)\n\n            let localMask: MLXFast.ScaledDotProductAttentionMaskMode\n            if let mask {\n                localMask = mask\n            } else if isGlobal {\n                localMask = fullMask\n            } else {\n                localMask = slidingWindowMask\n            }\n\n            h = layer(h, mask: localMask, cache: layerCache?[i])\n        }\n\n        return norm(h)\n    }\n}\n\n// MARK: - LanguageModel\n\nprivate class LanguageModel: Module, KVCacheDimensionProvider {\n    @ModuleInfo var model: GemmaModel\n    @ModuleInfo(key: \"lm_head\") var lmHead: Module  // Can be Linear or QuantizedLinear\n\n    let config: Gemma3TextConfiguration\n    var kvHeads: [Int]\n\n    init(_ config: Gemma3TextConfiguration) {\n        self.config = config\n        self.model = GemmaModel(config)\n        self._lmHead.wrappedValue = Linear(config.hiddenSize, config.vocabularySize, bias: false)\n        self.kvHeads = Array(repeating: config.kvHeads, count: config.hiddenLayers)\n    }\n\n    /// Creates appropriate cache types for each layer\n    public func newCache(parameters: GenerateParameters?) -> [any KVCache] {\n        var caches: [any KVCache] = []\n        let slidingWindow = config.slidingWindow > 0 ? config.slidingWindow : 4096\n        let slidingWindowPattern = config.slidingWindowPattern\n        for i in 0 ..< config.hiddenLayers {\n            let isGlobalLayer = (i % slidingWindowPattern == slidingWindowPattern - 1)\n            if isGlobalLayer {\n                caches.append(StandardKVCache())\n            } else {\n                caches.append(RotatingKVCache(maxSize: slidingWindow, keep: 0))\n            }\n        }\n        return caches\n    }\n\n    func callAsFunction(\n        _ inputs: MLXArray? = nil,\n        cache: [KVCache]? = nil,\n        inputEmbedding: MLXArray? = nil,\n        mask: MLXFast.ScaledDotProductAttentionMaskMode? = nil\n    ) -> LMOutput {\n        let optionalCache = cache?.map { $0 as KVCache? }\n        let out = model(inputs, inputEmbedding: inputEmbedding, mask: mask, cache: optionalCache)\n\n        // Call the lmHead (works whether it's Linear or QuantizedLinear)\n        var finalLogits: MLXArray\n        if let linear = lmHead as? Linear {\n            finalLogits = linear(out)\n        } else if let quantized = lmHead as? QuantizedLinear {\n            finalLogits = quantized(out)\n        } else {\n            fatalError(\"lmHead must be Linear or QuantizedLinear\")\n        }\n\n        // Apply final logit softcapping if configured\n        if let softcap = config.finalLogitSoftcapping, softcap > 0 {\n            let scale = MLXArray(softcap)\n            finalLogits = tanh(finalLogits / scale) * scale\n        }\n\n        return LMOutput(logits: finalLogits)\n    }\n\n    func sanitize(weights: [String: MLXArray], quantizationConfig: QuantizationConfig? = nil)\n        -> [String: MLXArray]\n    {\n        var processedWeights = weights\n\n        // Check if we have quantized weights\n        let hasQuantizedLmHead = hasQuantizedWeights(\n            layerPath: \"language_model.lm_head\", in: weights)\n\n        if hasQuantizedLmHead {\n            // Use quantization config from model configuration if available\n            let groupSize = quantizationConfig?.groupSize ?? 64\n            let bits = quantizationConfig?.bits ?? 4\n\n            // Only quantize layers that actually have quantized weights\n            quantize(model: self) { path, module in\n                // Check each specific layer path for quantized weights\n                let fullPath = \"language_model.\\(path)\"\n                if weights[\"\\(fullPath).scales\"] != nil && weights[\"\\(fullPath).biases\"] != nil\n                    && weights[\"\\(fullPath).weight\"]?.dtype == .uint32\n                {\n                    return (groupSize, bits)\n                }\n                return nil\n            }\n        } else {\n            // Handle weight tying for regular (non-quantized) lm_head\n            if processedWeights[\"language_model.lm_head.weight\"] == nil {\n                if let embedWeight = processedWeights[\"language_model.model.embed_tokens.weight\"] {\n                    processedWeights[\"language_model.lm_head.weight\"] = embedWeight\n                }\n            }\n        }\n\n        // Remove unused precomputed rotary freqs\n        return processedWeights.filter { key, _ in\n            !key.contains(\"self_attn.rotary_emb.inv_freq\")\n        }\n    }\n\n    /// Check if a layer has quantized weights\n    private func hasQuantizedWeights(layerPath: String, in weights: [String: MLXArray]) -> Bool {\n        let scalesKey = \"\\(layerPath).scales\"\n        let biasesKey = \"\\(layerPath).biases\"\n        let weightKey = \"\\(layerPath).weight\"\n\n        let hasScales = weights[scalesKey] != nil\n        let hasBiases = weights[biasesKey] != nil\n        let hasWeight = weights[weightKey]?.dtype == .uint32\n\n        return hasScales && hasBiases && hasWeight\n    }\n}\n\n// MARK: - Vision Model Components\n\nprivate class VisionAttention: Module {\n    @ModuleInfo(key: \"q_proj\") var queryProj: Linear\n    @ModuleInfo(key: \"k_proj\") var keyProj: Linear\n    @ModuleInfo(key: \"v_proj\") var valueProj: Linear\n    @ModuleInfo(key: \"out_proj\") var outputProj: Linear\n\n    let numHeads: Int\n    let scale: Float\n\n    init(\n        dimensions: Int,\n        numHeads: Int,\n        queryInputDimensions: Int? = nil,\n        keyInputDimensions: Int? = nil,\n        valueInputDimensions: Int? = nil,\n        valueDimensions: Int? = nil,\n        valueOutputDimensions: Int? = nil,\n        bias: Bool = true\n    ) {\n        if dimensions % numHeads != 0 {\n            fatalError(\"The input feature dimensions should be divisible by the number of heads\")\n        }\n\n        self.numHeads = numHeads\n        let headDim = dimensions / numHeads\n        self.scale = pow(Float(headDim), -0.5)\n\n        let queryInputDims = queryInputDimensions ?? dimensions\n        let keyInputDims = keyInputDimensions ?? dimensions\n        let valueInputDims = valueInputDimensions ?? keyInputDims\n        let valueDims = valueDimensions ?? dimensions\n        let valueOutputDims = valueOutputDimensions ?? dimensions\n\n        self._queryProj.wrappedValue = Linear(queryInputDims, dimensions, bias: bias)\n        self._keyProj.wrappedValue = Linear(keyInputDims, dimensions, bias: bias)\n        self._valueProj.wrappedValue = Linear(valueInputDims, valueDims, bias: bias)\n        self._outputProj.wrappedValue = Linear(valueDims, valueOutputDims, bias: bias)\n    }\n\n    func callAsFunction(_ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode = .none)\n        -> MLXArray\n    {\n        var queries = queryProj(x)\n        var keys = keyProj(x)\n        var values = valueProj(x)\n\n        let (B, L, _) = (queries.dim(0), queries.dim(1), queries.dim(2))\n        let S = keys.dim(1)\n\n        queries = queries.reshaped(B, L, numHeads, -1).transposed(0, 2, 1, 3)\n        keys = keys.reshaped(B, S, numHeads, -1).transposed(0, 2, 1, 3)\n        values = values.reshaped(B, S, numHeads, -1).transposed(0, 2, 1, 3)\n\n        let output = MLXFast.scaledDotProductAttention(\n            queries: queries,\n            keys: keys,\n            values: values,\n            scale: scale,\n            mask: mask\n        )\n        .transposed(0, 2, 1, 3)\n        .reshaped(B, L, -1)\n\n        return outputProj(output)\n    }\n}\n\nprivate class VisionMLP: Module, UnaryLayer {\n    @ModuleInfo(key: \"fc1\") var fc1: Linear\n    @ModuleInfo(key: \"fc2\") var fc2: Linear\n    @ModuleInfo var activationFn: GELU\n\n    init(config: Gemma3VisionConfiguration) {\n        self.activationFn = GELU(approximation: .precise)\n        self._fc1.wrappedValue = Linear(config.hiddenSize, config.intermediateSize, bias: true)\n        self._fc2.wrappedValue = Linear(config.intermediateSize, config.hiddenSize, bias: true)\n    }\n\n    func callAsFunction(_ x: MLXArray) -> MLXArray {\n        var x = fc1(x)\n        x = activationFn(x)\n        return fc2(x)\n    }\n}\n\nprivate class EncoderLayer: Module {\n    @ModuleInfo(key: \"self_attn\") var selfAttention: VisionAttention\n    @ModuleInfo(key: \"layer_norm1\") var layerNorm1: LayerNorm\n    @ModuleInfo var mlp: VisionMLP\n    @ModuleInfo(key: \"layer_norm2\") var layerNorm2: LayerNorm\n\n    let embedDim: Int\n\n    init(config: Gemma3VisionConfiguration) {\n        self.embedDim = config.hiddenSize\n\n        self._selfAttention.wrappedValue = VisionAttention(\n            dimensions: config.hiddenSize,\n            numHeads: config.attentionHeads,\n            bias: true\n        )\n\n        self._layerNorm1.wrappedValue = LayerNorm(dimensions: embedDim, eps: config.layerNormEps)\n        self.mlp = VisionMLP(config: config)\n        self._layerNorm2.wrappedValue = LayerNorm(dimensions: embedDim, eps: config.layerNormEps)\n    }\n\n    func callAsFunction(_ x: MLXArray, mask: MLXFast.ScaledDotProductAttentionMaskMode = .none)\n        -> MLXArray\n    {\n        let r = selfAttention(layerNorm1(x), mask: mask)\n        let h = x + r\n        let r2 = mlp(layerNorm2(h))\n        return h + r2\n    }\n}\n\nprivate class Encoder: Module {\n    @ModuleInfo var layers: [EncoderLayer]\n\n    init(config: Gemma3VisionConfiguration) {\n        self._layers.wrappedValue = (0 ..< config.hiddenLayers).map { _ in\n            EncoderLayer(config: config)\n        }\n    }\n\n    func callAsFunction(\n        _ x: MLXArray,\n        outputHiddenStates: Bool = false,\n        mask: MLXFast.ScaledDotProductAttentionMaskMode = .none\n    ) -> (MLXArray, [MLXArray]?) {\n        var encoderStates: [MLXArray]? = outputHiddenStates ? [x] : nil\n        var h = x\n\n        for layer in layers {\n            h = layer(h, mask: mask)\n            if outputHiddenStates {\n                encoderStates?.append(h)\n            }\n        }\n\n        return (h, encoderStates)\n    }\n}\n\nprivate class VisionEmbeddings: Module, UnaryLayer {\n    @ModuleInfo(key: \"patch_embedding\") var patchEmbedding: Conv2d\n    @ModuleInfo(key: \"position_embedding\") var positionEmbedding: Embedding\n\n    let config: Gemma3VisionConfiguration\n    let embedDim: Int\n    let imageSize: Int\n    let patchSize: Int\n    let numPatches: Int\n    let numPositions: Int\n\n    init(config: Gemma3VisionConfiguration) {\n        self.config = config\n        self.embedDim = config.hiddenSize\n        self.imageSize = config.imageSize\n        self.patchSize = config.patchSize\n\n        self._patchEmbedding.wrappedValue = Conv2d(\n            inputChannels: config.numChannels,\n            outputChannels: embedDim,\n            kernelSize: IntOrPair(patchSize),\n            stride: IntOrPair(patchSize)\n        )\n\n        self.numPatches = (imageSize / patchSize) * (imageSize / patchSize)\n        self.numPositions = numPatches\n\n        self._positionEmbedding.wrappedValue = Embedding(\n            embeddingCount: numPositions,\n            dimensions: embedDim\n        )\n    }\n\n    func callAsFunction(_ x: MLXArray) -> MLXArray {\n        var patchEmbeddings = patchEmbedding(x)\n        patchEmbeddings = flattened(patchEmbeddings, start: 1, end: 2)\n\n        // Check if we have the expected number of patches (safety net for config mismatches)\n        let actualNumPatches = patchEmbeddings.dim(1)\n        let useNumPositions = min(actualNumPatches, numPositions)\n\n        // Use position IDs from 0 to numPositions\n        let positionIds = MLXArray(Array(0 ..< useNumPositions))[.newAxis, 0...]\n        var embeddings = patchEmbeddings\n\n        // Add position embeddings only to the patches we have positions for\n        if useNumPositions == actualNumPatches {\n            // Normal case: add position embeddings to all patches\n            embeddings = embeddings + positionEmbedding(positionIds)\n        } else {\n            // Safety case: only add to first N patches to avoid broadcast error\n            let positionedPatches =\n                embeddings[0..., ..<useNumPositions, 0...] + positionEmbedding(positionIds)\n            let remainingPatches = embeddings[0..., useNumPositions..., 0...]\n            embeddings = concatenated([positionedPatches, remainingPatches], axis: 1)\n        }\n\n        return embeddings\n    }\n}\n\nprivate class SigLipVisionModel: Module {\n    @ModuleInfo var embeddings: VisionEmbeddings\n    @ModuleInfo var encoder: Encoder\n    @ModuleInfo(key: \"post_layernorm\") var postLayerNorm: LayerNorm\n\n    init(config: Gemma3VisionConfiguration) {\n        self.embeddings = VisionEmbeddings(config: config)\n        self.encoder = Encoder(config: config)\n        self._postLayerNorm.wrappedValue = LayerNorm(dimensions: config.hiddenSize)\n        super.init()\n    }\n\n    func callAsFunction(\n        _ x: MLXArray,\n        outputHiddenStates: Bool = false\n    ) -> (MLXArray, MLXArray, [MLXArray]?) {\n        let x = embeddings(x)\n\n        let (encoderOutput, encoderStates) = encoder(\n            x,\n            outputHiddenStates: outputHiddenStates,\n            mask: .none\n        )\n\n        let poolerOutput = postLayerNorm(encoderOutput)\n\n        return (poolerOutput, x, encoderStates)\n    }\n}\n\nprivate class VisionModel: Module {\n    @ModuleInfo(key: \"vision_model\") var visionModel: SigLipVisionModel\n\n    let modelType: String\n\n    init(config: Gemma3VisionConfiguration) {\n        self.modelType = config.modelType\n        self._visionModel.wrappedValue = SigLipVisionModel(config: config)\n    }\n\n    func callAsFunction(\n        _ x: MLXArray,\n        outputHiddenStates: Bool = false\n    ) -> (MLXArray, MLXArray, [MLXArray]?) {\n        visionModel(x, outputHiddenStates: outputHiddenStates)\n    }\n\n    /// Check if array is already in MLX format for conv2d weights\n    private func checkArrayShape(_ arr: MLXArray) -> Bool {\n        let shape = arr.shape\n\n        // Check if the shape has 4 dimensions\n        guard shape.count == 4 else { return false }\n\n        let (outChannels, kH, kW, _) = (shape[0], shape[1], shape[2], shape[3])\n\n        // Check if out_channels is the largest, and kH and kW are the same\n        return (outChannels >= kH) && (outChannels >= kW) && (kH == kW)\n    }\n\n    func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {\n        var sanitizedWeights = [String: MLXArray]()\n\n        for (k, v) in weights {\n            // Handle vision model quantized weights if they exist\n            if k.contains(\"vision_tower\") && hasQuantizedWeights(layerPath: k, in: weights) {\n                // Keep quantized weights as-is - they will be handled by QuantizedLinear at runtime\n                sanitizedWeights[k] = v\n            } else if k.contains(\"patch_embedding.weight\") {\n                // PyTorch conv2d weight tensors have shape:\n                //   [out_channels, in_channels, kH, KW]\n                // MLX conv2d expects the weight be of shape:\n                //   [out_channels, kH, KW, in_channels]\n                if checkArrayShape(v) {\n                    sanitizedWeights[k] = v\n                } else {\n                    sanitizedWeights[k] = v.transposed(0, 2, 3, 1)\n                }\n            } else {\n                sanitizedWeights[k] = v\n            }\n        }\n\n        return sanitizedWeights\n    }\n\n    /// Check if a layer has quantized weights (copied from LanguageModel)\n    private func hasQuantizedWeights(layerPath: String, in weights: [String: MLXArray]) -> Bool {\n        let scalesKey = \"\\(layerPath).scales\"\n        let biasesKey = \"\\(layerPath).biases\"\n        let weightKey = \"\\(layerPath).weight\"\n\n        return weights[scalesKey] != nil && weights[biasesKey] != nil\n            && weights[weightKey]?.dtype == .uint32\n    }\n}\n\n// MARK: - Multimodal Projector\n\nclass Gemma3MultiModalProjector: Module, UnaryLayer {\n    @ModuleInfo(key: \"mm_input_projection_weight\") var mmInputProjectionWeight: MLXArray\n    @ModuleInfo(key: \"mm_soft_emb_norm\") var mmSoftEmbNorm: Gemma.RMSNorm\n    @ModuleInfo var avgPool: AvgPool2d\n\n    let config: Gemma3Configuration\n    let patchesPerImage: Int\n    let tokensPerSide: Int\n    let kernelSize: Int\n\n    init(config: Gemma3Configuration) {\n        self.config = config\n\n        self._mmInputProjectionWeight.wrappedValue = ones([\n            config.visionConfiguration.hiddenSize,\n            config.textConfiguration.hiddenSize,\n        ])\n\n        self._mmSoftEmbNorm.wrappedValue = Gemma.RMSNorm(\n            dimensions: config.visionConfiguration.hiddenSize,\n            eps: config.visionConfiguration.layerNormEps\n        )\n\n        self.patchesPerImage =\n            config.visionConfiguration.imageSize / config.visionConfiguration.patchSize\n\n        self.tokensPerSide = Int(sqrt(Double(config.mmTokensPerImage)))\n        self.kernelSize = patchesPerImage / tokensPerSide\n\n        self.avgPool = AvgPool2d(\n            kernelSize: IntOrPair(kernelSize),\n            stride: IntOrPair(kernelSize)\n        )\n    }\n\n    func callAsFunction(_ x: MLXArray) -> MLXArray {\n        let (b, _, l) = (x.dim(0), x.dim(1), x.dim(2))\n\n        // Use fixed config values\n        var reshapedVisionOutputs = x.transposed(0, 2, 1)\n        reshapedVisionOutputs = reshapedVisionOutputs.reshaped(\n            b, l, patchesPerImage, patchesPerImage\n        )\n\n        // Transpose to place spatial dimensions in indices 1, 2\n        reshapedVisionOutputs = reshapedVisionOutputs.transposed(0, 2, 3, 1)\n        // Use fixed average pooling\n        var pooledVisionOutputs = avgPool(reshapedVisionOutputs)\n        pooledVisionOutputs = pooledVisionOutputs.transposed(0, 3, 1, 2).flattened(start: 2)\n        pooledVisionOutputs = pooledVisionOutputs.transposed(0, 2, 1)\n\n        let normedVisionOutputs = mmSoftEmbNorm(pooledVisionOutputs)\n\n        let projectedVisionOutputs = einsum(\n            \"btm,md->btd\",\n            normedVisionOutputs,\n            mmInputProjectionWeight\n        )\n\n        return projectedVisionOutputs.asType(x.dtype)\n    }\n}\n\n/// Inserts image features into text embeddings at specified token positions\n/// Implements the multimodal fusion approach used in Gemma3 VLM\nprivate func maskedScatter(\n    finalEmbedding: MLXArray,\n    imageMaskExpanded: MLXArray,\n    scaledImageFeatures: MLXArray\n) -> MLXArray {\n    // Reshape the tensors to 1D\n    let finalEmbeddingShape = finalEmbedding.shape\n    let scaledImageFeaturesFlattened = scaledImageFeatures.flattened()\n    let finalEmbeddingFlattened = finalEmbedding.flattened()\n    let imageMaskExpandedFlattened = imageMaskExpanded.flattened()\n\n    let maskValues = imageMaskExpandedFlattened.asArray(Bool.self)\n    let imagePositionIndices = maskValues.enumerated().compactMap { index, value in\n        value ? UInt32(index) : nil\n    }\n\n    guard !imagePositionIndices.isEmpty else {\n        return finalEmbedding\n    }\n\n    // Scatter the scaled image features into the special image token positions\n    let imagePositions = MLXArray(imagePositionIndices)\n    guard scaledImageFeaturesFlattened.shape[0] == imagePositions.shape[0] else {\n        fatalError(\n            \"\"\"\n            Critical error in maskedScatter: Size mismatch between image features and positions.\n            Image features: \\(scaledImageFeaturesFlattened.shape[0])\n            Image positions: \\(imagePositions.shape[0])\n            \"\"\")\n    }\n    finalEmbeddingFlattened[imagePositions] = scaledImageFeaturesFlattened\n    return finalEmbeddingFlattened.reshaped(finalEmbeddingShape)\n}\n\n// MARK: - Gemma 3 Model\n\npublic class Gemma3: Module, VLMModel, KVCacheDimensionProvider {\n    @ModuleInfo(key: \"vision_tower\") private var visionTower: VisionModel\n    @ModuleInfo(key: \"language_model\") private var languageModel: LanguageModel\n    @ModuleInfo(key: \"multi_modal_projector\") var multiModalProjector: Gemma3MultiModalProjector\n\n    public let config: Gemma3Configuration\n\n    public var vocabularySize: Int { config.vocabularySize }\n    public var kvHeads: [Int] { languageModel.kvHeads }\n\n    /// Create cache with proper types for each layer\n    public func newCache(parameters: GenerateParameters?) -> [any KVCache] {\n        return languageModel.newCache(parameters: parameters)\n    }\n\n    public init(_ config: Gemma3Configuration) {\n        self.config = config\n\n        self._visionTower.wrappedValue = VisionModel(config: config.visionConfiguration)\n        self._languageModel.wrappedValue = LanguageModel(config.textConfiguration)\n        self._multiModalProjector.wrappedValue = Gemma3MultiModalProjector(config: config)\n    }\n\n    private func getInputEmbeddings(\n        inputIds: MLXArray? = nil,\n        pixelValues: MLXArray? = nil,\n        mask: MLXArray? = nil\n    ) -> (MLXArray, MLXArray?) {\n        guard let pixelValues else {\n            return (languageModel.model.embedTokens(inputIds!), nil)\n        }\n\n        let inputsEmbeds = languageModel.model.embedTokens(inputIds!)\n\n        // Process image through vision tower\n        let processedPixels = pixelValues.transposed(0, 2, 3, 1).asType(inputsEmbeds.dtype)\n\n        let (hiddenState, _, _) = visionTower(\n            processedPixels,\n            outputHiddenStates: true\n        )\n\n        let imageFeatures = multiModalProjector(hiddenState)\n\n        let (finalEmbedding, finalAttentionMask4d) = prepareInputsForMultimodal(\n            imageFeatures: imageFeatures,\n            inputsEmbeds: inputsEmbeds,\n            inputIds: inputIds!,\n            attentionMask: mask\n        )\n\n        return (finalEmbedding, finalAttentionMask4d)\n    }\n\n    private func prepareInputsForMultimodal(\n        imageFeatures: MLXArray,\n        inputsEmbeds: MLXArray,\n        inputIds: MLXArray,\n        attentionMask: MLXArray?\n    ) -> (MLXArray, MLXArray?) {\n        let embedDim = inputsEmbeds.dim(2)\n        let batchSize = inputIds.dim(0)\n        let sequenceLength = inputIds.dim(1)\n\n        // Scale image features to match text embedding magnitude\n        let scaledImageFeatures = imageFeatures / sqrt(Float(config.textConfiguration.hiddenSize))\n\n        // Use input embeddings as starting point\n        var finalEmbedding = inputsEmbeds\n\n        let padTokenId = config.padTokenId\n        let imageTokenId = 262144  // Image token used after expansion\n\n        // Create masks for different token types\n        let textMask = MLX.logicalAnd(\n            MLX.notEqual(inputIds, MLXArray(imageTokenId)),\n            MLX.notEqual(inputIds, MLXArray(padTokenId))\n        )\n        let imageMask = MLX.equal(inputIds, MLXArray(imageTokenId))\n        let padMask = MLX.equal(inputIds, MLXArray(padTokenId))\n\n        // Expand masks to match embedding dimension\n        var imageMaskExpanded = expandedDimensions(imageMask, axis: -1)\n        imageMaskExpanded = repeated(imageMaskExpanded, count: embedDim, axis: -1)\n\n        // Apply pad mask to final embedding\n        var padMaskExpanded = expandedDimensions(padMask, axis: -1)\n        padMaskExpanded = repeated(padMaskExpanded, count: embedDim, axis: -1)\n        finalEmbedding = MLX.where(\n            padMaskExpanded, MLXArray.zeros(like: finalEmbedding), finalEmbedding)\n\n        // Insert image token embeddings using masked_scatter\n        finalEmbedding = maskedScatter(\n            finalEmbedding: finalEmbedding,\n            imageMaskExpanded: imageMaskExpanded,\n            scaledImageFeatures: scaledImageFeatures\n        )\n\n        var finalAttentionMask4d: MLXArray? = nil\n        if let attentionMask = attentionMask {\n            let attentionMaskExpanded1 = expandedDimensions(attentionMask, axis: 1)\n            let attentionMaskExpanded2 = expandedDimensions(attentionMask, axis: 2)\n            finalAttentionMask4d = attentionMaskExpanded1 * attentionMaskExpanded2\n            finalAttentionMask4d = expandedDimensions(finalAttentionMask4d!, axis: 1)\n        }\n\n        return (finalEmbedding.asType(inputsEmbeds.dtype), finalAttentionMask4d)\n    }\n\n    public func prepare(_ input: LMInput, cache: [any KVCache], windowSize: Int?) throws\n        -> PrepareResult\n    {\n        guard let imagePixels = input.image?.pixels else {\n            // Text-only input\n            let convertedCache = cache.compactMap { $0 as? KVCache }\n            let result = languageModel(\n                input.text.tokens, cache: convertedCache, inputEmbedding: nil, mask: nil)\n            return .logits(result)\n        }\n\n        let (inputEmbeddings, _) = getInputEmbeddings(\n            inputIds: input.text.tokens,\n            pixelValues: imagePixels,\n            mask: input.text.mask\n        )\n\n        let convertedCache = cache.compactMap { $0 as? KVCache }\n        // Use causal masking for text generation\n        let maskMode: MLXFast.ScaledDotProductAttentionMaskMode = .causal\n\n        let result = languageModel(\n            nil,  // Pass nil for tokens when using embeddings\n            cache: convertedCache,\n            inputEmbedding: inputEmbeddings,\n            mask: maskMode\n        )\n\n        return .logits(result)\n    }\n\n    public func callAsFunction(_ inputs: MLXArray, cache: [any KVCache]?) -> MLXArray {\n        return languageModel(inputs, cache: cache).logits\n    }\n\n    public func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {\n        let lmHeadKeys = weights.keys.filter { $0.contains(\"lm_head\") }\n\n        // Also check attention layer structures\n        let attnKeys = weights.keys.filter {\n            $0.contains(\"self_attn\")\n                && ($0.contains(\"q_proj\") || $0.contains(\"k_proj\") || $0.contains(\"v_proj\")\n                    || $0.contains(\"o_proj\"))\n        }\n\n        // Handle language model sanitization first (quantization, weight tying, etc.)\n        var processedWeights = languageModel.sanitize(\n            weights: weights, quantizationConfig: config.quantization)\n\n        // Handle vision model sanitization (conv2d weight reshaping, etc.)\n        processedWeights = visionTower.sanitize(weights: processedWeights)\n\n        return processedWeights\n    }\n}\n\npublic class Gemma3Processor: UserInputProcessor {\n    private let config: Gemma3ProcessorConfiguration\n    private let tokenizer: any Tokenizer\n\n    public init(_ config: Gemma3ProcessorConfiguration, tokenizer: any Tokenizer) {\n        self.config = config\n        self.tokenizer = tokenizer\n    }\n\n    public func preprocess(images: [CIImage], processing: UserInput.Processing?) throws -> (\n        MLXArray, THW\n    ) {\n        var userProcessing = processing ?? UserInput.Processing()\n        // Always use the vision configuration's imageSize. Ignore UserInput resize setting.\n        let targetSize = CGSize(width: config.imageSize, height: config.imageSize)\n\n        // Force the correct size for vision model alignment\n        userProcessing.resize = targetSize\n\n        let processedImages = try images.map { image in\n            let processedImage = MediaProcessing.apply(image, processing: userProcessing)\n            let srgbImage = MediaProcessing.inSRGBToneCurveSpace(processedImage)\n            let resizedImage = try MediaProcessing.resampleBicubic(srgbImage, to: targetSize)\n            let normalizedImage = MediaProcessing.normalize(\n                resizedImage, mean: config.imageMeanTuple, std: config.imageStdTuple)\n            return MediaProcessing.asMLXArray(normalizedImage)\n        }\n\n        let pixelValues = concatenated(processedImages)\n\n        return (pixelValues, THW(images.count, config.imageSize, config.imageSize))\n    }\n\n    public func prepare(input: UserInput) async throws -> LMInput {\n        // Use structured content message generator for Gemma3's chat template\n        let messages = Qwen2VLMessageGenerator().generate(from: input)\n\n        var promptTokens = try tokenizer.applyChatTemplate(messages: messages)\n\n        // Process images if any\n        var processedImage: LMInput.ProcessedImage?\n\n        if !input.images.isEmpty {\n            let imagePixelsAndFrames = try input.images.map {\n                try preprocess(images: [$0.asCIImage()], processing: input.processing)\n            }\n            let imagePixelsConcatenated = concatenated(imagePixelsAndFrames.map { $0.0 })\n            processedImage = LMInput.ProcessedImage(\n                pixels: imagePixelsConcatenated,\n                frames: imagePixelsAndFrames.map { $0.1 }\n            )\n\n            // Expand single <start_of_image> token to multiple image tokens\n            let startOfImageTokenId = 255999\n            let imageTokenId = 262144\n            let numImageTokens = config.imageSeqLength  // 256\n\n            var expandedTokens: [Int] = []\n\n            for token in promptTokens {\n                if token == startOfImageTokenId {\n                    // Replace with 256 image tokens\n                    expandedTokens.append(\n                        contentsOf: Array(repeating: imageTokenId, count: numImageTokens))\n                } else {\n                    expandedTokens.append(token)\n                }\n            }\n\n            promptTokens = expandedTokens\n        }\n\n        let promptArray = MLXArray(promptTokens).expandedDimensions(axis: 0)\n        let mask = ones(like: promptArray).asType(.int8)\n        return LMInput(\n            text: .init(tokens: promptArray, mask: mask),\n            image: processedImage\n        )\n    }\n}\n\npublic struct Gemma3ProcessorConfiguration: Codable, Sendable {\n    // Fields from the preprocessor_config.json\n    public let processorClass: String\n    public let imageProcessorType: String\n    public let doNormalize: Bool\n    public let doRescale: Bool\n    public let doResize: Bool\n    public let imageMean: [CGFloat]\n    public let imageStd: [CGFloat]\n    public let imageSeqLength: Int\n    public let resample: Int\n    public let rescaleFactor: Float\n    public let size: ImageSize\n\n    // Optional fields\n    public let doConvertRgb: Bool?\n    public let doPanAndScan: Bool?\n    public let panAndScanMaxNumCrops: Int?\n    public let panAndScanMinCropSize: Int?\n    public let panAndScanMinRatioToActivate: Float?\n\n    // Image token identifier from model configuration\n    public let imageTokenId: Int = 262144\n\n    public struct ImageSize: Codable, Sendable {\n        public let height: Int\n        public let width: Int\n    }\n\n    // Computed properties for convenience\n    public var imageSize: Int { size.height }\n\n    public var imageMeanTuple: (CGFloat, CGFloat, CGFloat) {\n        (imageMean[0], imageMean[1], imageMean[2])\n    }\n\n    public var imageStdTuple: (CGFloat, CGFloat, CGFloat) {\n        (imageStd[0], imageStd[1], imageStd[2])\n    }\n\n    enum CodingKeys: String, CodingKey {\n        case processorClass = \"processor_class\"\n        case imageProcessorType = \"image_processor_type\"\n        case doNormalize = \"do_normalize\"\n        case doRescale = \"do_rescale\"\n        case doResize = \"do_resize\"\n        case doConvertRgb = \"do_convert_rgb\"\n        case doPanAndScan = \"do_pan_and_scan\"\n        case imageMean = \"image_mean\"\n        case imageStd = \"image_std\"\n        case imageSeqLength = \"image_seq_length\"\n        case resample\n        case rescaleFactor = \"rescale_factor\"\n        case size\n        case panAndScanMaxNumCrops = \"pan_and_scan_max_num_crops\"\n        case panAndScanMinCropSize = \"pan_and_scan_min_crop_size\"\n        case panAndScanMinRatioToActivate = \"pan_and_scan_min_ratio_to_activate\"\n    }\n}\n\nextension Gemma3: LoRAModel {\n    public func loraLinearLayers() -> LoRALinearLayers {\n        return languageModel.model.layers.map { ($0.selfAttention, [\"q_proj\", \"v_proj\"]) }\n    }\n}"}]}
